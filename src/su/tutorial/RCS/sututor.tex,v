head	1.20;
access;
symbols;
locks
	john:1.20; strict;
comment	@% @;


1.20
date	2002.08.22.20.19.54;	author john;	state Exp;
branches;
next	1.19;

1.19
date	2001.06.06.18.31.42;	author john;	state Exp;
branches;
next	1.18;

1.18
date	98.09.29.15.28.51;	author john;	state Exp;
branches;
next	1.17;

1.17
date	98.08.10.21.49.03;	author john;	state Exp;
branches;
next	1.16;

1.16
date	98.01.15.17.46.03;	author john;	state Exp;
branches;
next	1.15;

1.15
date	97.10.14.20.15.53;	author john;	state Exp;
branches;
next	1.14;

1.14
date	97.06.17.21.43.13;	author john;	state Exp;
branches;
next	1.13;

1.13
date	96.09.20.21.33.50;	author jkc;	state Exp;
branches;
next	1.12;

1.12
date	96.09.20.21.16.54;	author john;	state Exp;
branches;
next	1.11;

1.11
date	96.08.05.20.14.36;	author john;	state Exp;
branches;
next	1.10;

1.10
date	96.02.07.21.50.06;	author jkc;	state Exp;
branches;
next	1.9;

1.9
date	96.01.30.15.33.03;	author john;	state Exp;
branches;
next	1.8;

1.8
date	95.07.26.18.58.22;	author jkc;	state Exp;
branches;
next	1.7;

1.7
date	95.07.26.11.07.48;	author john;	state Exp;
branches;
next	1.6;

1.6
date	95.03.01.10.46.57;	author jkc;	state Exp;
branches;
next	1.5;

1.5
date	95.03.01.10.33.05;	author jkc;	state Exp;
branches;
next	1.4;

1.4
date	94.10.31.09.53.45;	author jkc;	state Exp;
branches;
next	1.3;

1.3
date	94.10.13.09.17.39;	author jkc;	state Exp;
branches;
next	1.2;

1.2
date	94.10.13.09.14.16;	author jkc;	state Exp;
branches;
next	1.1;

1.1
date	94.07.06.11.19.53;	author jkc;	state Exp;
branches;
next	;


desc
@SU User's Manual
@


1.20
log
@22 August
@
text
@% copyright 2001 Colorado School of Mines, all rights reserved
\documentstyle[11pt,epsf]{book}

% make text fill more of the page
%\nofiles
%\scrollmode
\textwidth 6.25in
\textheight 8.75in
\oddsidemargin .125in
\evensidemargin .125in
\topmargin -.5in

\def\releasenumber{35}
\def\currentyear{2001}
\def\ftpsite{ftp.cwp.mines.edu}
\def\website{www.cwp.mines.edu/cwpcodes}
\def\ipaddress{138.67.12.4}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newenvironment{rmans}{\begin{answer} \em}{\end{answer}}

\begin{document}

\input title.tex
\input legal.tex

\tableofcontents
\newpage

\chapter*{Acknowledgments}
%\addtocontents{toc}{{\bf Acknowledgments}}
\addtocontents{toc}{\protect \contentsline {chapter}{\protect \numberline {}Acknowledgements}{v}}
\markboth{ACKNOWLEDGMENTS}{}

The Seismic Unix project is partially funded by the Society of 
Exploration Geophysicists (SEG), and by the Center for Wave Phenomena (CWP), 
Department of Geophysical Engineering, Colorado School of Mines.  Past 
support for SU has included these groups, as well as the Gas Research
Institute (GRI).

Thank you SEG, and CWP for your continued support!

The sponsors of the CWP Consortium Project have long been partners
in the  SU project and we are pleased to explicitly acknowledge that
relationship here.  In addition, we wish to acknowledge extra support
supplied in the past by IBM Corporation and by the Center for
Geoscience Computing at the Colorado School of Mines during the
late 1980's when SU was ported to the modern workstation from its
previous incarnation on graphics terminals.

So many faculty and students, both at our Center and elsewhere, have
contributed to SU, that it is impossible to list them all here.
However, certain people have made such important contributions that they
deserve explicit mention.

Einar Kjartansson wrote the first draft of what is now called SU
(the SY package) in cooperation with Shuki Ronen while both were 
at Stanford University.
In turn, some of the fundamental concepts they implemented were
formulated by their mentor, Jon Claerbout, Director of the Stanford
Exploration Project.  Ronen brought this work to our Center during a
two-year stay here and, during this time, aided Cohen in
turning SU into a supportable and exportable product.

Chris Liner, while a student at the Center, contributed to many of the graphics
codes used in the pre-workstation (i.e, graphics terminal) age of  SU.
Liner's broad knowledge of seismology and seismic processing enabled
him to make a positive and continuing influence on the SU coding
philosophy.

Craig Artley, now at Golden Geophysical, made major contributions to
the graphics codes while a student at CWP and continues to make
significant contributions to the general package.

Dave Hale wrote several of the ``heavy duty'' processing codes as well
as most of the core scientific and graphics libraries.  His knowledge
of good C-language coding practice helped make our package a good
example for applied computer scientists.

Ken Larner contributed many user interface ideas based on his
extensive knowledge of seismic processing in the ``real world.''

John Scales showed how to use  SU effectively in the classroom in his
electronic text, Theory of Seismic Imaging, Samizdat Press, 1994.
This text is available from the Samizdat press site at:  samizdat.mines.edu.

John Stockwell is largely responsible for the easy installation and
portability of the package, and continues to be the main contact
for the project since the first public release in September of 1992
(Release 18). 

The project has also has had extensive technical help from the
worldwide SU user community.
Among those who should be singled out for mention are Tony Kocurko at
Memorial University in Newfoundland, Toralf Foerster of the 
Baltic Sea Research Institute in Warnemuende Germany, Stewart A. Levin, John
Anderson, and Joe Oravetz at Mobil Oil, Joe Dellinger at Amoco, Matthew
Rutty, Jens Hartmann, Alexander Koek, Michelle Miller
of the University of Southern California at Chico,
Berend Scheffers, and Guy Drijkoningen, Delft,
Dominique Rousset, Pau and Marc Schaming, Strasbourg,
Matt Lamont, Curtin University, Australia, 
Wenying Cai at the University of Utah, Brian Macy of Phillips Petroleum,
Robert Krueger of Terrasys, Torsten Shoenfelder of the University
of Koeln, Germany, Ian Kay of the Geological Survey of Canada,
Dr. Toshiki Watanabe of the University of Kyoto.
Our apologies in advance
for undoubtedly omitting mention of other deserving contributors to
 SU---we promise to include you in future updates of this manual!

We especially thank Barbara McLenon for her detailed suggestions on the text
and also for her excellent design of this document, as well as the
SU pamphlets, and other materials, which we distribute at public meetings.

\section*{In Memoriam}
Dr. Jack K. Cohen passed away in October 1996. The Seismic Unix
package exists owing to his knowledge of seismic processing,
his creativity, and his desire to make a lasting contribution
to the scientific community.  He will surely be missed by all
who had contact with him in the mathematical, geophysical, and 
SU-user communities.

\chapter*{Preface}
In the 5 years that have elapsed since the first version of
this manual was distributed, there have been numerous changes
in the SU package.
These changes have been a result of
both in-house activities here at CWP and, equally important,
to the many contributions of code, bug fixes, extensions,
and suggestions from SU users outside of CWP.
After reviewing these many changes and extensions, and after
much discussion with members of the worldwide SU user community,
it has become apparent that a new manual was necessary.

This new version began in preparation for a short course on
SU at the CREWES project at the University of Calgary.
Many of the items that were in the original manual,
such as information about obtaining and installing the package,
have been have been moved to appendices.
Additional sections describing the core collection of
programs have been added.
The entire manual has been expanded to include more detailed
descriptions of the codes.

The intent is that you will be able to find your way around
the package, via the help mechanisms, that you will learn to
run the programs by running the demos, and finally will be
able to see how to begin writing SU code, drawing on the
source code of the package.

\chapter{About SU}
\pagenumbering{arabic}

In 1985, Jack K. Cohen and Shuki Ronen of the Center for Wave Phenomena (CWP)
at the Colorado School of Mines (CSM)
conceived a bold plan. This plan was to create a seismic processing 
environment for Unix-based systems,
written in the C language, that would extend the Unix operating 
system to seismic processing and research tasks.
Furthermore, they intended that the package be freely available as
full source code to anyone who would want it.
They began with a package called SY, created by Einar Kjartansson
and Shuki Ronen, while both men were graduate students at 
Jon Claerbout's Stanford Exploration Project (SEP).
This package was a sharp departure from seismic processing software
that was available at the time.
The industry standard at the time was to use Fortran programs on
VAX-VMS based systems.

By the time that Cohen and Ronen created the first version of
SU (c. 1985-1986) the sponsors of CWP had already begun showing interest
in the package.
The availability of Unix-based workstations combined with the influx of
Unix-literate geophysicists from the major academic institutions,
shifted the industry to using primarily Unix-based systems for research
and for processing, increasing the interest in Unix-based software,
including SU.

Until September of 1992, SU was used primarily in-house at CWP. 
Earlier versions of SU had been ported to CWP sponsor companies.
The package both inspired, and was inspired by in-house seismic 
processing packages developed at some of those companies.
Once SU became generally available on the Internet, it began
to be used by a much broader community.
The package is used by exploration geophysicists,
earthquake seismologists, environmental engineers, software developers
and others. It is used by scientific staff in both small geotechnical
companies and major oil and gas companies, and by academics and government
researchers, both as a seismic data processing and software development
environment. In addition, SU has found a niche in the ground penetrating
radar (GPR) community, owing to many similarities between GPR data
and seismic data.

\section{What SU is}

The SU package is {\em free software}, meaning that you may have
unrestricted use of the codes for both processing and software development,
provided that you honor the license that appears at the beginning of 
this manual and as the file LEGAL\_STATEMENT in the current release
of the package. (This latter file takes precedence).
The package is maintained and expanded periodically, with
each new release appearing at 3 to 6 month intervals, depending
on changes that accumulate in the official version here at CWP.
The package is distributed with the full source code, so that users
can alter and extend its capabilities.   The philosophy behind the package
is to provide both a free processing and development environment
in a proven structure that can be maintained and expanded to suit
the needs of a variety of users.

The package is not necessarily restricted to seismic processing tasks,
however.  A broad suite of wave-related processing can be done with SU,
making it a somewhat more general package than the word ``seismic'' implies.
SU is intended as an extension of the Unix operating system,
and therefore shares many characteristics of the Unix, including  Unix
flexibility and expandibility. 
The fundamental Unix philosophy is that all operating system commands
are programs run under that operating system.
The idea is that individual tasks be identified, and that small programs
be written to do those tasks, and those tasks alone.
The commands may have options that permit variations on those tasks,
but the fundamental idea is one-program, one-task.
Because Unix is a multi-tasking operating system, multiple processes
may be strung together in a cascade via ``pipes'' ($|$).

This decentralization has the advantage of minimizing overhead
by not launching single ``monster'' applications that try to do
everything, as is seen in Microsoft applications, or 
in some commercial seismic utilities, for example.

Unix has the added feature of supporting a variety of shell languages,
making Unix, itself, a meta-language. Seismic Unix benefits from all
of these attributes as well. In combination with standard Unix
programs, Seismic Unix programs may be used in shell scripts to
extend the functionality of the package.

Of course, it may be that no Unix or Seismic Unix program will fulfill
a specific task. 
This means that new code has to be written.
The availability of a standard set of source code, designed
to be readable by human beings, can expedite the process of extending
the package through the addition of new source code.

\section{What SU is not}

The SU package is not a graphical user interface driven utility
at this time, though there are several possibilities including Java
and TCL/TK scripts, which may be exploited for this purpose in the future.
Because most commercial seismic processing packages are GUI-based,
it is unavoidable that users will expect SU to be similar to these
packages.
However, this is not a fair comparison, for several reasons. 

As mentioned above, SU is an extension of the Unix operating system.
Just as there are no GUI driven Unix operating systems that give full
access to all of the capabilities of Unix from menus, it is not 
reasonable to expect full access to Seismic Unix through such an 
interface.   At most, any SU interface will give limited access
to the capabilities of the package.

SU is not a replacement for commercial seismic packages. Commercial
seismic software packages are industrial-strength utilities designed
for the express purpose of production-level seismic processing.
If you do commercial-level processing, and have dedicated, or plan
to dedicate funds to purchase one or more license of such commercial
software, then it is unlikely that you will be able to substitute
SU for the commercial utility.

However, SU can be an important adjunct to any commercial package
you use. Where commercial packages are used for 
production work, SU often has found a place as a prototyping package. 
Also, if new code needs to be written, SU can provide a starting base
for new software applications.
Indeed, the availability of seismic processing capability may encourage
non-processors to experiment with processing, non-software-developers
to experiment with software development, and non-researchers to 
engage in research.

SU is not confined to seismic applications. It may find use, both
in geophysical and more general signal processing applications. It certainly
can be useful for teaching students about ``wave related'' signal
processing and, particularly, Fourier transform properties.
This can include radar, non-seismic acoustics, and image processing,
just to name a few.

Another thing that SU is not, at least in its current version,
is a 3D package. However, it is not, expressly a 2D package, either,
as there are numerous filtering and trace manipulation tasks that
are the same in 2D as in 3D. It is likely, however, that there will
be 3D applications in future releases of SU. A single 3D migration
code appeared first in Release~32, which will hopefully set the
stage for new developments.

\section{Obtaining and Installing SU}

Because the coding standards of SU stress portability, the
package will install on any system that is running some form
of the Unix operating system, and has a decent version of ``make''
and an ANSI C compiler.
The programs GNU Make and GCC may be substituted for these respective
programs on most systems.\footnote{Remarkably, a software package called
{\bf CYGWIN32}, recently released by the GNU project, provides enough
Unix-like functionality, so that SU may be ported to Windows NT, 
with no changes to the SU source code or Makefiles!}

New releases of SU are issued at 3 to 6 month intervals, depending
upon the accumulation of changes in the home version at CWP.
Old releases are not supported. However, if materials appear to break
between releases, please contact me (John Stockwell) by email
so that we can fix the problem.

Instructions for obtaining and installing SU may be found in
Appendix~\ref{app:A} of this manual.

\chapter{Help facilities}

Like the Unix operating system, Seismic Unix may be thought of
as a language (or meta-language). As with any language, a certain 
amount of vocabulary must be mastered before useful operations may be
performed.
Because the SU contains many programs,
there must be a ``dictionary'' to permit the inevitable
questions about vocabulary can be answered.
It is intended that this manual be the beginning of such a dictionary.

SU does not have ``man pages,'' in the same way that
Unix does, but it does have equivalent internal documentation
mechanisms.
There is a general level help utilities which give an
overview of what is available.
For information about specific aspects of a particular code,
the majority of the programs contain a {\bf selfdoc}---a
self-documentation paragraph, which will appear when the name
of the program is typed on the commandline, with no options.

(In all of the examples that follow, the percent sign ``\%''
indicates a Unix commandline prompt, and is not typed
as part of the command.)

The following tools provide internal documentation at various
levels of detail for the main programs, shell scripts, and
library functions in the package:

\begin{itemize}
\item SUHELP - list the CWP/SU programs and shells
\item SUNAME - get name line from self-docs and location of the source code
\item The selfdoc - is an internal documentation utility which exists
in the majority of executable mains and shell scripts. The selfdoc
is seen by typing the name of the program or shell script on the commandline
with no arguments, and without redirection of input or output via
pipes $|$ or Unix redirects $>$ $<$,
For non-executables (library routines) and for programs without the
selfdoc feature, there is a dummy selfdoc included which provides a
database of information about those items, as well,
\item SUDOC - get DOC listing for code 
\item SUFIND - get info from self-docs 
\item GENDOCS - generate complete list of selfdocs in latex form 
\item suhelp.html - is an HTML global overview of SU programs by subject
matter,
\item SUKEYWORD -- guide to SU keywords in segy.h 
\end{itemize}

This chapter discusses each of these utilities, with the intent
of showing the reader how to get from the most general to the
most specific information about SU programs.

\section{SUHELP - List the Executable Programs and Shell Scripts} 

For a general listing of the contents of SU, which includes each
executable (that is, each main program and shell script) in the package
type:
{\small\begin{verbatim}
% suhelp

CWP PROGRAMS: (no self-documentation)
ctrlstrip       fcat            maxints         t  
downfort        isatty          pause           upfort  

PAR PROGRAMS: (programs with self-documentation)
a2b             kaperture       resamp          transp          vtlvz  
b2a             makevel         smooth2         unif2           wkbj  
farith          mkparfile       smoothint2      unisam  
ftnstrip        prplot          subset          unisam2  
h2b             recast          swapbytes       velconv  


press return key to continue
....
\end{verbatim}}\noindent
Please type {\bf suhelp\/} or see Appendix~{\ref{app:B} for the full text.

Another useful command sequence is:
\begin{verbatim}
% suhelp | lpr
\end{verbatim} \noindent
which will the output from {\bf suhelp\/} to the local print.


\section{SUNAME - Lists the Name and Short Description of Every Item in SU}

A more complete listing of the contents of the CWP/SU package
may be obtained by typing
{\small\begin{verbatim}
% suname

 -----  CWP Free Programs -----   
CWPROOT=/usr/local/cwp

Mains: 

In CWPROOT/src/cwp/main:
* CTRLSTRIP - Strip non-graphic characters
* DOWNFORT - change Fortran programs to lower case, preserving strings
* FCAT - fast cat with 1 read per file 
* ISATTY - pass on return from isatty(2)
* MAXINTS - Compute maximum and minimum sizes for integer types 
* PAUSE - prompt and wait for user signal to continue
* T - time and date for non-military types
* UPFORT - change Fortran programs to upper case, preserving strings

In CWPROOT/src/par/main:
A2B - convert ascii floats to binary 				
B2A - convert binary floats to ascii				
DZDV - determine depth derivative with respect to the velocity	",  
FARITH - File ARITHmetic -- perform simple arithmetic with binary files
FTNSTRIP - convert a file of floats plus record delimiters created 	
H2B - convert 8 bit hexidecimal floats to binary		
KAPERTURE - generate the k domain of a line scatterer for a seismic array
MAKEVEL - MAKE a VELocity function v(x,y,z)				
MKPARFILE - convert ascii to par file format 				
PRPLOT - PRinter PLOT of 1-D arrays f(x1) from a 2-D function f(x1,x2)
RAYT2D - traveltime Tables calculated by 2D paraxial RAY tracing	
RECAST - RECAST data type (convert from one data type to another)	
REGRID3 - REwrite a [ni3][ni2][ni1] GRID to a [no3][no2][no1] 3-D grid
RESAMP - RESAMPle the 1st dimension of a 2-dimensional function f(x1,x2)
...
\end{verbatim}}\noindent

Please type: {\bf suhelp\/} or see Appendix~{\ref{app:B} for the full text.


\section{The Selfdoc - Program Self-Documentation}

There are no Unix man pages for SU. To some people that seems
to be a surprise (even a disappointment) as this would seem to be
a standard Unix feature, which Seismic Unix should emulate.
The package does contain an equivalent mechanism called a 
{\bf selfdoc\/} or self-documentation feature.

This is a paragraph that is written into every program, and arranged
so that if the name of the program is typed on the commandline
of a Unix terminal window, with no options or redirects to or 
from files, the paragraph is printed to standard error (the screen).

For example:
{\small\begin{verbatim}
% sustack
                                                                
 SUSTACK - stack adjacent traces having the same key header word
                                                                
     sustack <input >output [Optional parameters]
                                                                
 Required parameters:                                           
        none                                                    
                                                                
 Optional parameters:                                           
        key=cdp         header key word to stack on             
        normpow=1.0     each sample is divided by the           
                        normpow'th number of non-zero values    
                        stacked (normpow=0 selects no division) 
        verbose=0       verbose = 1 echos information           
                                                                
 Note:  The offset field is set to zero on the output traces.   
        Sushw can be used afterwards if this is not acceptable. 
\end{verbatim}}\noindent

The first line indicates the name of the program {\bf sustack\/} and
a short description of what the program does. 
This is the same line that appears for the listing of  {\bf sustack\/}
in the {\bf suname\/} listing.
The second line
\begin{verbatim}
     sustack <stdin >stdin [Optional parameters]
\end{verbatim}\noindent
indicates how the program is to be typed on the commandline, with
the words ``stdin'' and ``stdout'' indicating that the input
is from the standard input and standard output, respectively.
What this means in Unix terms is that the user could be inputting
and outputting data via diskfiles or the Unix ``redirect in'' $<$ 
and ``redirect out'' $>$ symbols, or via pipes $|$.

The paragraphs labeled by ``Required parameters:'' and ``Optional parameters''
indicate the commandline parameters which are required for the operation
of the program, and those which are optional. The default values of
the Optional parameters are given via the equality  are the values that the program assumes
for these parameters when data are supplied,  with no additional commandline
arguments given. For example: ``key=cdp'' indicates that {\bf sustack\/} will
stack over the common depth point gather number field, ``cdp.''
(The shell script {\bf sukeyword\/} tells the choices of keyword that
are available.) 

\section{SUDOC - List the Full Online Documentation of any Item in SU}
As has been alluded to in previous sections of this manual, there
is a database of selfdocumentation items that is available for each
main program, shell script, and library function.
This database exists in the directory \$CWPROOT/src/doc  and is
composed of all of the selfdocumentation paragraphs of all of the
items in SU.

Because not all all items with selfdocs are executable, an additional
mechanism is necessary to see the selfdoc for these items.
For example, information about the Abel transform routines, located in
\$CWPROOT/src/cwp/lib/abel.c (on the system at CWP,
 CWPROOT=/usr/local/cwp) is obtained via

{\small\begin{verbatim}
% sudoc abel

In /usr/local/cwp/src/cwp/lib: 
ABEL - Functions to compute the discrete ABEL transform:

abelalloc	allocate and return a pointer to an Abel transformer
abelfree 	free an Abel transformer
abel		compute the Abel transform

Function prototypes:
void *abelalloc (int n);
void abelfree (void *at);
void abel (void *at, float f[], float g[]);

Input:
ns		number of samples in the data to be transformed
f[]		array of floats, the function being transformed

Output:
at		pointer to Abel transformer returned by abelalloc(int n)
g[]		array of floats, the transformed data returned by 
		abel(*at,f[],g[])

Notes:
The Abel transform is defined by:

	         Infinity
	g(y) = 2 Integral dx f(x)/sqrt(1-(y/x)^2)
		   |y|

Linear interpolation is used to define the continuous function f(x)
corresponding to the samples in f[].  The first sample f[0] corresponds
to f(x=0) and the sampling interval is assumed to be 1.  Therefore, the
input samples correspond to 0 <= x <= n-1.  Samples of f(x) for x > n-1
are assumed to be zero.  These conventions imply that 

	g[0] = f[0] + 2*f[1] + 2*f[2] + ... + 2*f[n-1]

References:
Hansen, E. W., 1985, Fast Hankel transform algorithm:  IEEE Trans. on
Acoustics, Speech and Signal Processing, v. ASSP-33, n. 3, p. 666-671.
(Beware of several errors in the equations in this paper!)

Authors:  Dave Hale and Lydia Deng, Colorado School of Mines, 06/01/90
\end{verbatim}}\noindent

Here we see that sudoc 
shows information about the routines, including their names, usage
information (via the function prototype), some theory of how the
items are used, published references, and finally the author's names.

As an another example, type:
{\small \begin{verbatim}
% sugabor
                                                                        
 SUGABOR -  Outputs a time-frequency representation of seismic data via
                the Gabor transform-like multifilter analysis technique 
                presented by Dziewonski, Bloch and  Landisman, 1969.    
                                                                        
    sugabor <stdin >stdout [optional parameters]                        
                                                                        
 Required parameters:                                                   
        if dt is not set in header, then dt is mandatory                
                                                                        
 Optional parameters:                                                   
        dt=(from header)        time sampling interval (sec)            
        fmin=0                  minimum frequency of filter array (hz)  
        fmax=NYQUIST            maximum frequency of filter array (hz)  
        beta=3.0                ln[filter peak amp/filter endpoint amp] 
        band=.05*NYQUIST        filter bandwidth (hz)                   
        alpha=beta/band^2       filter width parameter                  
        verbose=0               =1 supply additional info               
                                                                        
 Notes: This program produces a muiltifilter (as opposed to moving window)
 representation of the instantaneous amplitude of seismic data in the   
 time-frequency domain. (With Gaussian filters, moving window and multi-
 filter analysis can be shown to be equivalent.)                        
                                                                        
 An input trace is passed through a collection of Gaussian filters      
 to produce a collection of traces, each representing a discrete frequency
 range in the input data. For each of these narrow bandwidth traces, a 
 quadrature trace is computed via the Hilbert transform. Treating the narrow
 bandwidth trace and its quadrature trace as the real and imaginary parts
 of a "complex" trace permits the "instantaneous" amplitude of each
 narrow bandwidth trace to be compute. The output is thus a representation
 of instantaneous amplitude as a function of time and frequency.        
                                                                        
 Some experimentation with the "band" parameter may necessary to produce
 the desired time-frequency resolution. A good rule of thumb is to run 
 sugabor with the default value for band and view the image. If band is
 too big, then the t-f plot will consist of stripes parallel to the frequency
 axis. Conversely, if band is too small, then the stripes will be parallel
 to the time axis.                                                      
                                                                        
 Examples:                                                              
    suvibro | sugabor | suximage                                        
    suvibro | sugabor | suxmovie n1= n2= n3=                            
     (because suxmovie scales it's amplitudes off of the first panel,  
     may have to experiment with the wclip and bclip parameters        
    suvibro | sugabor | supsimage | ... ( your local PostScript utility)

\end{verbatim}} \noindent

If you compare this output to the output from typing:

{\small \begin{verbatim}
% sudoc sugabor 
\end{verbatim}}\noindent

You will see the same output as above, preceeded by
a line showing the location of the source code,
and followed by the additional paragraphs: 
{\small \begin{verbatim}

 Credits:

	CWP: John Stockwell, Oct 1994

 Algorithm:

 This programs takes an input seismic trace and passes it
 through a collection of truncated Gaussian filters in the frequency
 domain.

 The bandwidth of each filter is given by the parameter "band". The
 decay of these filters is given by "alpha", and the number of filters
 is given by nfilt = (fmax - fmin)/band. The result, upon inverse
 Fourier transforming, is that nfilt traces are created, with each
 trace representing a different frequency band in the original data.

 For each of the resulting bandlimited traces, a quadrature (i.e. pi/2
 phase shifted) trace is computed via the Hilbert transform. The 
 bandlimited trace constitutes a "complex trace", with the bandlimited
 trace being the "real part" and the quadrature trace being the 
 "imaginary part".  The instantaneous amplitude of each bandlimited
 trace is then computed by computing the modulus of each complex trace.
 (See Taner, Koehler, and Sheriff, 1979, for a discussion of complex
 trace analysis.

 The final output for a given input trace is a map of instantaneous
 amplitude as a function of time and frequency.

 This is not a wavelet transform, but rather a redundant frame
 representation.

 References: 	Dziewonski, Bloch, and Landisman, 1969, A technique
		for the analysis of transient seismic signals,
		Bull. Seism. Soc. Am., 1969, vol. 59, no.1, pp.427-444.

		Taner, M., T., Koehler, F., and Sheriff, R., E., 1979,
		Complex seismic trace analysis, Geophysics, vol. 44,
		pp.1041-1063.

 		Chui, C., K.,1992, Introduction to Wavelets, Academic
		Press, New York.

 Trace header fields accessed: ns, dt, trid, ntr
 Trace header fields modified: tracl, tracr, d1, f2, d2, trid, ntr

\end{verbatim}}\noindent

There is more information in the {\bf sudoc\/} listing,
than in the selfdoc listing. The selfdoc is intended as a quick reference,
whereas the sudoc listing can provide additional information
that we do not necessarily want to see everytime we are, say, simply wanting
to know what a particular parameter means, for example.
 
\section{SUFIND - Find SU Items with a Given String}

The ``doc'' database (located in \$CWPROOT/src/doc) may also be
searched for specific 
strings, or topics. The shell script {\bf sufind\/}
serves this purpose.
If you type {\bf sufind\/} with no options, you will see
{\small\begin{verbatim}
% sufind

sufind - get info from self-docs about SU programs
Usage: sufind [-v -n -P<command_pattern>] string
        ("string" can be an "egrep" pattern)
"sufind string" gives brief synopses
"sufind -v string" verbose hunt for relevant items
"sufind -n name_fragment" searches for command name
"sufind -P<pattern> string"
        gives brief synopses by searching for "string" among
        commands/libraries whose names match the "pattern"

\end{verbatim}}\noindent
showing that there is some sophisticated functionality
which may help you find items of a particular type in
the SU package.

As an example of this,  let's say that you are looking
for programs that use the fast Fourier transform
algorithms in SU, type
{\small\begin{verbatim}
% sufind fft

 FFTLAB - Motif-X based graphical 1D Fourier Transform

 Usage:  fftlab


HANKEL - Functions to compute discrete Hankel transforms

hankelalloc     allocate and return a pointer to a Hankel transformer
hankelfree      free a Hankel transformer

PFAFFT - Functions to perform Prime Factor (PFA) FFT's, in place

npfa            return valid n for complex-to-complex PFA
npfar           return valid n for real-to-complex/complex-to-real PFA

 SUAMP - output amp, phase, real or imag trace from             
        (frequency, x) domain data                              

 suamp <stdin >stdout mode=amp                                  

 SUFFT - fft real time traces to complex frequency traces       

 suftt <stdin >sdout sign=1                                     


 SUFRAC -- take general (fractional) time derivative or integral of     
            data, plus a phase shift.  Input is TIME DOMAIN data.       

 sufrac power= [optional parameters] <indata >outdata                   

 SUIFFT - fft complex frequency traces to real time traces      

 suiftt <stdin >sdout sign=-1                                   


 SUMIGPS - MIGration by Phase Shift with turning rays                   

 sumigps <stdin >stdout [optional parms]                                


 SUMIGTK - MIGration via T-K domain method for common-midpoint stacked data

 sumigtk <stdin >stdout dxcdp= [optional parms]                 


 SURADON - forward generalized Radon transform from (x,t) -> (p,tau) space.

 suradon <stdin >stdout [Optional Parameters]                           


For more information type: "program_name <CR>"
\end{verbatim}}\noindent
The final line of this output ends with a symbol meant to indicate that the 
user is to type a carriage return.\footnote{The 
phrase ``carriage return'' refers to an older technology, the typewriter.
Ask your parents (or grandparents) for further details.}
As you can see, there is a mixed collection of programs which 
are either demos of the CWP/SU prime factor fft routines (pfafft),
libraries containing the routines, or programs that use fft routines.

\subsection{Getting information about SU programs}

A more specific example would be to use
{\bf sufind\/} to look for dip moveout (DMO) programs:
{\small\begin{verbatim}
% sufind dmo

 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers

 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           


 SUDMOTX - DMO via T-X domain (Kirchhoff) method for common-offset gathers

 sudmotx <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [optional parms]

 SUDMOVZ - DMO for V(Z) media for common-offset gathers

 sudmovz <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]     

 SUFDMOD2 - Finite-Difference MODeling (2nd order) for acoustic wave equation

 sufdmod2 <vfile >wfile nx= nz= tmax= xs= zs= [optional parameters]     


 SUSTOLT - Stolt migration for stacked data or common-offset gathers    

 sustolt <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
\end{verbatim}}\noindent
The last two ``hits'' are spurious,
but we see that three DMO programs have been found.

Now use the self-doc facility to get more information about {\bf sudmofk\/}:
{\small\begin{verbatim}
% sudmofk
                                                                        
 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
                                                                        
 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
                                                                        
 Required Parameters:                                                   
 cdpmin                  minimum cdp (integer number) for which to apply DMO
 cdpmax                  maximum cdp (integer number) for which to apply DMO
 dxcdp                   distance between adjacent cdp bins (m) 
 noffmix                 number of offsets to mix (see notes)           
                                                                        
 Optional Parameters:                                                   
 tdmo=0.0                times corresponding to rms velocities in vdmo (s)
 vdmo=1500.0             rms velocities corresponding to times in tdmo (m/s)
 sdmo=1.0                DMO stretch factor; try 0.6 for typical v(z)   
 fmax=0.5/dt             maximum frequency in input traces (Hz) 
 verbose=0               =1 for diagnostic print                        
                                                                        
 Notes:                                                         
 Input traces should be sorted into common-offset gathers.  One common- 
 offset gather ends and another begins when the offset field of the trace
 headers changes.                                                       
                                                                        
 The cdp field of the input trace headers must be the cdp bin NUMBER, NOT
 the cdp location expressed in units of meters or feet.         
                                                                        
 The number of offsets to mix (noffmix) should typically equal the ratio of
 the shotpoint spacing to the cdp spacing.  This choice ensures that every
 cdp will be represented in each offset mix.  Traces in each mix will   
 contribute through DMO to other traces in adjacent cdps within that mix.
                                                                        
 The tdmo and vdmo arrays specify a velocity function of time that is   
 used to implement a first-order correction for depth-variable velocity.
 The times in tdmo must be monotonically increasing.                    
                                                                        
 For each offset, the minimum time at which a non-zero sample exists is 
 used to determine a mute time.  Output samples for times earlier than this
 mute time will be zeroed.  Computation time may be significantly reduced
 if the input traces are zeroed (muted) for early times at large offsets.
                                                                        
 Trace header fields accessed:  ns, dt, delrt, offset, cdp.             
\end{verbatim}}\noindent
By using {\bf sufind\/} in conjunction with the selfdoc feature (or {\bf sudoc}),
and choosing smart strings to search on, it is possible to find
a great deal of detailed information about SU's facilities.

\section{GENDOCS - An Instant LaTeX Document Containing All Selfdocs}
The ultimate shell script for exploiting the {\bf sudoc\/} database is
{\bf gendocs}. Typing:
{\small \begin{verbatim}
% gendocs -o
\end{verbatim}}\noindent
will generate the 528+ page document ``selfdocs.tex'', which is
in LaTeX format. You may process this using LaTeX on your system.
Obviously, you must {\em really\/} be sure that you want to print this
document, considering its size.
However, it does contain all of the self-documentations for all
CWP/SU programs, library routines, and shell scripts, and may be
a useful one-to-a-lab type reference.

\section{Suhelp.html}
Long-time SU contributor, Dr. Christopher L. Liner of the University
of Tulsa, created the following  document which may be accessed
from the CWP/SU web site, or from his location of:

\begin{verbatim}
http://www.mcs.utulsa.edu/~cll/suhelp/suhelp.html
\end{verbatim}

{\small\begin{verbatim}
                                SeismicUn*x

                          Version 33 (5 April 1999)

                            An HTML Help Browser

   * This is a help browser for the SeismicUn*x free software package
     developed and maintained by the Center for Wave Phenomena at the
     Colorado School of Mines. The SU project is directed at CWP by John
     Stockwell.
   * The author of this help facility is Dr. Christopher Liner (an alumnus
     of CWP) who is a faculty member in the Department of Geosciences at The
     University of Tulsa.
   * Last updated January 16, 1998



        o The arrangement below is by funtionality
        o Clicking on a program name pulls up the selfdoc for that item
        o Your web browser's Find capability is useful if you have a
          fragment in mind (e.g. sort or nmo)
        o While programs may logically apply to more than one catagory
          below, each program appears only once
     -----------------------------------------------------------------------

  1. Functional Listing
       1. Data Compression
       2. Editing, Sorting and Manipulation
       3. Filtering, Transforms and Attributes
       4. Gain, NMO, Stack and Standard Processes
       5. Graphics
       6. Import/Export
       7. Migration and Dip Moveout
       8. Simulation and Model Building
       9. Utilities

    * Alphabetical name list              * 280 items
    * List is for scanning only, it       * For further info on an
      does not pull up the selfdoc of       interesting item use your
      a selected item.                      browser's find command, then
                                            follow the link

  ------------------------------------------------------------------------

Data Compression

 Discrete Cosine Transform
                                   * dctcomp            * dctuncomp

 Packing                           * supack1            * suunpack1
                                   * supack2            * suunpack2
                                   * wpc1comp2          * wpc1uncomp2
 Wavelet Transform                 * wpccompress        * wpcuncompress
                                   * wptcomp            * wptuncomp
                                   * wtcomp             * wtuncomp

                                  Go to top
...
\end{verbatim}}\noindent
To see the full listing see Appendix~{\ref{app:B} or point your
web browser to the http address above.


\section{Demos}
The SU package contains a suite of demos, which are shell scripts
located in the directory \$CWPROOT/src/demos.

The instructions for accessing the demos are located in the
\$CWPROOT/src/demos/README 

\begin{itemize}
\item The Making\_Data demos shows the basics of making synthetic data
shot gathers and common offset sections using susynlv.  Particular
attention is paid to illustrating good display labeling.

\item The Filtering/Sufilter demo illustrates some real data processing to
eliminate ground roll and first arrivals.  The demos in the
Filtering subdirectories give instructions for accessing the data
from the CWP ftp site.

\item The Deconvolution demo uses simple synthetic spike traces to
illustrate both dereverberation and spiking decon using supef and
other tools.  The demos include commands for systematically
examining the effect of the filter parameters using loops.

\item The Sorting\_Traces Tutorial is an interactive script that
reinforces some of the basic UNIX and {\small\sf SU} lore discussed in this
document.  The interactivity is limited to allowing you to set the
pace.  Such tutorials quickly get annoying, but we felt that one
such was needed to cover some issues that didn't fit into our
standard demo format.  There is also a standard, but less complete,
demo on this topic.

\item The next step is to activate the Selecting\_Traces Demo.  Then
proceed to the NMO Demo. 
\end{itemize}
Beyond that, visit the Demo directories
that interest you.  The {\bf demos\/} directory tree is still under
active development---please let us know if the demos are helpful and
how they can be improved.

\section{Other Help Mechanisms}

\begin{itemize}
\item SUKEYWORD - List SU Header Field Key Words
Many of the SU programs that draw on header field information
have the parameter ``key='' listed in their selfdocs, with
the reference to ``keywords.''
The SU keywords are based on the SEGY trace header fields.
(This will be explained later, in the sections on tape reading
and data format conversion.)
To find out what these header fields are, and what they stand
for in the SU data type, type:
{ \small\begin{verbatim} 
% sukeyword -o
\end{verbatim} } \noindent
Please see Appendix~\ref{app:B} for the full text of the output. 
See Section~\ref{sukeyword} for details on the usage of this
command.  A number of programs sort data, window data, and display data
by making use of the values in the SU header fields, making
{\bf sukeyword\/} an oft-used utility.

\item The essence of {\small\sf SU} usage is the construction of shell programs
  to carry out coordinated data processing.  The {\bf su/examples\/}
  directory contains a number of such programs.  By the way, the terms
  ``shell scripts,'' ``shell programs,'' ``shell files,'' and
  ``shells,'' are used interchangeably in the UNIX literature.

\item The {\bf faq\/} directory contains a growing collection of answers
  to frequently asked questions about {\small\sf SU}, including detailed information
  about tape reading, data format questions, and seismic processing tips.

\item The text book, {\em Theory of Seismic Imaging}, by John A.
  Scales, Samizdat Press, 1994, is available in our anonymous ftp site
  in both 300 and 400 dots per inch PostScript format:
  \verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or
  \verb:imaging_400dpi.ps.Z:.  The exercises in this text make
  extensive use of {\small\sf SU}.
\end{itemize}

You should not hesitate to look at the source code itself.
Section~\ref{SU:sec:template} explains the key {\small\sf SU} coding idioms.
Please let us know if you discover any inconsistencies between the
source and our documentation of it.  We also welcome suggestions for
improving the comments and style of our codes.  The main
strength of the SU package is that it is a source-code product.
No commercial package can give you the source code, for obvious
reasons.

You may direct email to:  john@@dix.mines.edu 
if you have comments, questions, suggestions regarding {\small\sf SU},
or if you have your own {\small\sf SU}-style programs to contribute
to the package.


\chapter{Core Seismic Unix Programs} 

The core of the Seismic Unix program set performs a broad
collection of tasks, which may be viewed as being common
to a large collection of research and processing disciplines.
Many, however, are purely seismic in nature, and are indicated
as such in the text.

Some of these tasks include
\begin{itemize}
\item input/output issues
\item data format conversion,
\item setting, viewing, and editing trace header fields
\item viewing SU data,
\item windowing, sorting, and editing the data.
\item general operations,
\item transform and filtering operations,
\item seismic operations on SU data.
\end{itemize}

It is the intent of the chapters that follow is to deal with
many of these issues.
Please note that more detailed information about any
of the programs discussed can be obtained by typing the name
of the program on the commandline with no arguments,
{\small\begin{verbatim}
% programname
\end{verbatim}} \noindent
or by typing:
{\small\begin{verbatim}
% sudoc programname
\end{verbatim} } \noindent
to see the selfdoc information.

This chapter does not cover all SU programs, but just a sufficient
number such that a person with some Unix experience can get started
with the package. Once you get the idea of how SU is used, then 
you may draw on the help facilities to discover additional programs
in the package.

\section{Reading and Writing Data to and from Tapes}

{\em Reading tapes is more of an art than a science.\/} This is
true in general, and is especially true for SU.
The variability of hardware formats, as well as the variability
of data format types, makes the creation of a ``general tape
reading utility'' a challenging, if not impossible, proposition.

The following programs are useful for the specialized data input and output
tasks related to geophysical applications, as well as to the internal
SU data format
\begin{itemize}
\item BHEDTOPAR - convert a Binary tape HEaDer file to PAR file format
\item DT1TOSU - Convert ground-penetrating radar data in the 
Sensors \& Software X.dt1 GPR format to SU format
\item SEGDREAD - read an SEG-D tape 
\item SEGYCLEAN - zero out unassigned portion of header
\item SEGYREAD - read an SEG-Y tape 
\item SEGYHDRS - make SEG-Y ascii and binary headers for segywrite
\item SEGYWRITE - write an SEG-Y tape 
\item SETBHED - SET the fields in a SEGY Binary tape HEaDer file
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUSTRIP - remove the SEGY headers from the traces 
\item SUPASTE - paste existing SEGY headers on existing data
\end{itemize}

The following programs are useful for general data input, output,
and data type conversion, which may also find use in tape reading,

\begin{itemize}
\item A2B - convert ascii floats to binary
\item B2A - convert binary floats to ascii
\item FTNSTRIP - convert Fortran floats to C-style floats 
\item H2B - convert 8 bit hexidecimal floats to binary
\item RECAST - RECAST data type (convert from one data type to another)
\item TRANSP - TRANSPose an n1 by n2 element matrix 
\end{itemize}

\subsection{The SEGY format and the SU data format}

The data format that is expected by all programs in the CWP/SU package
whose names begin with the letters `su'  (with the exception of the
program {\bf subset}), consists of ``SEGY traces written in the native
binary format of the machine you are running the programs on.''
To understand what this phrase means, we must understand what the
SEGY standard is.

In the early 1980's, the most common data storage format was SEG-Y.
This is the Society of Exploration Geophysicists Y format which is
described in the SEG's publication {\em Digital Tape Standards\/}.
The format is still widely used, today, though there is no
guarantee that the format is used ``by the book.''

The SEGY data format consists of 3 parts. The first part is a
3200 byte EBCDIC card image header which contains 40 cards
(i.e. 40 lines of text with 80 characters per line) worth of
text data describing the tape.
The second part is a 400 byte binary header containing information
about the contents of the tape reel. The third portion of 
the SEG-Y format consists of the actual seismic traces. Each trace
has a 240 byte {\em trace header\/}. The data follow, written in
one of 4 possible 32 formats in IBM floating point notation
as defined in IBM Form GA 22-6821. (Note, this ``IBM format'' is
not the common IEEE format found on modern IBM PC's.)

The SU data format is based on the trace portion of the SEGY format.
The primary difference between the SEGY traces and SU traces is that
the data portion of the SU format are floats, written in the native
binary float format of the machine you are running SU on. SU data
consists of the SEGY traces {\em only\/}! The ebcdic and binary
reel headers are not preserved in the SU format,
so simply redirecting in a SEGY file will not work with any SU 
program.

To convert SEGY data into a form that can be used by SU programs,
you will need to use {\bf segyread}.


\subsection{SEGYREAD - Getting SEG-Y data into SU}
The program {\bf segyread\/} is used to convert data from the SEGY format
to the SU format.
If you type:
{\small\begin{verbatim}
% segyread
\end{verbatim} }  \noindent
You will see the selfdoc for this program.

When reading a SEGY tape, or datafile, you will need to be aware of
the byte-order (endian) of the machine you are running on.
The so-called ``big-endian'' or high-byte IEEE format is found on SGI,
SUN, IBM RS6000, and all Motorola chip-based systems. 
The ``little-endian'' or low-byte systems are systems that are based
on Intel and Dec chips.
You will also need to know what Unix device your tape drive is.

A typical execution of {\bf segyread\/} on a big-endian machine, 
looks like this:
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 > data.su
\end{verbatim}}\noindent

More often you will have to use the following
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 | segyclean > data.su
\end{verbatim}}\noindent
for reading a tape on a big-endian platform.

There are optional header fields (bytes 181-240) in the SEGY trace
headers. There is no standard for what may go in these fields, so
many people have items that they place in these fields for their
own purposes. SU is no exception. There are several parameters 
used by SU graphics programs that may be stored in these fields.
The program {\bf segyclean\/} zeros out the values of the optional header 
fields so that SU graphics programs don't become confused by this information.

There are additional issues, such as whether or not your device
is buffered or unbufferd (i.e. 9 track 1/2 reel tape, or 8mm Exabyte)
tape which may have to be experimented with when you actually
try to read a tape.
Also, if you are trying to read a tape on a different system than
the one it was made on, you may simply not be able to read the tape.

The most common problem with reading tapes is matching the density
that the tape was written in, with the tapedrive that the tape is
being read on.
Some systems, for example Silicon Graphics (SGI) systems, have
many tape devices, which support different hardware configurations
and tape densities.
Other systems, most
notably recent versions of Linux have an improved version of the Unix
command ``mt'' which has a ``setdensities'' option.
In either case, it is common for tapes to be made using the default
settings of a tape drive, or its default densities. 

As a last resort in all tape reading situations, it is often possible to
use the Unix device-to-device copying program {\bf dd\/} to make an image
of the entire tape on disk
{\small \begin{verbatim}
% dd if=/dev/rmtx of=filename bs=32767 conv=noerror
\end{verbatim}}\noindent
where ``/dev/rmtx'' is replaced with your tapedrive device and ``filename''
is some file name you choose.
If this works, then the next step is to try using {\bf segyread\/} as above,
with ``tape=filename.''
If {\bf dd\/} fails, then it is likely that the hardware format
of your tapedrive is not compatible with your tape.

Of course, the best way to prevent tape reading problems is to
make sure that you talk to the person who is writing the tape
before they write it. On SGI systems, in particular, there are
so many possible choices for the type of tape format, that the
person who is making the tape must have information about the platform
that the tape is intended to be read on, before they can make a
tape that is guaranteed to be readable.

\subsection{SEG-Y abuses}

Unfortunately, there are formats which are called ``SEGY'' but which
are not true to the SEG's standards for SEGY.
One common variation is to honor most of the SEGY convention, but
have the traces be in an IEEE format.

Such data would be read via:
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
where the ``conv=0'' tells the program not to attempt the IBM to float
conversion.

There is also a ``DOS SEGY'' format which is similar to the previous
format, with the exception that the traces and headers are all written
in a little-endian format. On a big-endian machine,
the command to read such a dataset would be
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=0 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
will read the data. Note, that endian=0 is set to swap the bytes.
(All of the bytes, header and data are in the swapped format.)
On a little-endian machine, the procedure is 
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
with endian=1, in this case preventing byteswapping.

In each case, if we had a diskfile with some `filename', we would
use ``tape=filename.''

\subsection{SEGYWRITE - Writing an SEGY Tape or Diskfile}
The companion program to {\bf segyread\/} is {\bf segywrite}. This program
permits data to be written either to a tape or a diskfile in
a number of variations on the SEGY format.
This program is useful for putting data into a form that can be
read by commercial seismic packages.
Before showing examples of how {\bf segywrite\/} is used, there
are several preprocessing steps that must be discussed in preparation
for actually writing a tape.

\subsection{SEGYHDRS - make SEG-Y ascii and binary headers for segywrite}

To write a tape in exactly the SEGY format as specified by the
SEG's Digital Tape Standards book, you will need to supply
the ASCII and binary tape reel header files which will become
the EBCDIC and binary tape reel headers in the SEGY tape or file.
These are the files ``header'' and ``binary'' created by segywrite. 

If you don't have the ``binary'' and ``header'' files, then you
must create them with the program {\bf segyhdrs\/} (pronounced SEG Y headers)
The command
{\small\begin{verbatim}
% segyhdrs < data.su
\end{verbatim}}\noindent
will write the files  ``header'' and ``binary'' in the current working
directory.
As an example, make some test data with {\bf suplane\/} and then
run {\bf segyhdrs\/} on it
{\small\begin{verbatim}
% suplane > data.su
% segyhdrs < data.su
\end{verbatim}}\noindent
You will see the files {\em binary\/} and {\em header\/} appear in
your current working directory.

The program has options that will allow you to set the
values of binary header fields. 
These fields may be seen by typing:
{\small\begin{verbatim}
% sukeyword jobid

        int jobid;      /* job identification number */

        int lino;       /* line number (only one line per reel) */

        int reno;       /* reel number */

        short ntrpr;    /* number of data traces per record */

        short nart;     /* number of auxiliary traces per record */

        unsigned short hdt; /* sample interval in micro secs for this reel */

        unsigned short dto; /* same for original field recording */   
...
\end{verbatim}}\noindent
where ``jobid'' is the first binary header field.

The file ``header'' is an ASCII file which may be edited with a
normal text editor. You can put anything in there, as long as the
format is 40 lines of 80 characters each. {\bf Segywrite\/} will
automatically convert this program 
The default header file created by {\bf segyhdrs\/} looks like this
{\small\begin{verbatim}
C      This tape was made at the                                               
C                                                                              
C      Center for Wave Phenomena                                               
C      Colorado School of Mines                                                
C      Golden, CO, 80401                                                       
C                                                                              
...                                                                              
C                                                                              
C                                                                              
\end{verbatim}}\noindent

\subsection{BHEDTOPAR, SETBHED - Editing the binary header file}
The binary header file must be converted to an ASCII form, to be
edited.  The program {\bf bhedtopar\/} permits the ``binary'' file
to be written in the form of a ``parfile''
{\small\begin{verbatim}
% bhedtopar  < binary outpar=binary.par
\end{verbatim}}\noindent
which for the case of the header file created for the test SU data
appears as follows
{\small\begin{verbatim}
jobid=1
lino=1
reno=1
ntrpr=0
nart=0
hdt=4000
...
\end{verbatim}}\noindent
The values that are assigned to the various header files may be
edited, and be reloaded into the header file via {\bf setbhed\/}
{\small\begin{verbatim}
% setbhed bfile=binary par=binary.par
\end{verbatim}}\noindent
Individual header field values may also be set. For example
{\small\begin{verbatim}
% setbhed bfile=binary par=binary.par lino=3
\end{verbatim}}\noindent
which uses the contents of binary.par but with the field lino set to 3.

Finally, the tape may be written via a command sequence like this
{\small \begin{verbatim}
% segywrite tape=/dev/rmtx verbose=1 < data.su
\end{verbatim}} \noindent
taking care to note that the files ``header'' and ``binary'' are
in the current working directory. You may use different names
for these files, if you wish. {\bf Segywrite\/} has a ``bfile='' 
and an ``hfile='' option to permit you to input the files by
the different names you choose.

\subsection{SEGDREAD - Other SEG formats}
There are other SEG formats (SEG-A, SEG-B, SEG-X, SEG-C, SEG-D, SEG-1,
and SEG-2).
Of these, SEG-D, SEG-B, and SEG-2 are the types that you will most
commonly encounter. 
There is a {\bf segdread\/} program which supports only 1 of the vast
number of variations on SEG-D.

In the directory \$CWPROOT/Third\_Party/   is a {\bf seg2segy\/} conversion
program which may be used to convert SEG-2 format to SEG-Y.
Future plans are to create a {\bf seg2read\/} program, which will
be similar to {\bf segyread\/} and {\bf segdread\/}.

\subsection{DT1TOSU - Non-SEG tape formats}

Currently, there is only one non-SEG tape format that is completely supported
in the SU package, and two others which are supported through third-party
codes which have not yet been integrated into the package.
This is the Sensors \& Software DT1 format,
via {\bf dt1tosu}, which is a GPR (ground penetrating radar) format.
In the \$CWPROOT/src/Third\_Party directory are two additional
non-SEG conversion programs, these are {\bf segytoseres\/} and 
{\bf bison2su}.
Future plans include incorporating each of these codes into the main
CWP/SU package.

\section{Data Format conversion}

Often, it is necessary to transfer data from other systems, or
to input data which may be in a variety of formats. A number
of tools and tricks are available in SU for dealing with these
issues.

The following programs may be useful for such conversion problems
\begin{itemize}
\item A2B - convert ascii floats to binary
\item B2A - convert binary floats to ascii
\item FTNSTRIP - convert Fortran floats to C-style floats 
\item FTNUNSTRIP - convert C-style floats to Fortran-style floats
\item H2B - convert 8 bit hexidecimal floats to binary
\item RECAST - RECAST data type (convert from one data type to another)
\item TRANSP - TRANSPose an n1 by n2 element matrix 
\item FARITH - File ARITHmetic -- perform simple arithmetic with binary files 
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUSTRIP - remove the SEGY headers from the traces 
\item SUPASTE - paste existing SEGY headers on existing data
\item SWAPBYTES - SWAP the BYTES of various  data types
\item SUSWAPBYTES - SWAP the BYTES in SU data to convert data from big endian
to little endian byte order, and vice versa
\end{itemize}

The purpose of this section is to discuss situations where these
programs may be used.
\subsection{A2B and B2A - ASCII to Binary, Binary to ASCII}

Of all of the formats of data, the most transportable (and most 
space consuming) is ASCII. No matter what system you are working
on, it is possible to transport ASCII data to and from that system.
Also, because text editors support ASCII, it is usually possible to
data entry and data editing in the simplest of text editors.

Such data probably come in a multicolumn format, separated either
by spaces or tabs. To convert such a, say 5 column, dataset into
binary floats, type:
{\small\begin{verbatim}
% a2b < data.ascii n1=5 > data.binary 
\end{verbatim}}\noindent
The reverse operation is
{\small\begin{verbatim}
% b2a < data.binary n1=5 > data.ascii 
\end{verbatim}}\noindent

\subsection{FTNSTRIP - Importing Fortran Data to C}
Often, because Fortran is a popular language in seismic data processing,
data may be obtained that was either created or processed in some
way with Fortran.
Binary data in Fortran are separated by beginning-of-record
and end-of-record delimiters. Binary data created by C programs do not have
any such delimiters.
To use Fortran data in a C program requires that the Fortran labels
be stripped off, via
{\small\begin{verbatim}
ftnstrip < fortdata > cdata
\end{verbatim}}\noindent
This will produce C-style floats, most of the time.
The program assumes that each record of fortran data is preceded
and followed by an integer listing the size of the record in bytes.
There have been fortran data types which have had only one or the
other of these integer markers, but having both a beginning-of-record
(BOR) and an end-of-record (EOR) markers seems to be standard today.

\subsection{Going from C to Fortran}

It is fairly easy to transport data made with Fortran code to C, however,
it may not be so easy to go the other way. On
the SGI Power Challenge, it is possible to read a file of C-floats
called ``infile'' via, open and read statements that look like:

{\small\begin{verbatim}
OPEN(99,file='infile',form='system') 

DO i=1,number
READ(99) tempnumber
Array(i)=tempnumber
END DO

CLOSE(99)
\end{verbatim}}\noindent

The statement "form='system'" does not work on all machines, however,
as it is likely that this is not standard Fortran.  The general format
command to read in binary is "form='unformatted'".  This may not work
on other systems, (for example, SUN). Indeed, it may not be generally
guaranteed that you can
read binary files in Fortran that have been created with a C-programs
(as in SU).

If you have problems with binary and the input files are not too
big you could convert to ASCII (using 'b2a') and use formatted I/0

{\small\begin{verbatim}
OPEN(99,file='infile')

DO i=1,number
READ(99,*) tempnumber
Array(i)=tempnumber
END DO

CLOSE(99)
\end{verbatim}}\noindent

\subsubsection{FTNUNSTRIP - convert C binary floats to Fortran style floats }

However, another possibility is to make ``fake'' Fortran floats
with a C-program. Such a program is {\bf ftnunstrip}.

This program assumes that the record length is constant
throughout the input and output files.
In fortran code reading these floats, the following implied
do loop syntax would be used:
        DO i=1,n2
                 READ (10) (someARRAY(j), j=1,n1)
        END DO
Here n1 is the number of samples per record, n2 is the number
of records, 10 is some default file (fort.10, for example) which
is opened as form='unformatted'. Here  ``someArray(j)'' is an array
dimensioned to size n1.

Please note that the Fortran style of having BOR and EOR markers
is smart, if used properly, but is stupid if used incorrectly. 
The Fortran READ statement finds out from the BOR marker how many 
bytes will follow, reads the number of values specified, keeping 
track of the number of bytes. When it finishes reading, it compares
the number of bytes read to the number of bytes listed in the EOR
value, and can effectively trap errors in bytes read, or premature
EOR.

Because there is an additional ``sizeof(int)'' (usually 4 bytes)
at the beginning and end of every record, it is smart to have as few
records as is necessary. The worst case scenario is to have every
value of data be a record, meaning that the size of the file could
be 2/3 BOR and EOR markers and 1/3 data!

\subsection{H2B - Importing 8 Bit Hexidecimal}

The issue of converting 8 bit hexidecimal may seem to be one
that would not come up very often.
However 8 bit hex is a common format for bitmapped images
(grayscale PostScript) and if you wish to take a scanned image
and turn it into floats for further processing, then it will
come up.\footnote{This issue came up originally, when a student
had destroyed an original dataset, by accident, but only had
a bit-mapped PostScript image of the data. Using {\bf h2b}
it was possible to recover the data from the PostScript file.}

If you have a scanned image, written as a 256 level grayscale
bitmapped PostScript image, then the bitmap portion is in  8 bit hex.
By removing all of the PostScript commands, and leaving only the
bitmap then the command
{\small\begin{verbatim}
% h2b < hexdata > floatdata
\end{verbatim}}\noindent
will convert the bitmap into a form that can be viewed and processed
by programs in the CWP/SU package.

\subsection{RECAST - Changing Binary Data Types}

Of course, C supports a variety of types, and instead of having
a bunch of program to convert each type into every other type,
there is a program called ``recast'' that will do the job for a
large collection of these types. 

Types supported by recast for input and output:
\begin{itemize}
\item float - floating point
\item double - double precision
\item int - (signed) integer
\item char - character
\item uchar - unsigned char
\item short - short integer
\item long - long integer
\item ulong - unsigned long integer
\end{itemize}
For example, to convert integers to floats
{\small\begin{verbatim}
% recast < data.ints  in=int out=float > data.floats
\end{verbatim}}\noindent
The name of this program derives from the fact that an explicit
type conversion in the C-language is called a ``cast.''

\subsection{TRANSP - Transposing Binary Data}

In the course of any of the operations, it is often necessary
to transpose datasets. In particular, data which are represented
in a multi-column ASCII format, will likely need to be transposed
after being converted from ASCII to binary with {\bf a2b}.
The reason for this is that it is convenient to have the fast
dimension of the data be the time (or depth) dimension
for seismic purposes. 

Our example above was a 5 column dataset, which you might think
of as  5 seismic traces, with the same number of samples in each
trace, side by side in the file. The processing
sequence
{\small\begin{verbatim}
% a2b < data.ascii n1=5 > data.binary 
\end{verbatim}}\noindent
will result in a file with the fast dimension being in the 
trace number direction, rather than the number of samples direction
The additional processing sequence
{\small\begin{verbatim}
% transp < data.binary n1=5 > data.transp
\end{verbatim}}\noindent
will put the data in the desirable form of the fast dimension being
in the number of samples direction, so that each trace is accessed
successively.

\subsection{FARITH - Performs simple arithmetic on binary data.}

It is often necessary to perform arithmetical operations on files,
or between two files of binary data. The program {\bf farith\/} has
been provided for many of these tasks.

Some of the tasks for single files supported by {\bf farith\/} include
\begin{itemize}
\item scaling value,
\item polarity reversal,
\item signum function,
\item absolute value,
\item exponential,
\item logarithm,
\item square root,
\item square,
\item inverse (punctuated),
\item inverse of square (punctuated),
\item inverse of square root (punctuated).
\end{itemize}
Binary operations (operations involving two files) include
value by value
\begin{itemize}
\item addition,
\item subtraction,
\item multiplication,
\item division,
\item cartesian product.
\end{itemize}
Seismic operations (which assume files consist of wavespeeds) include
\begin{itemize}
\item slowness perturbation (difference of inverses of files),
\item sloth perturbation (difference of inverses of files),
\end{itemize}
Examples of using {\bf farith\/} are
{\small \begin{verbatim}
% farith in=data.binary op=pinv out=data.out.bin
% farith in=data1.binary in2=data2.binary op=add > data.out2.bin
\end{verbatim}} \noindent


\section{Trace Header Manipulation}

The data format of SU inherits the seismic trace headers
from SEGY data. If your data are not SEGY, but are created
by conversion from some other type, there is a minimum collection
of headers that you will need to set, for the data to be compatible
with commonly used SU programs.

The issues of interest in this section are:
\begin{itemize}
\item adding trace headers,
\item removing trace headers,
\item pasting trace headers back on,
\end{itemize}

\subsection{SUADDHEAD - Adding SU Headers to Binary Data}
Once data have been put in the correct form, that is to say, 
an array of C-style floats organized with the fast dimension in
the direction of increasing sample number per trace, then
it is necessary to add headers so that these data may be accessible
to other SU programs.

If the data are SEGY, SEGD, DT1, or Bison/Geometrics data, 
read from a tape or diskfile, then the programs {\bf segyread},
{\bf segdread}, {\bf dt1tosu}, or {\bf bison2su} will have set
the trace header values so that the output will be ``SU data.''

For data read from some other means, such as an ASCII dataset
by {\bf a2b}, 
headers have to be added, this is done by using {\bf suaddhead}.
If our dataset consists of a file of binary C-style floats with,
say 1024 samples per trace, then the command sequence

{\small\begin{verbatim}
% suaddhead < data.bin ns=1024 > data.su
\end{verbatim} } \noindent
will yield the SU datafile ``data.su.''

\subsection{SUSTRIP - Strip SU headers SU data}
The inverse operation to {\bf suaddhead} is {\bf sustrip}.
Sometimes, you will have data with SU headers on it, but you may
want to export the data to another program that does not understand
SU headers. The command sequence
{\small\begin{verbatim}
% sustrip < data.su head=data.headers  >data.bin
\end{verbatim}}\noindent
will remove the SU headers and save them in the file ``data.headers.''

\subsection{SUPASTE - Paste SU Headers on to Binary Data}

Having performed an operation on the binary data, we may want to
paste the headers back on. This is done with {\bf supaste}.
The command sequence
{\small\begin{verbatim}
% supaste < data.bin head=data.headers  >data.su
\end{verbatim}}\noindent
will paste the headers contained in data.headers back on to the
data.

\section{Byte Swapping}
Even the best of human intentions can be circumvented. This is
the case with the IEEE floating point data format.
The implementation of the IEEE standard by chip manufacturers
has resulted in two types of commonly encountered data types.
These are called ``big-endian'' (high-byte) or ``little-endian''
(low-byte) types. Little-endian machines are Intel or Dec-based,
whereas big-endian is every other chip manufacturer.

Transporting data (either regular floating point, or SU data)
requires that the bytes be swapped, in order for the data to
be read. Two issues are necessary to address. These are
the issues of swapping the bytes in
\begin{itemize}
\item normal floating point data,
\item SU data.
\end{itemize}

Two programs are provided to perform these tasks. These are
\begin{itemize}
\item SWAPBYTES - SWAP the BYTES of various  data types
\item SUSWAPBYTES - SWAP the BYTES in SU data to convert data from big endian
\end{itemize}

\subsection{SWAPBYTES - Swap the Bytes of Binary (non-SU) Data}

If you transport binary data (data without SU headers) between 
platforms of a different ``endian'' (that is to say, the IEEE byte-order),
from the one you are working on, then you will have to swap the
bytes to make use of that data.

This problem arises because the order of the mantissa and exponent
of binary data comes in two different possible order
under the IEEE data standard.
The so-called ``big-endian'' or high-byte IEEE format is found on SGI,
SUN, IBM RS6000, and all Motorola chip-based systems. 
The ``little-endian'' or low-byte systems are systems that are based
on Intel and Dec chips.

The program {\bf swapbytes\/} is provided to do this for a variety of
data format types, but not SU data. For example,
{\small\begin{verbatim}
% swapbytes < data.bin in=float  >data.swap
\end{verbatim}}\noindent
will swap the bytes in a file of containing C-style floating point numbers.

\subsection{SUSWAPBYTES - Swap the Bytes of SU Data}

If you transport SU data (data with SU headers) between systems,
you have to swap both the data and the SU headers. The program
{\bf suswapbytes\/} is provided to do this.
The command sequence
{\small\begin{verbatim}
% suswapbytes < data.su format=0 > data.su.swapped
\end{verbatim}}\noindent
will swap data in the SU format from a machine of the reverse endian
to the byte order of your system.

The command sequence
{\small\begin{verbatim}
% suswapbytes < data.su format=1 > data.su.swapped
\end{verbatim}}\noindent
will swap data in the SU format that is in your system's byte
order to the opposite byte order.
You would use this if you were transporting your SU data from
you system to another system of the opposite byte order.

\section{Setting, Editing, and Viewing Trace Header Fields}

Seismic data can have a large number of parameters associated with
it. In SU data (following the example of the SEG-Y format) the
values of these parameters are stored in the header fields
of the traces.
There are a number of programs which access the header fields
allowing you to set, view, and modify those fields for a variety
of purposes.

The tasks of interest are
\begin{itemize}
\item adding SU headers,
\item stripping SU headers off of data and then pasting them back on, 
\item identifying SU keywords,
\item viewing the range of SU header values,
\item setting specified SU header fields,
\item computing a third header field from the values of two given fields,
\item getting the values of header fields,
\item editing specific header fields,
\end{itemize}
all of which are necessary, and may come under the heading of 
``geometry setting.''

The following list of programs will be discussed in this chapter
\begin{itemize}
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUSTRIP - remove the SEGY headers from the traces 
\item SUPASTE - paste existing SEGY headers on existing data
\item SUKEYWORD -- guide to SU keywords in segy.h 
\item SURANGE - get max and min values for non-zero header entries
\item SUSHW - Set one or more Header Words using trace number, mod and
integer divide to compute the header word values or input the header word
values from a file 
\item SUCHW - Change Header Word using one or two header word fields 
\item SUGETHW - Get the Header Word(s) in SU Data
\item SUEDIT - examine segy diskfiles and edit headers 
\item SUXEDIT - examine segy diskfiles and edit headers 
\end{itemize}

Some of these items were discussed in the previous section, but 
for completeness, let's recap what these programs do,
with a bit more information.

\subsection{SUADDHEAD - add SU (SEGY-style) Trace Headers}

To add headers to a datafile consisting of C-style binary floating
point numbers do:
{\small\begin{verbatim}
% suaddhead < data.bin ns=1024 > data.su
\end{verbatim}}\noindent
or for some other type, such as integers, use {\bf recast\/}
{\small\begin{verbatim}
% recast < data.ints in=int out=float | suaddhead ns=1024 > data.su
\end{verbatim}}\noindent
Here we have used a pipe $|$ to cascade the processing flow.

If the data are integers were originally from Fortran, then this is a likely
processing flow:
{\small\begin{verbatim}
% ftnstrip < data.fortran | recast in=int out=float | suaddhead ns=1024 > data.su
\end{verbatim}}\noindent

Other variations are obviously possible.
\subsection{SUSTRIP and SUPASTE - Strip and Paste SU Headers}
{\small\begin{verbatim}
% sustrip < data.su head=data.head > data.strip
... other processing that doesn't change the number of traces ...
% supaste < datanew.strip head=data.head > datanew.su
\end{verbatim}}\noindent
See the discussion of {\bf sustrip\/} and {\bf supaste\/} above.

\subsection{SUKEYWORD - See SU Keywords \label{sukeyword}}
The next 5 programs that are discussed in this section have
the option ``key='' in their selfdocs and the reference to
the trace header field ``keywords.''
As has been discussed in Chapter~2, {\bf sukeyword\/} is used to
determine just what these header keywords are.
Typing:
{\small\begin{verbatim}
% sukeyword -o
\end{verbatim}}\noindent
shows this list. (You may also see this list in Appendix~\ref{app:B}.)
However, of the 80+ fields which are defined in the SU header,
only a relatively small subset are used most of the time. These 
fields are  given by the {\bf sukeyword\/} listing as
{\small\begin{verbatim}
	int tracl;	/* trace sequence number within line */
	int tracr;	/* trace sequence number within reel */
	int cdp;	/* CDP ensemble number */
	int cdpt;	/* trace number within CDP ensemble */
 ...
	short trid;	/* trace identification code:
 ...			1 = seismic data

	int offset;	/* distance from source point to receiver
 ...
	int  sx;	/* X source coordinate */
	int  sy;	/* Y source coordinate */
	int  gx;	/* X group coordinate */
	int  gy;	/* Y group coordinate */
	short counit;	/* coordinate units code:
 ...
	short delrt;	/* delay recording time, time in ms between
			   initiation time of energy source and time
			   when recording of data samples begins
			   (for deep water work if recording does not
			   start at zero time) */
	unsigned short ns;	/* number of samples in this trace */
	unsigned short dt;	/* sample interval; in micro-seconds */

 ...
	/* local assignments */
	float d1;	/* sample spacing for non-seismic data */

	float f1;	/* first sample location for non-seismic data */

	float d2;	/* sample spacing between traces */

	float f2;	/* first trace location */
\end{verbatim}}\noindent
It is a good idea to be aware of this collection when using
data derived either from modeling programs or from field datasets.

\subsection{SURANGE - Get the Range of Header Values}

A useful piece of information about trace headers is to see
the range of values of the headers in a given dataset.
Typing, 
{\small\begin{verbatim}
% surange < data.su
\end{verbatim}}\noindent
will return the ranges of all SU header fields that are nonzero.
For example:
{\small\begin{verbatim}
% suplane | surange
32 traces:
 tracl=(1,32)  tracr=(1,32)  offset=400 ns=64 dt=4000
\end{verbatim}}\noindent
The program {\bf suplane\/} generates a test pattern whose header
values simulate a common-offset dataset. The default parameters for
{\bf suplane\/} are to make 32 traces, with 64 samples per trace
at 4 ms (4000 microseconds by SEG convention) sampling, with
offset=400. The output consists of three intersecting lines
of spikes.

Please note that corrupt data may show really strange values for
a large number of the header fields. Detecting such a problem
is one of the primary uses of {\bf surange}.

\subsection{SUGETHW - Get the Values of Header Words in SU Data}

Having header fields on data means that there are many additional
pieces of information to be kept track of, besides just the seismic
data, themselves. 
If you read data from a SEGY format tape, {\bf segyread\/} will
preserve the trace header information in the main part of the
SEGY header.
We saw above that {\bf surange\/} would permit us to see the min
and max header values over an entire dataset.
However, we often need to see the values of trace header
fields, trace by trace, and in an order that we choose.

The program {\bf sugethw\/} (pronounced, SU get header word)
is just such a utility. For example, the command sequence:
{\small\begin{verbatim}
% sugethw < data.su key=keyword1,keyword2,... | more
\end{verbatim} } \noindent
Will dump the values of each of the header fields specified by
the keywords listed.

A more tangible example, using {\bf suplane\/} data input via a pipe $|$
is
{\small\begin{verbatim}
% suplane | sugethw key=tracl,tracr,offset,dt,ns | more
 tracl=1         tracr=1        offset=400          dt=4000         ns=64       

 tracl=2         tracr=2        offset=400          dt=4000         ns=64       

 tracl=3         tracr=3        offset=400          dt=4000         ns=64       

 tracl=4         tracr=4        offset=400          dt=4000         ns=64       

 tracl=5         tracr=5        offset=400          dt=4000         ns=64       

 tracl=6         tracr=6        offset=400          dt=4000         ns=64       

 tracl=7         tracr=7        offset=400          dt=4000         ns=64       
...
\end{verbatim} } \noindent
There is no requirement regarding the order of the key words specified,
or the number of keywords, as long as at least one is specified.

If, for some reason, you need to dump the values in binary format,
an example, again using {\bf suplane\/} data
{\small\begin{verbatim}
% suplane | sugethw key=tracl,tracr,offset,dt,ns output=binary >  file.bin
\end{verbatim} } \noindent
outputs the values sequentially in order of keyword given, trace
by trace.

For ``geometry setting,'' you may want to use a command sequence
(again illustrated by piping suplane data into sugethw)

{\small\begin{verbatim}
% suplane | sugethw key=tracl,tracr,offset,dt,ns output=geom >  hdrfile
\end{verbatim} } \noindent

The contents of  hdrfile may be viewed via
{\small\begin{verbatim}
% more hdrfile

1 1 400 4000 64
2 2 400 4000 64 
3 3 400 4000 64 
4 4 400 4000 64 
5 5 400 4000 64 
...
\end{verbatim} } \noindent
where only the first 5 rows of data have been shown.
\subsection{SUSHW - Set the Header Words in SU Data}

The program {\bf sushw\/} (pronounced, SU set header word) is
an all purpose utility for setting the value of seismic trace
headers. This program permits the user to set one or more
trace header words.
A common use of sushw is to just set a particular field to
one value. For example, sometimes data don't have the ``dt''
field set. Let's say that the data are sampled at 2 ms,
By using {\bf sukeyword}, we see that dt is in microseconds
{\small\begin{verbatim}
% sukeyword dt

...skipping
        unsigned short ns;      /* number of samples in this trace */

        unsigned short dt;      /* sample interval; in micro-seconds */
...

\end{verbatim}} \noindent
This means that the following command sequence will set all dt values
to 2000 microseconds
{\small\begin{verbatim}
% sushw < data.su key=dt a=2000 > data.out.su
\end{verbatim}} \noindent
A more tangible example can be seen by piping {\bf suplane\/} data into
{\bf sushw\/}
{\small\begin{verbatim}
% suplane | sushw key=dt a=2000 | sugethw key=dt | more
161 wenzel> suplane | sushw key=dt a=2000 | sugethw key=dt | more

    dt=2000     

    dt=2000     

    dt=2000     

    dt=2000     

    dt=2000     
...
\end{verbatim}} \noindent

From the selfdoc for {\bf sushw\/} we see that the following optional
parameters are defined
{\small\begin{verbatim}
 Optional parameters ():						
 key=cdp,...			header key word(s) to set 		
 a=0,...			value(s) on first trace			
 b=0,...			increment(s) within group		
 c=0,...			group increment(s)	 		
 d=0,...			trace number shift(s)			
 j=ULONG_MAX,ULONG_MAX,...	number of elements in group		
\end{verbatim}} \noindent
These extra options permit more complicated operations
to be performed.
This is necessary, because often there is a relationship between
header fields and the position of the trace within the dataset.
The value of the header field is computed by the following formula
{\small\begin{verbatim}
 	i = itr + d							
 	val(key) = a + b * (i % j) + c * (i / j)			
 where itr is the trace number (first trace has itr=0, NOT 1)		
\end{verbatim}} \noindent
where percent \% indicates the ``modulo'' function and / is division.

For example if we want to 
set the sx field of the first 5 traces to 6400, the second 5 traces
    to 6300, decrementing by -100 for each 5 trace groups		
{\small\begin{verbatim}
% sushw < data.su key=sx a=6400 c=-100 j=5  > data.new.su
\end{verbatim}} \noindent
Again, piping in {\bf suplane\/} data into {\bf sushw\/}
{\small\begin{verbatim}
% suplane | sushw  key=sx a=6400 c=-100 j=5 | sugethw key=sx | more

    sx=6400     

    sx=6400     

    sx=6400     

    sx=6400     

    sx=6400     

    sx=6300     

    sx=6300     

    sx=6300     

    sx=6300     

    sx=6300     

    sx=6200     

    sx=6200     
...
\end{verbatim}} \noindent

As another example, if we wanted set the ``offset'' fields of each group
of 5 traces to 200,400,...,6400
{\small\begin{verbatim}
%  sushw  < data.su key=offset a=200 b=200 j=5 > data.out.su
\end{verbatim}} \noindent
As before, piping {\bf suplane\/} data into {\bf sushw\/} yields the following
{\small\begin{verbatim}
% suplane | sushw  key=offset a=200 b=200 j=5 | sugethw key=offset | more

offset=200      

offset=400      

offset=600      

offset=800      

offset=1000     

offset=200      

offset=400      

offset=600      

offset=800      

offset=1000     

offset=200      

...
\end{verbatim}} \noindent

We can perform all 3 operations with one call to {\bf sushw}, via:
{\small\begin{verbatim}
% sushw < data.su key=dt,sx,offset a=2000,6400,200 b=0,0,200 c=0,-100,0 j=0,5,5 > newdata.su
\end{verbatim}} \noindent
Or with {\bf suplane\/} data piped in
{\small\begin{verbatim}
% suplane | sushw key=dt,sx,offset a=2000,6400,200 b=0,0,200 c=0,-100,0 j=0,5,5 |
 sugethw key=dt,sx,offset | more

    dt=2000         sx=6400     offset=200      

    dt=2000         sx=6400     offset=400      

    dt=2000         sx=6400     offset=600      

    dt=2000         sx=6400     offset=800      

    dt=2000         sx=6400     offset=1000     

    dt=2000         sx=6300     offset=200      

    dt=2000         sx=6300     offset=400      

    dt=2000         sx=6300     offset=600      

    dt=2000         sx=6300     offset=800      

    dt=2000         sx=6300     offset=1000     

    dt=2000         sx=6200     offset=200      

    dt=2000         sx=6200     offset=400      
...
\end{verbatim}} \noindent
As you can see, it is natural to use pipes and redirects to control
job flow, but this becomes ungainly on a single command line.
Later in this document, we will see how to construct complicated
processing sequences in the controlled environment of shell scripts.

\subsection{Setting Geometry - Converting Observers' Logs to Trace Headers}
There is an often unpleasant task called ``setting geometry''
which must be performed on field data.
Often, a SEGY tape will have only a rudimentary set of header
fields set in the data.
The rest of the information (shot location, geophone location, etc...)
will be supplied in the form of observers' logs. 

For setting geometry, you may wish to dump a specific
collection of header fields of interest into a file,
read the file into a text editor or spreadsheet program
so that you can make changes.

For example, if you have some file ``sudata'' which has
some header fields set incorrectly or incompletely
then the following command sequence illustrates a
possible way of working with such data.
You begin by reading the selected header fields
into a file ``hdrfile''.
{\small\begin{verbatim}
% sugethw < sudata output=geom key=key1,key2,... > hdrfile 		
\end{verbatim} } \noindent
Now edit the ASCII file hdrfile with any editor, setting the fields	
appropriately. Convert hdrfile to a binary format via:		
{\small\begin{verbatim}
% a2b < hdrfile n1=nfields > binary_file				
\end{verbatim} } \noindent
were ``nfields'' is the number of header fields in the ``key=..''
list above.
Then load the new file of header fields via:					
{\small\begin{verbatim}
% sushw < sudata infile=binary_file key=key1,key2,... > sudata.edited
\end{verbatim} } \noindent
Again, ``key=key1,key2,...'' here is the same list as in the
{\bf sugethw\/} statement above.
The finished product is the file sudata.edited.

If you are just beginning to set the header fields,
you may build the ASCII header file  ``hdrfile''  any way you
want. This could be with your favorite text editor, or with
a spreadsheet program. It is not important how the ascii file is
created, as long as it is in multi-column ASCII format for
the sequence above.

Of course, if you have the header values in a file consisting
of C-style floats, (which you can make either from a C-program,
or from Fortran data with {\bf ftnstrip}) listed trace-by-trace, 
then you already have the ``binary\_file'' and need only execute the final
sequence.

\subsection{SUCHW - Change (or Compute) Header Words in SU Data}

Some header fields such as ``cdp'' may be computed from existing
header fields. The program {\bf suchw\/} provides this functionality.

From the selfdoc of {\bf suchw},
{\small \begin{verbatim}
...
 key1=cdp,...	output key(s) 						
 key2=cdp,...	input key(s) 						
 key3=cdp,...	input key(s)  						

...
 a=0,...		overall shift(s)				
 b=1,...		scale(s) on first input key(s) 			
 c=0,...		scale on second input key(s) 			
 d=1,...		overall scale(s)				
\end{verbatim}}\noindent
we can see that this program uses the values of 2 header fields,
key2 and key3, to compute a third, key3, via the equation 
{\small \begin{verbatim}
...
	val(key1) = (a + b * val(key2) + c * val(key3)) / d		
..
\end{verbatim}}\noindent

For example, to shift the values of the  cdp header field by a constant
amount, say $-1$
{\small \begin{verbatim}
% suchw <data >outdata a=-1					
\end{verbatim}}\noindent
or to add a constant amount, say 1000, to a header field, say ``tracr,''
{\small \begin{verbatim}
% suchw key1=tracr key2=tracr a=1000 <infile >outfile		
\end{verbatim}}\noindent

Another possible example is that of setting the ``gx'' 
field by summing the offset and ``sx'' (shot point) values
using {\bf sushw\/} and then computing the  ``cdp'' field
by averaging the ``sx'' and ``gx.''
Here, we are using the
actual cpp locations as the cdp numbers, instead of
the conventional 1, 2, 3, ... enumeration
{\small \begin{verbatim}
% suchw <indata key1=gx key2=offset key3=sx b=1 c=1 |			
% suchw key1=cdp key2=gx key3=sx b=1 c=1 d=2 >outdata			
\end{verbatim}}\noindent

It is possible to perform both operations in one call via:
{\small \begin{verbatim}
%  suchw<indata key1=gx,cdp key2=offset,gx key3=sx,sx b=1,1 c=1,1 d=1,2 >outdata
\end{verbatim}}\noindent

\subsection{SUEDIT and SUXEDIT - Edit the Header Words in SU Data}

Finally, it may be that you wish to examine, and possibly change
just  few headers. For this purpose, we have {\bf suedit\/} and
{\bf suxedit}.
the SU editing programs are executed via:
{\small \begin{verbatim}
% suedit diskfile  (open for possible header modification if writable)	
% suedit <diskfile  (open read only)					
\end{verbatim}}\noindent
and permit the interactive viewing and editing of the header
fields.

For example, making test data with {\bf suplane\/}
{\small \begin{verbatim}
% suplane > data.su
% suedit  data.su
\end{verbatim}}\noindent
yields the following 
{\small \begin{verbatim}
32 traces in input file
 tracl=32 tracr=32 offset=400 ns=64 dt=4000
>                          <------- prompt for interactive use 
\end{verbatim}}\noindent

The commands that may be used interactively in {\bf suedit\/} and 
{\bf suxedit\/}
may be seen by typing a question mark (?) at the prompt. For
example
{\small \begin{verbatim}
32 traces in input file
 tracl=32 tracr=32 offset=400 ns=64 dt=4000
> ?

 n              read in trace #n
 <CR>           step
 +              next trace;   step -> +1
 -              prev trace;   step -> -1
 dN             adv N traces; step -> N
 %              percentiles
 r              ranks
 p [n1 [n2]]    tabplot
 ! key=val      modify field
 ?              print this file
 q              quit
>
\end{verbatim}}\noindent
This program allows the user to view traces as a tabplot of
the data sample values or view or change individual header values

The program {\bf suxedit\/} is similar to {\bf suedit}, with the addition
of X-windows graphics for plotting traces
{\small \begin{verbatim}
% suxedit diskfile  (open for possible header modification if writable)	
% suxedit <diskfile  (open read only)					
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suxedit data.su
32 traces in input file
 tracl=32 tracr=32 offset=400 delrt=5 ns=64 dt=4000

> ?

 n              read in trace #n
 <CR>           step
 +              next trace;   step -> +1
 -              prev trace;   step -> -1
 dN             adv N traces; step -> N
 %              percentiles
 r              ranks
 p [n1 [n2]]    tabplot
 g [tr1 tr2] ["opts"]   wiggle plot
 f              wig plot Fourier Transf
 ! key=val      modify field
 ?              print this file
 q              quit   
\end{verbatim}}\noindent
Again, the options of this program are largely self-explanatory.
Please note that the selfdoc is more informative than the help
menu given by typing the question mark ``?.''

\chapter{Viewing SU Data in X-Windows and PostScript}

The Seismic Unix package has a small collection of graphics utilities
for plotting data, both in general C-style float format, and in
SU format, both in the X-windows environment, for screen viewing
and in the PostScript form for hard copy.

The types of plotting that are available in SU are
\begin{itemize}
\item contour plots,
\item gray or colorscale image plots,
\item wiggle trace plots,
\item line or symbol graphs,
\item movies,
\item 3D cube plots (PostScript only).
\end{itemize}

These programs have lengthy selfdocs, reflecting the large number
of options for selecting the appearance and labeling of the plots.
However, functionality such as windowing data, should be done via
the programs {\bf subset\/} or {\bf suwind\/} as a preprocessing step
before the data are actually sent to the plotting program.
These plotting programs are purposely not designed to window data,
as raw seismic datasets are often huge.

Also, following the ``small is beautiful'' philosophy of Unix,
we have seprate, but more or less equivalent codes for generating
PostScript output for hardcopy plotting.

\section{X-Windows Plotting Programs}

X-windows provides a unified environment for the creation of
screen graphics routines, which can be quite portable,
provided that the code is written using only the items that
can be guaranteed to come with the general distributions of X.

Therefore, all of our X-windows codes are written using straight
X calls, or with the X-Toolkit. While coding in such widget sets
as Motif is easier in many respects, the code that results
is not nearly as portable. There are often great differences
between the implementation of commercial widget sets on various
platforms.

\subsection{Plotting General Floating Point Data}

The programs that are used for viewing general floating point
data (data without SU headers) in the X-window environment are
\begin{itemize}
\item XCONTOUR - X CONTOUR Plot of f(x1,x2) via vector plot call,
\item XIMAGE - X IMAGE plot of a uniformly-sampled function f(x1,x2),
\item XWIGB - X WIGgle-trace plot of f(x1,x2) via Bitmap,
\item XGRAPH - X GRAPHer Graphs n[i] pairs of (x,y) coordinates,
\item XMOVIE - image one or more frames of a uniformly sampled 
function f(x1,x2).
\end{itemize}

Try the following. Make some binary data by stripping the headers
off of some SU data. For example
{\small\begin{verbatim}
% suplane | sustrip > data.bin
n1=64 n2=32 d1=0.004000
nt=64 ntr=32 dt=0.004000
ns=64
\end{verbatim}\noindent
The items which follow indicate that the dimensions of the data
set are ``n1=64'' by ``n2=32.''
Now view these data in each of the plotting programs listed above
(except {\bf xgraph}) via:
{\small\begin{verbatim}
% xcontour < data.bin n1=64 n2=32 title="contour"  &
% ximage < data.bin n1=64 n2=32  title="image"  &
% xwigb < data.bin n1=64 n2=32  title="wiggle trace"  &
% xmovie < data.bin n1=64 n2=32  title="movie"  &
\end{verbatim}}\noindent
Please note that the ampersand ``\&'' is a Unix command
telling the working shell to run the program in background.

To test {\bf xgraph}, make an ASCII file containing  a double
column listing of pairs of data to be plotted such as the
following
{\small\begin{verbatim}
1 1
2 1.5
3 3
4 8
10 7
\end{verbatim}\noindent
Call this file ``data.ascii''
Then use {\bf a2b\/} to convert the file to binary and then plot it
with {\bf xgraph\/}
{\small\begin{verbatim}
% a2b < data.ascii n1=2 > data.bin
n=5
% xgraph < data.bin n=5
\end{verbatim}}\noindent
Note that the ``n=5'' that is echoed by a2b is the same as
the input to xgraph.

When you are finished, and wish to get rid of the windows,
you may click on the window and type the letter ``q,'' for quit.
On some systems, you may have to actually select the ``destroy''
option by clicking and dragging on the square in the upper left
hand side of the window frame.

Please note that all of these programs have a huge number of
options, reflecting a fairly large collection of functionalities.
See the selfdoc of each of these programs by typing
{\small\begin{verbatim}
% programname
\end{verbatim}}\noindent
or
{\small\begin{verbatim}
% sudoc programname
\end{verbatim}}\noindent

\subsection{X-Windows Plotting of SU Data}

To plot data that are in the SU format, a number of programs
(many of which have parallel functionality to those already discussed)
have been created.
These are
\begin{itemize}
\item SUXCONTOUR - X CONTOUR plot of Seismic UNIX tracefile via vector
plot call,
\item SUXIMAGE - X-windows IMAGE plot of an SU data set,
\item SUXWIGB - X-windows Bit-mapped WIGgle plot of an SU data set,
\item SUXGRAPH - X-windows GRAPH plot of an SU data set, 
\item SUXMOVIE - X MOVIE plot of an SU data set,
\item SUXMAX - X-windows graph of the MAX, min, or absolute max value on     
each trace of an SU data set.
\end{itemize}
Rather than maintain multiple codes, each of these programs actually
calls one or more of the X-windows graphics programs listed in the
previous subsection. Please note, that the selfdoc of the non-SU
version of a given graphics program also applies to the SU version,
meaning that, these programs also have a large functionality.

You may test each of these programs using suplane data via:
{\small\begin{verbatim}
% suplane | suxcontour title="contour"  &
% suplane | suximage  title="image"  &
% suplane | suxwigb  title="wiggle trace"  &
% suplane | suxgraph  title="graph" &
% suplane | suxmovie  title="movie" &
% suplane | suxmax    title="max"  &
\end{verbatim}}\noindent
Again, note that the ampersand ``\&'' is a Unix command
telling the working shell to run the program in background.
When you are finished, and wish to get rid of the windows,
you may click on the window and type the letter ``q,'' for quit.
On some systems, you may have to actually select the ``destroy''
option by clicking and dragging on the square in the upper left
hand side of the window frame.

\subsection{Special Features of X-Windows Programs}
To find out all of the many options of these programs, please
see their selfdocs, by typing the names of each of these programs
on the commandline:
{\small\begin{verbatim}
% suxcontour
% suxwigb
% suximage
% suxmovie
% suxmax
\end{verbatim}}\noindent

In addition, the names of the non-SU versions of some of these programs
may be typed to yield additional information
{\small\begin{verbatim}
% xcontour
% xwigb
% ximage
% xmovie
\end{verbatim}}\noindent

However there are some properties that these programs have that
can only be illustrated with examples.

\subsubsection{Plotting wiggle traces in true offset with SUXWIGB}
It is possible to plot wiggle traces in true offset, that is to
say, to take the values for the horizontal dimension of the wiggle
plot from the values in the header.
This is done with the ``key='' parameter.

For example, lets make some test data with {\bf suplane\/} and
plot it using the ``key=offset'' via:
{\small\begin{verbatim}
% suplane | suchw key1=offset key2=tracl a=0 b=100  | suxwigb key=offset   &
\end{verbatim}}\noindent
The result is a plot with the x2 axis labeled in the values of the offset
header field (which count by 100's).

\subsubsection{Making a movie with SUXMOVIE}

It is possible to make movies of seismic data with {\bf suxmovie\/}.
An example of this is to make several synthetic data panels with {\bf  suplane\/}
appending each successive panel with the double redirect sign ``>>''

{\small\begin{verbatim}
% suplane > junk1.su
% suplane | suaddnoise sn=20 >> junk1.su
% suplane | suaddnoise sn=15 >> junk1.su
% suplane | suaddnoise sn=10 >> junk1.su
% suplane | suaddnoise sn=5 >> junk1.su
% suplane | suaddnoise sn=3 >> junk1.su
% suplane | suaddnoise sn=2 >> junk1.su
% suplane | suaddnoise sn=1 >> junk1.su

% suxmovie < junk1.su n2=32 title="frame=%g" loop=1  &
\end{verbatim}}\noindent
The final command has ``n2=32'' set to show that there are 32 traces
per frame of data. The usage of ``\%q'' permits the frame
number to be listed as part of the title, and ``loop=1'' runs the
movie in a continuous loop.

To make the movie go faster or slower, simply enlarge or shrink the
window by clicking and dragging on the lower right corner of the plot.
Clicking the far-right mouse button once will freeze the frame, and clicking
it a second time will start the movie again.

\subsection{PostScript Plotting Programs}

To complement our collection of X-Windows plotting utilities
are a collection of very similar PostScript codes.
The idea was to create PostScript plotting codes which would
correspond to each of the X-Windows codes listed above.

\subsection{PostScript Plotting of General Floating Point Data}

The programs that are used for PostScript plotting of general
floating point data (data without SU headers) are
\begin{itemize}
\item PSCONTOUR - PostScript CONTOURing of a two-dimensional function f(x1,x2),
\item PSIMAGE - PostScript IMAGE plot of a uniformly-sampled function f(x1,x2),
\item PSCUBE - PostScript image plot of a data CUBE,
\item PSGRAPH - PostScript GRAPHer Graphs n[i] pairs of (x,y) coordinates,
\item PSMOVIE - PostScript MOVIE plot of a uniformly-sampled function f(x1,x2,x3),
\item PSWIGB - PostScript WIGgle-trace plot of f(x1,x2) via Bitmap,
\item PSWIGP - PSWIGP - PostScript WIGgle-trace plot of f(x1,x2) via Polygons.
\end{itemize}

Again, you may create binary data to test these programs by stripping
off the headers of some suplane data.
{\small\begin{verbatim}
% suplane | sustrip > data.bin
n1=64 n2=32 d1=0.004000
nt=64 ntr=32 dt=0.004000
ns=64
\end{verbatim}\noindent
The dimensions of the data are n1=64 samples per trace by n2=32 traces.
{\small\begin{verbatim}
% pscontour < data.bin n1=64 n2=32 title="contour" > data1.eps
% psimage < data.bin n1=64 n2=32  title="image"  > data2.eps
% pscube < data.bin n1=64 n2=32  title="cube plot"  > data4.eps
% pswigb < data.bin n1=64 n2=32  title="bitmap wiggle trace"  > data3.eps
% pswigp < data.bin n1=64 n2=32  title="wiggle trace"  > data4.eps
% psmovie < data.bin n1=64 n2=32  title="movie"  > data5.eps
\end{verbatim}}\noindent
The output files contain Adobe Level 2 Encapsulated PostScript.
You should be able to view these files with any standard X-windows
PostScript previewer (such as ``ghostview'').

Please note, that the output from ``psmovie'' may not work on your
system. This output works under NeXTStep, but is multi-page Encapsulated
PostScript, which is not generally supported by PostScript devices.

To test psgraph, make an ascii file containing  a double
column listing of pairs of data to be plotted such as the
following
{\small\begin{verbatim}
1 1
2 1.5
3 3
4 8
10 7
\end{verbatim}\noindent
Call this file ``data.ascii''
Then use ``a2b'' to convert the file to binary and then plot it
with psgraph
{\small\begin{verbatim}
% a2b < data.ascii n1=2 > data.bin
n=5
% psgraph < data.bin n=5 > data6.eps
\end{verbatim}}\noindent
Note that the ``n=5'' that is echoed by {\bf a2b\/} is the same as
the input to {\bf psgraph}.
This permits the following trick to be used
{\small\begin{verbatim}
% a2b < data.ascii outpar=junk.par n1=2 > data.bin
% psgraph < data.bin par=junk.par > data6.eps
\end{verbatim}}\noindent
to yield the same output.

\subsection{PostScript Plotting of SU Data}

Just as there are X-Windows codes for plotting SU data, there
are also codes for making PostScript plots of these data.
The programs for PostScript graphics are
\begin{itemize}
\item SUPSCONTOUR - PostScript CONTOUR plot of an SU  data set 
\item SUPSIMAGE - PostScript IMAGE plot of an SU data set
\item SUPSCUBE - PostScript CUBE plot of an SU data set
\item SUPSGRAPH - PostScript GRAPH plot of an SU data set
\item SUPSWIGB - PostScript Bit-mapped WIGgle plot of an SU data set
\item SUPSWIGP - PostScript Polygon-filled WIGgle plot of an SU data set
\item SUPSMAX - PostScript of the MAX, min, or absolute max value on
each trace of a SU data  set
\end{itemize}

We can use {\bf suplane\/} data to test each of these programs
as we did with the X-Windows codes
{\small\begin{verbatim}
% suplane > junk.su
\end{verbatim}}\noindent
{\small\begin{verbatim}
% supscontour < junk.su title="contour" > data1.eps
% supsimage < junk.su title="image" label1="sec" label2="trace number"  > data2.eps
% supscube < junk.su  title="cube plot"  > data4.eps
% supswigb < junk.su title="bitmap wiggle trace"  > data3.eps
% supswigp < junk.su title="wiggle trace"  > data4.eps
% supsmovie  < junk.su title="movie"  > data5.eps
% supsmax < junk.su title="max"  > data5.eps
\end{verbatim}}\noindent
The output files contain Adobe Level 2 Encapsulated PostScript,
and is compatible with TeX, LaTeX, and many draw tools.
You will need to use a PostScript previewer, such as GhostScript
or Ghostview to view these files on the screen.

Please note again, that programs for which there is a non-SU version,
have selfdoc information which applies to these codes, as well.

\section{Additional PostScript Support}
There are several additional tools for supporting PostScript
operations in SU. These are
\begin{itemize}
\item PSBBOX - change BoundingBOX of existing PostScript file
\item PSMERGE - MERGE PostScript files 
\item MERGE2 - MERGE2 PostScript figures onto one page 
\item MERGE4 - MERGE4 figures onto one page
\item PSLABEL - output PostScript file consisting of a single TEXT string
on a specified background. (Use with psmerge to label plots.)
\item PSMANAGER - printer MANAGER for HP 4MV and HP 5Si Mx Laserjet
PostScript printing 
\item PSEPSI - add an EPSI formatted preview bitmap to an EPS file
\end{itemize}

The programs {\bf psbbox}, {\bf pslabel}, {\bf psmerge}, {\bf merge2},
and {\bf merge4\/} are designed to help in constructing figures
made from SU-style graphics programs, and are not guaranteed to
work with EPS files generated by other means.

\subsection{PSBBOX - Changing the BoundingBox}
For example, create test PostScript data with {\bf suplane\/} and
{\bf supswigb\/} via
{\small\begin{verbatim}
% suplane | supswigb > junk1.eps
\end{verbatim}}\noindent
where the ``.eps'' extension is chosen as a reminder that this is
encapsulated PostScript.
Let's say that there is too much white-space surrounding this figure.
To fix this problem, we want to change the size of the BoundingBox
at the top of the file. For example
{\small\begin{verbatim}
% more junk1.eps
\end{verbatim}}\noindent
shows the dimensions of the BoundingBox
{\small\begin{verbatim}
%!PS-Adobe-2.0 EPSF-1.2
%%DocumentFonts:
%%BoundingBox: 13 31 603 746  
...
\end{verbatim}}\noindent
We could manually edit this, but with {\bf psbbox\/} we can
type:
{\small\begin{verbatim}
% psbbox < junk1.eps llx=40 lly=80 urx=590 ury=730 > junk2.eps
Original:  %%BoundingBox: 13 31 603 746
Updated:   %%BoundingBox: 40 80 590 730 
\end{verbatim}}\noindent
to yield a smaller BoundingBox, with correspondingly less white
space.

\subsection{PSMERGE, MERGE2, MERGE4 - Merging PostScript Plots}

It is often useful to merge several plots to make a compound
figure.
The program {\bf psmerge\/} is the general tool in SU provided
for this. 
There are two additions shell scripts, which call {\bf psmerge\/}
called {\bf merge2\/} and {\bf merge4}.
If we make a couple of test datasets
{\small\begin{verbatim}
% suplane > junk.su
% suplane | sufilter > junk1.su
\end{verbatim}}\noindent
and display these by various means
{\small\begin{verbatim}
% supswigb < junk.su title="Wiggle trace" label1="sec" label2="trace number" > junk1.eps
% supsimage < junk.su title="Image Plot" label1="sec" label2="trace number" > junk2.eps
% supscontour < junk.su title="Contour Plot" label1="sec" label2="trace number" > junk3.eps
% supswigb < junk1.su title="Filtered" label1="sec" label2="trace number" > junk4.eps
\end{verbatim}}\noindent
we now have 4 PostScript files which can be merged to make new plots.

Merging 2 plots may be done via:
{\small\begin{verbatim}
% merge2 junk1.eps junk2.eps > junk.m2.eps
\end{verbatim}}\noindent
while merging all 4 plots may be done via 
{\small\begin{verbatim}
% merge2 junk1.eps junk2.eps junk3.eps junk4.eps > junk.m4.eps
\end{verbatim}}\noindent
Of course, neither {\bf merge2\/} nor {\bf merge4\/} are robust
enough to handle all plot sizes, so you may need to manually 
merge plots with psmerge. Also, if you want to overlay plots,
such as a graph on top of a wiggle trace plot, then you will
also need to use {\bf psmerge}.

Here is an example of creating plots and merging them with {\bf psmerge\/}.
Because the command sequence is ungainly for typing on the commandline
it is expressed as a shell script

{\small\begin{verbatim}
#! /bin/sh
# shell script for demonstrating PSMERGE

# make data
suplane > junk.su
suplane | sufilter > junk1.su

# make PostScript Plots of data
supswigb < junk.su wbox=6 hbox=2.5 \
 title="Wiggle trace" label1="sec" label2="traces" > junk1.eps
supscontour < junk.su wbox=2.5 hbox=2.5  \
 title="Contour Plot" label1="sec" label2="traces" > junk3.eps
supswigp < junk1.su wbox=2.5 hbox=2.5 \
 title="Filtered" label2="traces" > junk4.eps

# merge PostScript plots
psmerge in=junk1.eps translate=0.,0. \
 in=junk3.eps translate=0.0,3.7 \
 in=junk4.eps translate=3.3,3.7 > junk5.eps

echo "You may view the files: junk1.eps, junk3.eps, junk4.eps, junk5.eps"
echo "with your PostScript Previewer"

exit 0
\end{verbatim}}\noindent
In this case, the original files were made small to fit within an
8-1/2'' by 11'' window.
However, {\bf psmerge\/} has the capability of scaling plots.
(This is how the {\bf merge2\/} and {\bf merge4\/} shells work.
You can examine the texts of these for further information by typing
{\small\begin{verbatim}
% more $CWPROOT/bin/merge2
or
% more $CWPROOT/bin/merge4
\end{verbatim}}\noindent
An additional example of merging 3 plots of different sizes is given
by the following shell script
{\small\begin{verbatim}
#! /bin/sh
# shell script for demonstrating PSMERGE

# make data
suplane > junk.su
suplane | sufilter > junk1.su

# make PostScript Plots of data
supswigb < junk.su wbox=7 hbox=4 \
 title="Wiggle trace" label1="sec" label2="traces" > junk1.eps
supscontour < junk.su \
 title="Contour Plot" label1="sec" label2="traces" > junk3.eps
supswigp < junk1.su label1="sec" \
 title="Filtered" label2="traces" > junk4.eps

# merge PostScript plots
psmerge in=junk1.eps translate=0.,0. scale=.6,.6 \
 in=junk3.eps scale=.4,.4 translate=0.0,3.7 \
 in=junk4.eps scale=.4,.4 translate=3.3,3.7 > junk5.eps

echo "You may view the files: junk1.eps, junk3.eps, junk4.eps, junk5.eps"
echo "with your PostScript Previewer"

exit 0
\end{verbatim}}\noindent

In this case, the plots are of normal size, and are then scaled to
fit within an 8-1/2'' by 11'' window.

\section{Trace Picking Utilities}
For lack of a better location to discuss this is the subject,
we will now list``trace picking'' utilities.
The X-Windows wiggle trace, image, and contour plotting programs
all have the attribute that by placing the cursor on the point to
be picked and typing the letter `s' the coordinates of the point
are saved in memory. When the letter `q' is typed, the values
are saved in a user-specified file ``mpicks.''

There are two additional programs which are dedicated to picking
issues. These are
\begin{itemize}
\item XPICKER - X wiggle-trace plot of f(x1,x2) via Bitmap with PICKing 
\item SUXPICKER - X-windows  WIGgle plot PICKER of an SU data set
\item SUPICKAMP - pick amplitudes within user defined and resampled window 
\end{itemize}
with ``xpicker'' being the non-SU data version which ``suxpicker''
calls. The xpicker/suxpicker programs are interactive tools for picking.
The program ``supickamp'' is a simple automated picking program,
which seeks the largest amplituded within a user-specified window.

See demos in \$CWPROOT/src/demos/Picking
for more information about ``supickamp.''

\section{Editing SU Data}

Once the data are read in, and the headers are set properly,
then there will often be manipulation and data editing issues
to be dealt with.

It is often necessary to perform tasks of varying so
that data may be
\begin{itemize}
\item windowed,
\item sorted,
\item truncated,
\item tapered,
\item zeroed,
\item made uniform in number of samples,
\item concatenated,
\end{itemize}
which are dataset editing issues.

This section, therefore will deal with the following programs
\begin{itemize}
\item SUWIND - window traces by key word 
\item SUSORT - sort on any segy header keywords 
\item SURAMP - Linearly taper the start and/or end of traces to zero. 
\item SUTAPER - Taper the edge traces of a data panel to zero. 
\item SUNULL - create null (all zeroes) traces 
\item SUZERO -- zero-out data within a time window
\item SUKILL - zero out traces 
\item SUMUTE - mute above (or below) a user-defined polygonal curve with
the distance along the curve specified by key header word
\item SUVLENGTH - Adjust variable length traces to common length 
\item SUVCAT - append one data set to another (trace by trace)
\end{itemize}
which provide a variety of trace editing utilities.

\subsection{SUWIND - window traces by key word}

It is very common to view or process only a subset of a
seismic dataset. Because seismic data have a number of parameters
that we may want to window the data about, ``suwind'' has been
written.

\subsubsection{Windowing by trace header field}
In its simplist usage, ``suwind'' permits the user to set
min and max values of a specific header field 
{\small \begin{verbatim}
       key=tracl       Key header word to window on (see segy.h)       
       min=LONG_MIN    min value of key header word to pass            
       max=LONG_MAX    max value of key header word to pass            
\end{verbatim}}\noindent

For example, windowing suplane data by trace number yields
{\small \begin{verbatim}
% suplane  | suwind key=tracl  min=5 max=10 | sugethw key=tracl | more

 tracl=5

 tracl=6

 tracl=7

 tracl=8

 tracl=9

 tracl=10   
\end{verbatim}}\noindent
On a large dataset, the ``count'' parameter should be used,
instead of setting the max value. If you set an explicit
``max'' value, suwind will have to go through the entire
dataset to capture all possible traces with values between
the min and max value, because the program assumes that multiple
occurrences of trace labeling are possible.
For example, compare
{\small \begin{verbatim}
% suplane ntr=100000 | suwind key=tracl min=5 max=10 | sugethw tracl | more
\end{verbatim}}\noindent
(it's ok to type ``control-c'' after a few minutes) with
{\small \begin{verbatim}
%suplane ntr=100000 | suwind key=tracl min=5 count=5 | sugethw tracl | more                                                               
\end{verbatim}}\noindent
where suplane has been set to create 100000 traces in each case.

More sophisticated windowing, (decimating data, for example)
{\small \begin{verbatim}
       j=1             Pass every j-th trace ...                       
       s=0             ... based at s  (if ((key - s)%j) == 0)         
\end{verbatim}} \noindent
can be illustrated with suplane data by showing every 2nd trace
{\small \begin{verbatim}
% suplane  | suwind key=tracl j=2 | sugethw key=tracl | more

 tracl=2

 tracl=4

 tracl=6

 tracl=8

 tracl=10
...
\end{verbatim}} \noindent
or by every 2nd trace, based at 1
{\small \begin{verbatim}
% suplane  | suwind key=tracl j=2 s=1 | sugethw key=tracl | more

 tracl=1

 tracl=3

 tracl=5

 tracl=7

 tracl=9         

...
\end{verbatim} } \noindent

Accepting and rejecting traces is also possible with ``suwind.''

{\small \begin{verbatim}
...
       reject=none     Skip traces with specified key values           
...    accept=none     Pass traces with specified key values(see notes)
\end{verbatim} } \noindent
The reject parameter is a straightforward rejecting of numbered
traces.
For example
{\small \begin{verbatim}
suplane | suwind key=tracl reject=3,8,9 | sugethw key=tracl | more
 tracl=1

 tracl=2

 tracl=4

 tracl=5

 tracl=6

 tracl=7

 tracl=10

 tracl=11

 tracl=12    
\end{verbatim} } \noindent
traces 3, 8, and 9 are rejected.

The accept option is a bit strange--it does {\em not\/} mean
accept {\em only\/}
the traces on the accept list!  It means accept these traces,   
even if they would otherwise be rejected.
For example:

{\small \begin{verbatim}
suplane | suwind key=tracl reject=3,8,9 accept=8 | sugethw key=tracl
| more
 tracl=1

 tracl=2

 tracl=4

 tracl=5

 tracl=6

 tracl=7

 tracl=8

 tracl=10

 tracl=11   
....
\end{verbatim} } \noindent

If you want to {\em accept only \/} the traces listed, then you
need to set ``max=0''

{\small \begin{verbatim}
% suplane | suwind key=tracl accept=8 max=0 | sugethw key=tracl | more
 tracl=8 
\end{verbatim} } \noindent
Only trace 8 is passed in this example.

The count parameter overrides the accept    
parameter, so you can't specify count if you want true          
unconditional acceptance.

See the demos in \$CWPROOT/src/demos/Selecting\_Traces for specific
examples.

\subsubsection{Time gating}

The second issue windowing is time gating.
In fact, when people want to window data, they often want to
do both trace and time gating.
{\small \begin{verbatim}
 Options for vertical windowing (time gating):                         
       tmin = 0.0              min time to pass                        
       tmax = (from header)    max time to pass                        
       itmin = 0               min time sample to pass                 
       itmax = (from header)   max time sample to pass                 
       nt = itmax-itmin+1      number of time samples to pass          
\end{verbatim}} \noindent
The result of setting either ``itmin and itmax'' or ``tmin
and tmax'' will be to create a time gate which is to the
nearest sample. If you want to time gate to an arbitrary
(inter-sample) window, then this is a problem in data resampling
and ``suresamp'' is the program of choice.

\subsection{SUSORT - sort on any SEGY header keywords}

One of the advantages of working in the Unix operating system is
that when a superior Unix system call for a particular operation
exists, it is advantageous to use that call to perform the necessary
task. Such a task is sorting, and the Unix ``sort'' command is
just such a utility.

Susort takes advantage of the Unix system sort command to permit
the sorting of traces by header field key work.

For example, sorting data (with values in ascending order)
for two fields (cdp and offset) would be done via:
{\small\begin{verbatim}
% susort <indata.su >outdata.su cdp offset			
\end{verbatim}} \noindent
In decending order for, offset, and ascending order for cdp
{\small\begin{verbatim}
% susort <indata.su >outdata.su cdp -offset			
\end{verbatim}} \noindent
 Note:	Only the following types of input/output are supported	
Disk input to any output, but  pipe input to disk output.				

Please see the demo in \$CWPROOT/src/demos/Sorting\_Traces
for specific examples of trace sorting.

\subsection{SURAMP and SUTAPER - tapering data values}
Many seismic processing algorithms will show spurious artifacts
resulting from sharp edges on a dataset.  Tapering the amplitudes 
on the edges of a dataset is the one of the easiest ways of 
suppressing these artifacts.
For this purpose, we have  ``sutaper'' to taper the edges of
the dataset (for example here, linear taper over 5 traces on each
end of the dataset)
{\small\begin{verbatim}
% sutaper <diskfile >stdout ntaper=5   
\end{verbatim}} \noindent
and ``suramp'' to smooth the beginning and/or end of traces
(for the example here, ramp up from 0 to tmin=.05 seconds,
and ramp down from tmax=1.15 seconds to the end of the traces
{\small\begin{verbatim}
% suramp <diskfile tmin=.05 tmax=1.15 >stdout 
\end{verbatim}} \noindent

\subsection{SUKILL, SUZERO, SUNULL, SUMUTE - zeroing out data}

It is often useful to zero out noisy traces, or traces
on the edge of a dataset (abrupt analog to tapering), or to create
null traces as separators to be used between successive panels of data
in plotting.

\subsubsection{SUKILL - zero out traces}

To zero out a block of traces type,
{\small \begin{verbatim}
% sukill <stdin >stdout min=MIN_TRACE count=COUNT				
\end{verbatim}}\noindent
where COUNT is the number of traces to be zeroed, and MIN\_TRACE
is the minimum trace number in the block of traces.

\subsubsection{SUNULL - Create a Panel of Empty traces}
It is sometimes necessary to create a panel of zero value traces.
To create a panel of traces of NTR traces with NT time samples 
{\small \begin{verbatim}
% sunull nt=NT ntr=NTR <stdin >stdout min=MIN_TRACE count=COUNT				
\end{verbatim}}\noindent

\subsubsection{SUZERO -- zero-out data within a time window}
To zero out data within a time window, use  ``suzero''
{\small \begin{verbatim}
% suzero itmin=MIN_TIME_SAMPLE itmax=MAX_TIME_SAMPLE <indata.su > outdata.su				
\end{verbatim}}\noindent

\subsubsection{SUMUTE - Surgically Muting Data}
The last collection of programs perform a form of crude muting of
the data.
For a more precise muting operation, there is ``sumute''
which can perform surgical muting of SU data.
The program may be used by specifying trace header field
to mute on via the ``key='' parameter,
{\small\begin{verbatim}
% sumute <indata.su >outdata.su key=KEYWORD xmute=x1,x2,x3,... tmute=t1,t2,t3,... 		
\end{verbatim}} \noindent
An suplane data example of this is to compare
original suplane data, made by
{\small\begin{verbatim}
% suplane | suxwigb &
\end{verbatim}} \noindent
with surgically muted data
{\small\begin{verbatim}
% suplane | sumute key=tracl xmute=1,10,12 tmute=.06,.1,.11 | suxwigb &
\end{verbatim} } \noindent
This says to mute every arrival above the polygonal curve defined
by the curve defined by the xmute= and  tmute= values.

The program also permits the x,t values to be input from binary
files by setting the options ``nmute, xfile, and tfile''
as listed in a portion of the selfdoc for this program
{\small\begin{verbatim}
...
 nmute=		number of x,t values defining mute		
 xfile=		file containing position values as specified by	
 			the `key' parameter				
 tfile=		file containing corresponding time values (sec)	
...
 				=tracl  use trace number instead	
 ntaper=0		number of points to taper before hard		
			mute (sine squared taper)			
 below=0		=1 to zero BELOW the polygonal curve		
...
\end{verbatim} } \noindent
Please note also that ``above'' and ``below'' refer to the appearance
on a seismic plot, such as the suxwigb plot and not to the time
values.

\subsection{SUVCAT and CAT - Concatenating Data}
There are two possible ways of appending one dataset onto another
(concatenating). The first way would be to append one dataset
to another, so that the traces from the second file simply follow
the traces of the first file. This is done with the Unix ``cat''
command via:
{\small\begin{verbatim}
% cat data1.su data2.su > data3.su
\end{verbatim}} \noindent
In addition, it may be necessary to renumber the traces via:
{\small\begin{verbatim}
% cat data1.su data2.su | sushw key=tracl a=1 > data3.su
\end{verbatim}} \noindent
so that the tracl parameter is monotonically increasing.

The second way of appending one dataset to another is to ``vertically''
append each trace of the second dataset to the end of the first dataset.
This is done via ``suvcat.''
{\small\begin{verbatim}
% suvcat data1.su data2.su > data3.su
\end{verbatim}} \noindent
In this case, no modification of the header fields should be necessary.

\subsection{SUVLENGTH - Adjust Variable Length Traces to a Common Number of Samples}

Sometimes the data consists of traces which have different numbers
of samples on each trace. The program ``suvlength''  We can construct an example 
of this with suplane data 
{\small \begin{verbatim}
% suplane nt=64 > data1.su
% suplane nt=32 > data2.su
% cat data1.su data2.su > data3.su
\end{verbatim}} \noindent
Attempts to use any SU program on the resulting file ``data3.su''
will probably result in failure, because most SU programs require
that the number of samples be constant on a panel of data.
Applying ``suvlength'' fixes this problem
{\small \begin{verbatim}
% suvlength ns=64 < data3.su > data4.su
% suxwigb < data4.su title="Test of suvlength"  &
\end{verbatim}} \noindent
by making all of the traces the same length.

\chapter{General Operations on SU Data}
Beyond the task of editing SU data are operations level codes
which perform 
\begin{itemize}
\item gaining,
\item resampling,
\item unary operations (arithmetic operations involving a single data file),
\item binary operations (arithmetic operations involving two data files),
\end{itemize}

The purpose of this section, therefore will be to discuss the programs
\begin{itemize}
\item SUADDNOISE - add noise to traces,
\item SUGAIN - apply various types of gain to display traces,
\item SUOP - do unary arithmetic operation on segys,
\item SUOP2 - do a binary operation on two data sets,
\end{itemize}
that perform these operations

\subsection{SUADDNOISE - Add noise to SU data}

While it may seem that having a program that {\em adds\/} noise
to seismic data is counterproductive, it is often useful
for testing purposes to have the capability of simulating
noise.

It is also useful for demonstration purposes, and many of the
demos below use suaddnoise to ``fill in the blanks'' in the
suplane testpattern. A couple of examples of the output of this
program, using suplane data

{\small\begin{verbatim}
% suplane | suxwigb title="no noise" &
% suplane | suaddnoise | suxwigb title="noise added" &
% suplane | suaddnoise sn=2 | suxwigb title="noise added" &
\end{verbatim}}\noindent

\subsection{SUGAIN - Gaining to SU data}

There are numerous operations which come under the heading of
gaining, which ``sugain'' performs.
These operations include
\begin{itemize}
\item scaling the data,
\item multiplying the data by a power of time,
\item taking the power of the data,
\item automatic gain control,
\item trapping noise spiked traces,
\item clipping specified amplitudes or  quantiles,
\item balancing traces by quantile clip, rms value, or mean,
\item scaling the data,
\item biasing or debiasing the data.
\end{itemize}
The heirarchy of the operations is stated by the following equation
{\small\begin{verbatim}
out(t) = scale * BAL{CLIP[AGC{[t^tpow * exp(epow * t) * ( in(t)-bias )]^gpow}]}
\end{verbatim}}\noindent

You may see what sugain does by running the following examples
using suplane data. Noise has been added with ``suaddnoise'' to
make the affects of AGC apparent. 
Type only the items following the percent \%.
Create some SU data (with noise added, via)
{\small\begin{verbatim}
% suplane | suaddnoise > data.su
\end{verbatim}}\noindent

{\small\begin{verbatim}
% suxwigb < data.su title="Ungained Data"  &
% sugain < data.su scale=5.0 | suxwigb title="Scaled data"  &
% sugain < data.su agc=1 wagc=.01 | suxwigb title="AGC=1 WAGC=.01 sec &
% sugain < data.su agc=1 wagc=.2 | suxwigb title="AGC=1 WAGC=.1 sec &
% sugain < data.su pbal=1 | suxwigb title="traces balanced by rms" &
% sugain < data.su qbal=1 | suxwigb title="traces balanced by quantile" &
% sugain < data.su mbal=1 | suxwigb title="traces balanced by mean" &
% sugain < data.su tpow=2 | suxwigb title="t squared factor applied" &
% sugain < data.su tpow=.5 | suxwigb title="square root t factor applied" &
\end{verbatim}}\noindent
Please note, on your terminal window, there will be a message
with ``clip=" some number, for example:
{\small\begin{verbatim}
xwigb: clip=1
\end{verbatim}}\noindent
This indicates the amplitude value above which traces are clipped.
You may think of this as the value of the maximum on the trace.

\subsection{SUOP - Unary Arithmetic Operations on SU Data}

Occassionally we want to apply mathematical functions or
other operations which go beyond gaining to data. Such
operations might include
\begin{itemize}
\item absolute value,
\item signed square root,
\item square,
\item signed square,			
\item signum function,			
\item exponential,
\item natural logarithm,
\item signed common logarithm,		
\item cosine,				
\item sine,				
\item tangent,
\item hyperbolic cosine,		
\item hyperbolic sine,
\item hyperbolic tangent,
\item divide trace by Max. Value,	
\item express trace values in decibels:  20 * slog10 (data)		
\item negate values,
\item pass only positive values,	
\item pass only negative values.
\end{itemize}
Operations involving logarithms are ``punctuated'' meaning
that if the  contains 0 values,	0 values are returned.		

Examples of suop using SU plane data may be easily run
{\small\begin{verbatim}
% suplane | suaddnoise > data.su
% suop < data.su op=abs | suxwigb title="absolute value" &
% suop < data.su op=ssqrt | suxwigb title="signed square root" &
% suop < data.su op=sqr | suxwigb title="signed square" &
...
\end{verbatim}}\noindent

Please type:
{\small\begin{verbatim}
% suop
\end{verbatim}}\noindent
to see the selfdoc and the other options.

\subsection{SUOP2 - Binary Operations with SU data}

To perform operations two SU datasets, the progam ``suop2''
has been provided.
Some of the opperations supported are to compute the
\begin{itemize}
\item difference,
\item sum,
\item product,
\item quotient,
\item difference of a panel and a single trace,
\item sum of a panel and a single trace,
\item product of a panel and a single trace,
\item quotient of a panel and a single trace.
\end{itemize}
The first 4 options assume that there are the same number
of traces in each SU datafile. In the last 4,
it is assumed that there is only a single trace in the
second file.

From the selfdoc of ``suop2'' please note that there are
8 equivalent shell scripts commands which perform these
operations 
{\small\begin{verbatim}
...
 	susum file1 file2 == suop2 file1 file2 op=sum			
 	sudiff file1 file2 == suop2 file1 file2 op=diff			
 	suprod file1 file2 == suop2 file1 file2 op=prod			
 	suquo  file1 file2 == suop2 file1 file2 op=quo			

 For:  panel "op" trace  operations: 				
 	suptsum  file1 file2 == suop2 file1 file2 op=ptsum		
 	suptdiff file1 file2 == suop2 file1 file2 op=ptdiff		
 	suptprod file1 file2 == suop2 file1 file2 op=ptprod		
 	suptquo  file1 file2 == suop2 file1 file2 op=ptquo		
...
\end{verbatim}}\noindent
All of these call ``suop2'' to perform the computation.

Try the following. Make two files of SU data with ``suplane''
{\small\begin{verbatim}
% suplane > junk1.su
% suxwigb < junk1.su | suxwigb title="Data without noise" &
% suplane | suaddnoise > junk2.su
% suxwigb < junk2.su  | suxwigb title="Data with noise added" &
% suop2 junk2.su junk1.su op=diff | suxwigb title="difference" &
\end{verbatim}}\noindent
Note, that the filenames must appear before the ``op=.'' 

\section{Transform and Filtering Operations}

A major aspect of seismic research and seismic processing is
related to operations which are based on mathematical {\em transform\/} 
methods.
In particular, much of seismic processing would not exist without
numerical Fourier transforms.
Filtering is a related subject, because the majority of filters
are applied in the frequency domain, or are at least representable
mathematically as frequency domain operations.

In addition to standard Fourier transforms, there are a couple
of other transforms, such as the Hilbert transform, and the Gabor
transform which are also included in the SU package.
These items may find more use as educational tools, rather than
processing tools.

\subsection{Fourier Transform Operations}

There are Fourier transform operations for both 1D and 2D applications
in the SU package.
The 1D transforms provide spectral (either amplitude or phase)
information about each trace in a panel of seismic data.

The 2D transforms include the seismic F-K variety,
assuming that the fast dimension of the input data is temporal, and
the second dimension is spatial, and the non-seismic  K1-K2
variety, in which the input is assumed to be purely spatial (x1,x2)
data.

\subsection{1D Fourier Transforms}
\begin{itemize}
\item SUFFT - fft real time traces to complex frequency traces
\item SUIFFT - fft complex frequency traces to real time traces
\item SUAMP - output amp, phase, real or imag trace from (frequency, x)
\item SUSPECFX - Fourier SPECtrum (T to F) of traces 
domain data
\end{itemize}

The program ``sufft'' produces the output of the Fourier transform
operation as a complex data type.
The program ``suifft'' is designed to accept complex input as
would be generated by sufft to perform the inverse Fourier transform.
The cascade of these operations is not quite an non-operation,
because there is zeropadding which is automatically implemented for
the transforms. Also, the header fields will not quite be
right for seismic data. For example, try:
{\small \begin{verbatim}
% suplane | suxwigb title="Original Data"  &
% suplane | sufft | suifft | sushw key=d1,dt a=0,4000 | suxwigb  &
\end{verbatim}}\noindent
The result is the same as the input, except there are more samples
on the traces due to zero-padding required for the transform.

To view the amplitude and phase spectra, and the real
and imaginary parts of the of the output of sufft, do the following
{\small \begin{verbatim}
% suplane | sufft | suamp mode=amp | suxwigb title="amplitude" &
% suplane | sufft | suamp mode=phase | suxwigb title="phases" &
% suplane | sufft | suamp mode=real | suxwigb title="real" &
% suplane | sufft | suamp mode=imag | suxwigb title="imaginary" &
\end{verbatim}}\noindent
SU data has a format which allows for the storage of the real
and imaginary parts of data in a complex datatype.
To see the header field settings for that format, type:
{\small \begin{verbatim}
% suplane | sufft | surange
sufft: d1=3.571428
32 traces:
 tracl=(1,32)  tracr=(1,32)  trid=11 offset=400 ns=72
 dt=4000 d1=3.571428 
\end{verbatim}}\noindent
You will notice that the setting for the trace id (trid)
is 11. Typing:
{\small \begin{verbatim}
% sukeyword trid
...
                        11 = Fourier transformed - unpacked Nyquist
                             xr[0],xi[0],...,xr[N/2],xi[N/2]   
...
\end{verbatim}}\noindent
shows that trid=11 how the data are arranged in the output of
the fft.

Of course, most of the time, we only want to have a quick look
at the amplitude spectrum of a seismic trace, or a panel
of seismic traces.
For these purposes use ``suspecfx''
{\small \begin{verbatim}
% suplane | suspecfx | suximage title="F-X Amplitude Spectrum"  &
\end{verbatim}}\noindent
which directly displays the amplitude spectra of each trace
of the input SU data.


\subsection{2D Fourier Transforms}

Seismic data are generally at least 2D datasets. If the data
really are data in (time, space) coordinates, then the output
should be in (frequency, wavenumber), the F-K domain.
However,
there are applications where we consider the data to be
in two spatial dimensions (x1,x2) meaning that the output
is in (k1,k2)

We have the programs
\begin{itemize}
\item SUSPECFK - F-K Fourier SPECtrum of data set 
\item SUSPECK1K2 - 2D (K1,K2) Fourier SPECtrum of (x1,x2) data set
\end{itemize}
For each of these cases. Examples using suplane data are easily
run using
{\small \begin{verbatim}
% suplane | suspecfk | suximage title="F-K Amplitude Spectrum"  &
% suplane | suspeck1k2 | suximage title="K1-K2 Amplitude Spectrum"  &
\end{verbatim}}\noindent
Please note, as these are {\em display\/} programs, so the intent is that 
plots of the output should appear correct when compared with a
plot of the original data. The effect of the 2D Fourier transform 
on a line of spikes is to produce a line of spikes normal to the
original line, if the (k1,k2) data are plotted on the same plot as
the (x1,x2) data, 

\section{Hilbert Transform, Trace Attributes, and Time-Frequency Domain}

A number of classical techniques for representing instantaneous trace
attributes have been created over the years. Many of these techniques
involve the construction of a quadrature trace to be used as the
imaginary part of a ``complex trace'' (with the real part, being 
the real data). The quadrature trace is created with the Hilbert
transform following the construction of the so-called ``allied function.''
This representation permits ``instantaneous amplitude, phase,
and frequency'' information to be generated for a dataset, by
taking the modulus, phase, and time derivative of the phase, respectively.
An alternate approach is to perform multi-filter analysis on data,
to represent it as a function of both time and frequency.

Tools in SU which perform these operations are
\begin{itemize}
\item SUHILB - Hilbert transform 
\item SUATTRIBUTES - trace ATTRIBUTES instantanteous amplitude, phase, or frequency 
\item SUGABOR -  Outputs a time-frequency representation of seismic data via
the Gabor transform-like multifilter analysis,
\end{itemize}
To generate the Hilbert transform of a test dataset, try
{\small \begin{verbatim}
% suplane | suhilb | suxwigb title="Hilbert Transform"  &
\end{verbatim}}\noindent
This program is useful for instructional and testing purposes.

To see an example of ``trace attributes'' with ``suattributes''
it is necessary to make data which has a strong time-frequency
variability. This is done here with ``suvibro'' which makes a
synthetic vibroseis sweep. Compare the following:
{\small \begin{verbatim}
% suvibro | suxgraph title="Vibroseis sweep" &
% suvibro | suattributes mode=amp | suxgraph title="Inst. amplitude"  &
% suvibro | suattributes mode=phase unwrap=1.0 | suxgraph title="Inst. phase"  &
% suvibro | suattributes mode=freq | suxgraph title="Inst. frequency"  &
\end{verbatim}}\noindent
which show, respectively, instantaneous amplitude, phase, and frequency.

To see the synthetic vibroseis trace in the time-frequency domain,
try the following:
{\small \begin{verbatim}
% suvibro | sugabor | suximage title="time frequency plot"  &
\end{verbatim}}\noindent
The result is an image which shows the instantaneous or apparent
frequency increasing from 10hz to 60hz, with time, exactly as
stated by the default parameters of ``suvibro.''

See the demos in \$CWPROOT/src/demos/Time\_Freq\_Analysis
and \$CWPROOT/src/demos/Filtering/Sugabor
for further information.

\section{Radon Transform - Tau\_P Filtering}

The Radon or ``tau-p'' transform, as it is sometimes called in
geophysical literature is useful for a variety of multiple suppression,
and other ``surgical'' data manipulation tasks.
The programs
\begin{itemize}
\item SUTAUP - forwared and inverse T-X and F-K global slant stacks
\item SUHARLAN - signal-noise separation by the invertible linear
transformation method of Harlan, 1984
\item SURADON - compute forward or reverse Radon transform or remove multiples
by using the parabolic Radon transform to estimate multiples and subtract.
\item SUINTERP - interpolate traces using automatic event picking 
\end{itemize}
are provided to make use of this transform.
You may tests using suplane data as we have with other programs
to see the output from each of these codes. Both ``suinterp'' and ``suradon''
have some sophisticated options which my require some experimentation.

See also the demos in \$CWPROOT/src/demos/Tau\_P  for examples.

\section{1D Filtering Operations}

A large part of what is called seismic processing, may be
thought of as being ``filtering.''
There are filtering operations for 1D applications in
the SU package that span simple filtering tasks, to more
sophisticated tasks including deconvolution and wavelet shaping operations.
These operations are 1D, in that they are applied trace by trace.

Several types of filtering operations that arise in seismic
processing are
\begin{itemize}
\item zero bandpass, bandreject, lowpass, and highpass , and notch filtering,
\item minimum or zero phase Butterworth filtering,
\item Wiener prediction error (deconvolution),
\item Wiener wavelets shaping,
\item convolution,
\item crosscorrelation,
\item autocorrelation,
\item data resampling with sinc interpolation,
\item fractional derivatives/integrals,
\item median filtering,
\item time varying filtering.
\end{itemize}

The programs that are provided to meet these needs are:
\begin{itemize}
\item SUFILTER - applies a zero-phase, sine-squared tapered filter 
\item SUBFILT - apply Butterworth bandpass filter
\item SUACOR - auto-correlation
\item SUCONV, SUXCOR - convolution, correlation with a user-supplied filter
\item SUPEF - Wiener predictive error filtering
\item SUSHAPE - Wiener shaping filter 
\item SURESAMP - Resample in time,
\item SUFRAC -- take general (fractional) time derivative or integral of
data, plus a phase shift.  Input is TIME DOMAIN data.
\item SUMEDIAN - MEDIAN filter about a user-defined polygonal curve with     
the distance along the curve specified by key header word 
\item SUTVBAND - time-variant bandpass filter (sine-squared taper)
\end{itemize}

\subsection{SUFILTER - applies a zero-phase, sine-squared tapered filter}
The program {\bf sufilter\/} provides a general purpose zero-phase filtering
capability for the usual tasks of bandpass, bandreject, lowpass, highpass,
and notch filtering.  Examples of each of these using {\bf sufilter\/}
data are provided
{\small \begin{verbatim}
% suplane | sufilter f=10,20,30,60 amps=0,1,1,0 | suxwigb title="10,20,30,60 hz bandpass"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suplane | sufilter f=10,20,30,60 amps=1,0,0,1 | suxwigb title="10,20,30,60 hz bandreject"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suplane | sufilter f=10,20,30,60 amps=1,1,0,0 | suxwigb title="10,20 hz lowpass"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suplane | sufilter f=50,60,70 amps=1,0,1 | suxwigb title="60 hz notch"  &
\end{verbatim}}\noindent

The filter is polygonal, with the corners of the polygon defined by
the vector of frequency values defined by array of {\bf f=\/} values,
and a collection of amplitude values, defined by the array of {\bf amps=\/}
values. The amplitudes may be any floating point numbers greater than,
or equal to zero. The only rule is that there must be the same number
of {\bf amps\/} values, as {\bf f\/} values.  The segments are sine-squared
tapered between {\bf f\/} values of different amplitude. It is
best to select {bf f\/} values such that tapering to zero is done
over an octave to prevent ringing.

With {\bf suplane\/}, {\bf sufilter\/} provides a way of easily
generating bandlimited testpattern data in SU format.

\subsection{SUBFILT - apply Butterworth bandpass filter}

An alternative to {\bf sufilter\/} is {\bf subfilt\/}, which
applies a Butterworth filter to data. 

{\small \begin{verbatim}
% suplane | subfilt fstoplo=10 fpasslo=20 fpasshi=30 fstophi=60 | suxwigb title="10,20,30,60 hz  bandpass bfilt"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}

\subsection{SUACOR - auto-correlation}
This program is used to compute the autocorrelation of a trace.
This process is useful to see the size of a wavelet, or the repetitions
of the wavelet, as an aid in choosing the {\bf maxlag\/} parameter
of {\bf supef}. {\bf Suacor\/} is also useful for determining the
frequency range of data in terms of the power spectrum. For example
try
{\small \begin{verbatim}
% suplane | sufilter | suacor | suspecfx | suxwigb &
\end{verbatim}}\noindent

\subsection{SUCONV, SUXCOR - convolution, correlation with a user-supplied filter}

The standard operations of convolution and cross correlation may be
performed with {\bf suconv\/} and {\bf suxcor\/}, respectively.
The filter may be supplied as a vector input on the commandline,
or as a file containing a single trace in SU format. In addition
to taking the input as a single trace, it is possible to supply
a panel of filters, each to be used trace by trace on a panel of
SU data.

An example of correlating a vibroseis sweep may be seen by creating
vibroseis-like data, with {\bf suvibro\/}, {\bf suplane\/} and {\bf suconv\/}.
To make ``vibroseis'' {\bf suplane\/} data

{\small \begin{verbatim}
% suvibro > junk.vib.su
% suplane | suconv sufile=junk.vib.su > plane.vib.su
\end{verbatim}}\noindent

Because {\bf surange\/} tells us that
{\small \begin{verbatim}
% surange < junk.vib.su
1 traces:
 tracl=1 ns=2500 dt=4000 sfs=10 sfe=60
 slen=10000 styp=1
\end{verbatim}}\noindent

there are 2500 samples on the vibroseis sweep, we can do the following
correlation
{\small \begin{verbatim}
%  suxcor < plane.vib.su sufile=junk.vib.su |
     suwind itmin=2500 itmax=2563 | sushw key=delrt a=0.0 > data.su
(this line is broken to make it fit on the page here, the real
command is typed on a single line)
\end{verbatim}}\noindent
The value of {\bf itmin=sweeplength\/} and {\bf itmax=sweeplength+nsout\/}
where {\bf nsout\/} is the number of samples expected in the output.
The final step using {\bf sushw\/} is to set the trace delay to 0.
Choosing {\bf itmin=sweeplength\/} will ensure that the data start
at the correct value. Choosing {\bf nsout=sweeplength-nsin\/}, where
{\bf nsin\/} is the number of samples in the input, will yield the
correct number of samples to keep.

\subsection{SUPEF - Wiener predictive error filtering}
The prediction error filtering method, also known as Wiener filtering,
is the principle process of traditional Wiener-Levinson deconvolution.
The reason that this subsection is not headed ``deconvolution'' is
because there are two additional issues that have to be addressed
before prediction error filtering can be used for effective deconvolution.
These issues are preprocessing of the data, and postprocess filtering.
Indeed, much feedback has come back claiming that {\bf supef\/}
doesn't work properly, when in fact, it simply being used improperly.
(This is our fault, for not supplying sufficient documentation.)

Using {\bf supef\/}, itself, generally requires that the value
of {\bf maxlag\/} be set. This may be determined by first establishing the
size of the wavelet being spiked, through application {\bf suacor\/}.

As the preprocessing step, you will probably need to use {\bf sugain\/}
to remove any decay in amplitudes with time that may result from geometric
spreading. 

As a postprocessing step, you need to remove any increase in frequency
content which has occurred due to the whitening effect of the 
prediction error filter. The demos in the demos/Deconvolution  directory
demonstrate the functioning of the program. However, these examples
are a bit unrealistic for field data. For example, in the demos, the
data are spiked, and then reverberations are removed via a cascaded
of {\bf supef\/} calls with {\bf maxlag=.04\/} and 
{\bf minlag=.05 maxlag=.16\/}, respectively. However on field data,
we would probably only use a single pass of the filter to remove
reverberations, or to spike arrivals.

The prediction error filter has a spectral whitening effect, which
is likely to put high frequencies in the data that were not there
originally. These must be filtered out. It is a good idea to assume
that there is a {\em loss\/} of frequency information, and design
the parameters for the postprocessing filtering, using {\bf sufilter\/}
to slightly reduce the frequency content from its original values.

Also, an added feature of the Release~32 version of {\bf supef\/}
is the mixing parameter, which permits the user to apply a weighted
moving average to the autocorrelations that are computed as part of
the prediction error filter computation. This can provide additional
stability to the operation.

\subsection{SUSHAPE - Wiener shaping filter}

The demos in demos/Deconvolution also contain a demonstration of
the Wiener shaping filter {\bf sushape\/}.


\subsubsection{2D Filtering Operations}
Filtering in the (k1,k2) domain, and the (F,K) domain is
often useful for changing dip information in data. 
The programs
\begin{itemize}
\item SUKFILTER - radially symmetric K-domain, sin\^2-tapered, polygonal filter 
\item SUK1K2FILTER - symmetric box-like K-domain filter defined by the
cartesian product of two sin\^2-tapered polygonal filters defined in k1 and k2
\item SUKFRAC - apply FRACtional powers of i|k| to data, with phase shift 
\item SUDIPFILT - DIP--or better--SLOPE Filter in f-k domain
\end{itemize}
provide the beginnings of a set of K-domain and F-K-domain filtering
operations.

\subsection{SURESAMP - Resample Data in Time}
Often data need to be resampled to either reduce or increase
the number of samples for processing or data storage.
For seismic data, the smart way of doing this is by the 
method of sinc interpolation.
The program ``suresamp''
\begin{itemize}
\item SURESAMP - Resample in time 
\end{itemize}
performs this operation.

See the demos in \$CWPROOT/src/demos/Filtering   for further information
about filtering in SU.

\chapter{Seismic Modeling Utilities}

An important aspect of seismic exploration and research are programs
for creating synthetic data. Such programs find their use,
both in the practical problem of modeling real data, as well
as in the testing of new processing programs. A processing program
that will not work on idealized model data will likely not work
on real seismic data.

Another important aspect of modeling programs is the fact that many
seismic processing algorithms (such as migration) may be viewed
as {\em inverse processes\/}.  The first step in such an inverse 
problem may be to create a method to solve the forward problem,
and then formulate the solution to the inverse
problem as a ``backpropagation'' of the recorded data to its position
in the subsurface.

There are two parts to the seismic modeling task. The first part is the
construction of background wavespeed profiles, which may consist
of uniformly sampled arrays of floating point numbers.
The second part is the construction of the synthetic wave
information which propagates in that wavespeed profile.

Because of the intimate relationship between seismic modeling and
seismic processing, background wavespeed profiles created for
modelling tasks, may also be useful for processing tasks.

Of course, if some simple assumptions are made, it may be possible
for background wavespeed information to be built into the modeling
program.

\section{Background Wavespeed Profiles}

There are many approaches to creating background wavespeed profiles.
For many processes, it may be that a simple array of floating point
numbers, each representing the wavespeed, slowness (1/wavespeed), or
sloth (slowness squared) on a uniformly sampled grid will be sufficient.

However, more advanced techniques may involve wavespeed profile generation
in triangulated or tetrahedrized media.

\section{Uniformly Sampled Models}
In Seismic Unix there are several programs which may be used to generate
background wavespeed profile data. Often, such data need to be
smoothed.

These programs are:
\begin{itemize}
\item UNISAM - UNIformly SAMple a function y(x) specified as x,y pair
\item UNISAM2 - UNIformly SAMple a 2-D function f(x1,x2)
\item MAKEVEL - MAKE a VELocity function v(x,y,z)
\item UNIF2 - generate a 2-D UNIFormly sampled velocity profile from a 
layered model. In each layer, velocity is a linear function of position.
\item SMOOTHINT2 - SMOOTH non-uniformly sampled INTerfaces, via the damped
least-squares technique 
\item SMOOTH2 - SMOOTH a uniformly sampled 2d array of data, within a user-
defined window, via a damped least squares technique
\item SMOOTH3D - 3D grid velocity SMOOTHing by the damped least squares 
\end{itemize}

Please see the selfdoc of each of these programs for further information.
Also see the demos in \$CWPROOT/src/Velocity\_Profiles

\section{Synthetic Data Generators}

There are a number of programs for generating synthetic seismic 
and seismic-like data in the SU package.
These are
\begin{itemize}
\item SUPLANE - create common offset data file with up to 3 planes 
\item SUSPIKE - make a small spike data set
\item SUIMP2D - generate shot records for a line scatterer embedded 
in three dimensions using the Born integral equation 
\item SUIMP3D - generate inplane shot records for a point scatterer
embedded in three dimensions using the Born integral equation
\item SUFDMOD2 - Finite-Difference MODeling (2nd order) for acoustic 
wave equation
\item SUSYNCZ - SYNthetic seismograms for piecewise constant V(Z) function   
True amplitude (primaries only) modeling for 2.5D
\item SUSYNLV - SYNthetic seismograms for Linear Velocity function
\item SUSYNVXZ - SYNthetic seismograms of common offset V(X,Z) media via     
Kirchhoff-style modeling
\item SUSYNLVCW - SYNthetic seismograms for Linear Velocity function
for mode Converted Waves
\item SUSYNVXZCS - SYNthetic seismograms of common shot in V(X,Z) media via  
Kirchhoff-style modeling
\end{itemize}
Of these, only sufdmod2, susynvxz, and sysnvxzcs require an file
of input wavespeed. The other programs use commandline arguments
for wavespeed model input.

Please see the demos in \$CWPROOT/src/demos/Synthetic for further
information. Also, a number of the other demos use these programs
for synthetic data generation.

\section{Delaunay Triangulation}

More sophisticated methods of synthetic data generation use
assumptions regarding the nature of the medium (as represented
by the input wavespeed profile data format) to expedite computations
of the synthetic data. One such method is triangulation via the
Delaunay method. 

\subsection{Triangulated Model Building}
There are two way of making triangulated models. The first
is to explicitly input boundary coordinates and wavespeed (actually
sloth values) with ``trimodel.'' The second is to make a uniformly
sampled model, perhaps with one of the model building utilities
above, and then use ``uni2tri'' to convert the uniformly sampled
model to a triangulated model. (Please note, that this will work
better if the wavespeed model is smoothed, prior to conversion.)
These programs are
\begin{itemize} 
\item TRIMODEL - make a triangulated sloth (1/velocity squared) model
\item UNI2TRI - convert UNIformly sampled model to a TRIangulated model
\item TRI2UNI - convert a TRIangulated model to UNIformly sampled model
\end{itemize}

\subsection{Synthetic Seismic Data in Triangulated Media}

Several programs make use of triangulated models to create ray
tracing, or ray-trace based synthetic seismograms.
There is also a code to create Gaussian beam synthetic seismograms
in a triangulated medium. 
These programs are:
\begin{itemize} 
\item NORMRAY - dynamic ray tracing for normal incidence rays in a sloth model
\item TRIRAY - dynamic RAY tracing for a TRIangulated sloth model 
\item GBBEAM - Gaussian beam synthetic seismograms for a sloth model
\item TRISEIS - Synthetic seismograms for a sloth model 
\end{itemize}

There is a comprehensive set of demos located in the directory
\$CWPROOT/src/Delaunay\_Triangulation

\section{Tetrahedral Methods}
Some new functionality will be entering SU in future releases
for tetrahedral model building and ray tracing in tetrahedral models.
One code that is in the current release is

\begin{itemize}
\item TETRAMOD - TETRAhedron MODel builder. In each layer, velocity gradient
is constant or a 2-D grid; horizons could be a uniform grid and/or added by
a 2-D grid specified.
\end{itemize}


\chapter{Seismic Processing Utilities}
There are a collection of operations which are uniquely seismic
in nature, representing operations which are designed to
perform some aspect of the involved process which takes seismic
data and converts it into images of the earth.

\begin{itemize}
\item stacking data,
\item picking data,
\item velocity analysis,
\item normal moveout correction,
\item dip moveout correction,
\item seismic migration and related operations.
\end{itemize}

\section{SUSTACK, SURECIP, SUDIVSTACK - Stacking Data}
\begin{itemize}
\item SUSTACK - stack adjacent traces having the same key header word, 
\item SURECIP - sum opposing offsets in prepared data,
\item SUDIVSTACK -  Diversity Stacking using either average power or peak   
power within windows 
\end{itemize}

\section{SUVELAN, SUNMO - Velocity Analysis and Normal Moveout Correction}
\begin{itemize}
\item SUVELAN - compute stacking velocity semblance for cdp gathers
\item SUNMO - NMO for an arbitrary velocity function of time and CDP 
\end{itemize}

Please see the demos in  \$CWPROOT/src/demos/Velocity\_Analysis
and \$CWPROOT/src/demos/NMO for further information. 

\section{SUDMOFK, SUDMOTX, SUDMOVZ - Dip Moveout Correction}

Dip-moveout is a data transformation which converts data recorded
with offset to zero offset data.
The following programs
\begin{itemize}
\item SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
\item SUDMOTX - DMO via T-X domain (Kirchhoff) method for common-offset gathers
\item SUDMOVZ - DMO for V(Z) media for common-offset gathers 
\end{itemize}
perform this operation.

\section{Seismic Migration}
The subject of seismic migration is one of the most varied in
seismic data processing. Many algorthms have been developed
to perform this task. 
Methods include Kirchhoff, Stolt, Finite-Difference,
Fourier Finite-Difference, and several types of Phase-Shift
or Gazdag Migration.

Please see the demos in \$CWPROOT/src/demos/Migration
for further information.

\subsection{SUGAZMIG, SUMIGPS, SUMIGPSPI, SUMIGSPLIT - Phase Shift Migration}
\begin{itemize}
\item SUGAZMIG - SU version of Jeno GAZDAG's phase-shift migration for
zero-offset data. 
\item SUMIGPS - MIGration by Phase Shift with turning rays 
\item SUMIGPSPI - Gazdag's phase-shift plus interpolation migration         
for zero-offset data, which can handle the lateral velocity variation.
\item SUMIGSPLIT - Split-step depth migration for zero-offset data.
\end{itemize}

\subsection{SUKDMIG2D, SUMIGTOPO2D, SUDATUMK2DR, SUDATUMK2DS - 2D Kirchhoff Migration,
and Datuming}
\begin{itemize}
\item SUKDMIG2D - Kirchhoff Depth Migration of 2D poststack/prestack data 
\item SUDATUMK2DR - Kirchhoff datuming of receivers for 2D prestack data
(shot gathers are the input)
\item SUDATUMK2DS - Kirchhoff datuming of sources for 2D prestack data
(input data are receiver gathers)
\item SUMIGTOPO2D - Kirchhoff Depth Migration of 2D postack/prestack data
from the (variable topography) recording surface
\end{itemize}

\subsection{SUMIGFD, SUMIGFFD - Finite-Difference Migration}
\begin{itemize}
\item SUMIGFD - 45 and 60 degree Finite difference migration for zero-offset data.
\item SUMIGFFD - Fourier finite difference migration for zero-offset data. This method is a hybrid migration which
combines the advantages of phase shift and finite difference migrations.
\end{itemize}

\subsection{SUMIGTK - Time-Wavenumber Domain Migration}

This algorthm was created by Dave Hale ``on the fly'' and, as
far as we know, exists nowhere in else geophysical literature.

\begin{itemize}
\item SUMIGTK - MIGration via T-K domain method for common-midpoint stacked data
\end{itemize}

\subsection{SUSTOLT - Stolt Migration}
This is the classic F-K migration method of Clayton Stolt.
\begin{itemize}
\item SUSTOLT - Stolt migration for stacked data or common-offset gathers 
\end{itemize}

\chapter{Processing Flows with SU}

\section{SU and UNIX}
You need not learn a special seismic language to use
{\small\sf SU}.  If you know how
to use UNIX shell-redirecting and pipes, you are ready to start
using {\small\sf SU}---the seismic commands and options can be used just as you
would use the built-in UNIX commands.  In particular, you
can write ordinary UNIX shell scripts to combine frequent
command combinations into meta-commands (i.e., processing flows).
These scripts can be thought of as ``job files.''

\begin{table}[htbp]
\label{SU:tab:unix}
\caption{UNIX Symbols}
\begin{tabular}{||l||l||}  \hline\hline
process1 $<$ file1 & process1 takes input from file1 \\
process2 $>$ file2 & process2 writes on (new) file2 \\
process3 $>>$ file3 & process3 appends to file3  \\
process4 $|$ process5 & output of process4 is input to process5  \\
process6 $<<$ text & take input from following lines  \\ \hline \hline
\end{tabular}
\end{table}

So let's begin with a capsule review of the basic UNIX operators
as summarized in Table~\ref{SU:tab:unix}.
The symbols $<$, $>$, and $>>$ are known as ``redirection operators,''
since they redirect input and output into or out of the command
(i.e., process).
The symbol $|$ is called a ``pipe,'' since we can picture
data flowing from one process to another through the ``pipe.''
Here is a simple {\small\sf SU} ``pipeline'' with input ``indata'' and
output ``outdata'':

{\small\begin{verbatim}
sufilter f=4,8,42,54 <indata |
sugain tpow=2.0 >outdata
\end{verbatim}}\noindent
This example shows a band-limiting operation being ``piped'' into
a gaining operation.  The input data set \verb:indata: is directed into
the program {\bf sufilter\/} with the \verb:<: operator, and similarly, the output data set \verb:outdata: receives the data because of the \verb:>: operator.
The output of {\bf sufilter\/} is connected to the input of {\bf sugain\/} by use of the \verb:|: operator.

\label{SU:page:getpar}The strings with the \verb:=: signs illustrate
how parameters are passed to {\small\sf SU} programs.  The program {\bf sugain\/}
receives the assigned value 2.0 to its parameter \verb:tpow:, while
the program {\bf sufilter\/} receives the assigned four component {\em vector}
to its parameter \verb:f:.  To find out what the valid parameters are
for a given program, we use the self-doc facility.

By the way, space around the UNIX
redirection and pipe symbols is optional---the example shows
one popular style.  On the other hand, spaces around the \verb:=:
operator are {\em not} permitted.

The first four symbols in
Table~\ref{SU:tab:unix} are the basic grammar of UNIX;
the final $<<$ entry
is the symbol for the less commonly used ``here document'' redirection.
Despite its rarity in interactive use,
{\small\sf SU} shell programs are significantly enhanced by
appropriate use of the $<<$ operator---we will illustrate this below.

Many built-in UNIX commands do not have a self-documentation
facility like {\small\sf SU}'s---instead, most do have ``man'' pages.
For example,

{\small\begin{verbatim}
% man cat

CAT(1)              UNIX Programmer's Manual               CAT(1)



NAME
     cat - catenate and print

SYNOPSIS
     cat [ -u ] [ -n ] [ -s ] [ -v ] file ...

DESCRIPTION
     Cat reads each file in sequence and displays it on the stan-
     dard output.  Thus

                    cat file

     displays the file on the standard output, and

                    cat file1 file2 >file3
--More--
\end{verbatim}}\noindent
You need to know a bit more UNIX lore
to use {\small\sf SU} efficiently---we'll introduce these tricks of the trade in
the context of the examples discussed later in this chapter.



\section{Understanding and using SU shell programs}
The essence of good {\small\sf SU} usage is constructing (or cloning!)
UNIX shell programs to create and record processing flows.
In this section, we give some
annotated examples to get you started. 
\subsection{A simple SU processing flow example\label{SU:sec:Plotshell}}
Most {\small\sf SU} programs read from standard input and write to standard output.
Therefore, one can build complex processing flows by simply
connecting {\small\sf SU} programs with UNIX pipes.
Most flows will end with one of the {\small\sf SU} plotting programs.
Because typical processing flows are lengthy and involve many
parameter settings, it is convenient to put the {\small\sf SU} commands in a
shell file.

{\bf Remark}: All the UNIX shells, Bourne (sh), Cshell (csh),
Korn (ksh), \ldots, include a programming language.  In this document,
we exclusively use the Bourne shell programming language.

Our first example is a simple shell program called {\bf Plot}.
The numbers in square brackets at the
end of the lines in the following listing are not part of the
shell program---we added them as keys to the discussion
that follows the listing.

{\small\begin{verbatim}
#! /bin/sh                                              [1]
# Plot:   Plot a range of cmp gathers
# Author: Jane Doe
# Usage:  Plot cdpmin cdpmax

data=$HOME/data/cmgs                                    [2]

# Plot the cmp gather.
suwind <$data key=cdp min=$1 max=$2 |                   [3]
sugain tpow=2 gpow=.5 |
suximage f2=0 d2=1 \                                    [4]
        label1="Time (sec)" label2="Trace number" \
        title="CMP Gathers $1 to $2" \
        perc=99 grid1=solid &                           [5]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item The symbol \verb:#: is the comment symbol---anything on the remainder
of the line is not executed by the UNIX shell.  The combination
\verb:#!: is an exception to this rule: the shell uses the
file name following
this symbol as a path to the program that is to execute the remainder
of the shell program.

\item The author apparently intends that the shell be edited
if it is necessary to change the data set---she made this easier to
do by introducing the shell variable \verb:data: and assigning
to it the full pathname of the data file.  The assigned value
of this parameter is accessed as \verb:$data: within the shell program.
The parameter \verb:$HOME: appearing as the first component of the file
path name is a UNIX maintained environment variable
containing the path of the user's home directory.  In general,
there is no need for the data to be located in the user's home
directory, but the user would need ``read permission'' on the
data file for the shell program to succeed.

{\bf WARNING!\/}  Spaces are significant to the UNIX shell---it  uses
them to parse command lines.  So despite all we've learned about
making code easy to read, do {\em not} put spaces next to the \verb:=: symbol.
(Somewhere around 1977, one author's (Jack) first attempt to learn UNIX was
derailed for several weeks by making this mistake.)

\item The main pipeline of this shell code selects a certain set of cmp gathers with {\bf suwind}, gains this subset with {\bf sugain\/} and pipes the result into
the plotting program {\bf suximage}.  As indicated in the Usage comment,
the cmp range is specified by command line arguments.
Within the shell program, these arguments are
referenced as \verb:$1:, \verb:$2: (i.e., first argument, second argument).

\item The lines within the {\bf suximage\/} command are continued by the
backslash escape character.

\noindent{\bf WARNING!\/}  The line continuation backslash must be the {\em final}
character on the line---an invisible space or tab following the
backslash is one of the most common and frustrating bugs in UNIX
shell programming.

\item The final \verb:&: in the shell program
puts the plot window into ``background'' so we can continue
working in our main window.  This is the X-Windows
usage---the \verb:&: should {\em not} be used with the analogous PostScript
plotting programs (e.g., supsimage).  For example, with {\bf supsimage\/} in
place of {\bf suximage}, the \verb:&: might be replaced by \verb:| lpr:.

The {\small\sf SU} plotting programs are special---their self-doc doesn't
show all the parameters accepted.  For example, most of the parameters
accepted by {\bf suximage\/}
are actually specified in the self-documentation for the
generic {\small\sf CWP} plotting program {\bf ximage}.  This apparent flaw
in the self-documentation is actually a side
effect of a key {\small\sf SU} design decision.  The {\small\sf SU} graphics
programs call on the generic plotting programs to do the actual plotting.
The alternative design was to have tuned graphics programs
for various seismic applications.
Our design choice keeps things simple,
but it implies a basic limitation in {\small\sf SU}'s graphical capabilities.

The plotting programs are the vehicle for presenting your results.
Therefore you should take the time to carefully look
through the self-documentation for {\em both} the ``{\small\sf SU} jacket'' programs
({\bf suximage}, {\bf suxwigb}, \ldots) and the generic plotting
programs ({\bf ximage}, {\bf xwigb}, \ldots).

\end{enumerate}

\subsection{Executing shell programs}
The simplest way to execute a UNIX shell program is to give
it ``execute permission.''  For example, to make our above {\bf Plot} shell
program executable:
{\small\begin{verbatim}
chmod +x Plot
\end{verbatim}}\noindent
Then to execute the shell program:
{\small\begin{verbatim}
Plot 601 610
\end{verbatim}}\noindent
Here we assume that the parameters \verb:cdpmin=601:, \verb:cdpmax=610: are
appropriate values for the \verb:cmgs: data set.
Figure~\ref{fig:Plot} shows an output generated by the \verb:Plot: shell
program.
\begin{figure}[htbp]
\epsfysize 280pt
\centerline{\epsffile{Plot.eps}}
\caption{Output of the \protect\verb:Plot: shell program.}
\label{fig:Plot}
\end{figure}


\subsection{A typical SU processing flow\label{SU:sec:Dmoshell}}
Suppose you want to use {\bf sudmofk}.  You've read the self-doc, but
a detailed example is always welcome isn't it?  The place to look is
the directory {\bf su/examples}.  In this case, we are lucky and find
the shell program, {\bf Dmo}.  Again, the numbers in square brackets at the
end of the lines shown below are {\em not} part of the listing.
{\small\begin{verbatim}
#! /bin/sh
# dmo
set -x                                                            [1]

# set parameters
input=cdp201to800                                                 [2]
temp=dmocogs
output=dmocmgs
smute=1.7
vnmo=1500,1550,1700,2000,2300,2600,3000                           [3]
tnmo=0.00,0.40,1.00,2.00,3.00,4.00,6.00


# sort to common-offset, nmo, dmo, inverse-nmo, sort back to cmp
susort <$input offset cdp |                                       [4]
sunmo smute=$smute vnmo=$vnmo tnmo=$tnmo |                        [5]
sudmofk cdpmin=201 cdpmax=800 dxcdp=13.335 noffmix=4 verbose=1 |  [6]
sunmo invert=1 smute=$smute vnmo=$vnmo tnmo=$tnmo >$temp          [7]
susort <$temp cdp offset >$output                                 [8]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

The core of the shell program (lines 5-7) is recognized as the typical
dmo process: crude nmo, dmo, and then ``inverse'' nmo.
The dmo processing is surrounded by sorting operations
(lines 4 and 8).  Here is a detailed discussion of the shell program
keyed to the numbers appended to the listing (see also the discussion
above for the \verb:Plot: shell):

\begin{enumerate}
\item Set a debugging mode that asks UNIX
to echo the lines that are executed.  You can comment
this line off when its output is no longer of interest.  An
alternate debugging flag is \verb:set -v: which echos
lines as they are read by the shell interpreter.  You
can use both modes at once if you like.

\item This line and the next two lines set filenames that,
in this case, are in the same directory as the shell program itself.
Again, the reason for using parameters here is to make it easy
to ``clone'' the shell for use with other data sets.
Those of us who work with only a few data sets at any given time,
find it convenient to devote a directory to a given data set and
keep the shells used to process the data in that directory as
documentation of the processing parameters used.  ({\small\sf SU} does not have
a built-in ``history'' mechanism.)

\item The dmo process requires a set of velocity-time picks for
the subsidiary nmo processes.  Because these picks must be consistent
between the nmo and the inverse nmo, it is a good idea to make them
parameters to avoid editing mistakes.  Again, note the format
of {\small\sf SU} parameter vectors: comma-separated strings with no spaces.
The nmo program ({\bf sunmo}) will give an error message and abort
if the \verb:vnmo: and \verb:tnmo: vectors have different lengths.

\item Note that {\bf susort} allows the use of {\em secondary}
sort keys.  Do not assume that a secondary field that is
initially in the ``right'' order will remain in that order
after the sort---if you care about the order of some secondary
field, specify it (as this shell program does). In this line,
we sort the data according to increasing offsets and then, within
each offset, we sort according to increasing cdp number.

\item The forward nmo step.

\item The dmo step.

\item The inverse nmo step.

\item Sort back to cdp and have increasing offset within each cdp.
\end{enumerate}

If you want to thoroughly understand this shell program, your next
step is to study the self-docs of the programs involved:

{\small\begin{verbatim}
% sunmo

SUNMO - NMO for an arbitrary velocity function of time and CDP

sunmo <stdin >stdout [optional parameters]

Optional Parameters:
vnmo=2000         NMO velocities corresponding to times in tnmo
tnmo=0            NMO times corresponding to velocities in vnmo

...
\end{verbatim}}\noindent
Related shell programs are {\bf su/examples/Nmostack} and
{\bf su/examples/Mig}.

\section{Extending SU by shell programming}
Shell programming can be used to
greatly extend the reach of {\small\sf SU} without writing C code.
See, for example, {\bf CvStack}, {\bf FilterTest}, {\bf FirstBreak}, and
{\bf Velan} in {\bf su/examples}.

It is a sad fact that the UNIX shell is not
a high level programming language---consequently, effective shell
coding often involves arcane tricks.  In this section, we'll
provide some useful templates for some of the
common UNIX shell programming idioms.

We use {\bf CvStack} as an
illustration.  The core of this shell is a
double loop over velocities and cdps that produces
{\em velocity panels}---a concept
not contained in any single {\small\sf SU} program.

{\bf Remark}:  For most of us,
writing a shell like {\bf CvStack} from scratch is a time-consuming affair.
To cut down the development time,
your authors excerpt from existing shells to make new ones
even when we don't quite remember what every detail means.
We suggest that you do the same!

We won't comment on the lines already explained in our previous
two shell code examples
(see Sections~\ref{SU:sec:Plotshell} and~\ref{SU:sec:Dmoshell}),
but instead focus on the new features used in {\bf CvStack}.

{\small\begin{verbatim}
#! /bin/sh
# Constant-velocity stack of a range of cmp gathers
# Authors: Jack, Ken
# NOTE: Comment lines preceding user input start with  #!#
set -x

#!# Set input/output file names and data parameters
input=cdp601to610
stackdata=cvstack
cdpmin=601 cdpmax=610
fold=30
space=1         # 1 null trace between panels

#!# Determine velocity sampling.
vmin=1500   vmax=3000   dv=150

### Determine ns and dt from data (for sunull)
nt=`sugethw ns <$input | sed 1q | sed 's/.*ns=//'`                [1]
dt=`sugethw dt <$input | sed 1q | sed 's/.*dt=//'`

### Convert dt to seconds from header value in microseconds
dt=`bc -l <<END                                                   [2]
        scale=4
        $dt / 1000000
END`


### Do the velocity analyses.
>$stackdata  # zero output file                                   [3]
v=$vmin
while [ $v -le $vmax ]                                            [4]
do
        cdp=$cdpmin
        while [ $cdp -le $cdpmax ]                                [5]
        do
                suwind <$input \                                  [6]
                        key=cdp min=$cdp max=$cdp count=$fold |
                sunmo cdp=$cdp vnmo=$v tnmo=0.0 |
                sustack >>$stackdata
                cdp=`bc -l <<END                                  [7]                               
                        $cdp + 1
END`
        done
        sunull ntr=$space nt=$nt dt=$dt >>$stackdata              [8]
        v=`bc -l <<END
                $v + $dv
END`
done


### Plot the common velocity stacked data
ncdp=`bc -l <<END
        $cdpmax-$cdpmin+1
END`
f2=$vmin
d2=`bc -l <<END
        $dv/($ncdp + $space)                                      [9]
END`

sugain <$stackdata tpow=2.0 |

suximage perc=99 f2=$f2 d2=$d2 \
        title="File: $input  Constant-Velocity Stack " \
        label1="Time (s)"  label2="Velocity (m/s)" & 

exit                                                              [10]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item This elaborate construction gets some information
from the first trace header of the data set.  The program {\bf sugethw}
lists the values of the specified keys in the successive traces.  For
example,
{\small\begin{verbatim}
% suplane | sugethw tracl ns
 tracl=1            ns=64       

 tracl=2            ns=64       

 tracl=3            ns=64       

 tracl=4            ns=64       

 tracl=5            ns=64       

 tracl=6            ns=64    
   
 ...
\end{verbatim}}\noindent
Although {\bf sugethw} is eager to give the values for every trace in the
data set, we only need it once.  The solution is to use the UNIX stream
editor ({\bf sed}).  In fact, we use it twice.  By default, {\bf sed} passes
along its input to its output.  Our first use is merely to tell {\bf sed}
to quit after it puts the first line in the pipe.  The second pass through
{\bf sed} strips off the unwanted material before the integer.
In detail, the second {\bf sed} command reads: replace (or substitute)
everything up to the characters \verb:ns=: with nothing, i.e., delete
those characters.


\item We are proud of this trick.
The Bourne shell does not provide floating point
arithmetic.  Where this is needed, we use the UNIX built-in
{\bf bc} calculator program with the ``here document'' facility.
Here, we make the commonly needed conversion of sampling interval which
is given in micro-seconds in the {\sf SEG-Y} header,
but as seconds in {\small\sf SU} codes.  Note carefully the {\em back}quotes
around the entire calculation---we assign the result of this
calculation to the shell variable on the left of the equal sign,
here \verb:dt:.  The calculation may take several lines.
We first set the number of decimal places with \verb:scale=4:
and then do the conversion to seconds.  The characters \verb:END:
that follow the here document redirection symbol \verb:<<: are arbitrary,
the shell takes its input from the text in the shell file
until it comes to a line that contains the same
characters again.  For more information about {\bf bc}:
{\small\begin{verbatim}
% man bc
\end{verbatim}}\noindent

\item As the comment indicates, this is a special use of the output
redirection symbol that has the effect of destroying any pre-existing
file of the same name or opening a new file with that name.  In fact,
this is what \verb:>: always does as its first action---it's a dangerous
operator!  If you intend to {\em append}, then, as mentioned earlier, use
\verb:>>:.

\item This is the outer loop over velocities.
Another warning about spaces---the spaces around the bracket
symbols are essential.

{\bf Caveat}: The bracket notation is a nice
alternative to the older clunky \verb:test: notation:
{\small\begin{verbatim}
while test $v -le $vmax
\end{verbatim}}\noindent
Because the bracket notation is not documented on the typical {\bf sh} manual
page, we have some qualms about using it.  But, as far as we know,
all modern {\bf sh} commands support it---please let us know
if you find one that doesn't.

{\bf WARNING!}  OK, now you know that there is a UNIX command
called \verb:test:.  So don't use the name ``test'' for one of your
shell (or C) programs---depending on your \verb:$PATH: setting, you could
be faced with seemingly inexplicable output.

\item This is the inner loop over cdps.

\item Reminder: No spaces or tabs after the line continuation
symbol!

\item Notice that we broke the nice indentation structure by
putting the final \verb:END: against the left margin.  That's because
the {\bf sh} manual page says that the termination should contain
only the \verb:END: (or whatever you use).  In fact, most versions
support indentation.  We didn't think the added beautification was
worth the risk in a shell meant for export.  Also note that we used
{\bf bc} for an integer arithmetic calculation even though
integer arithmetic is built into the Bourne shell---why learn
two arcane rituals, when one will do?  See \verb:man expr:, if
you are curious.
\begin{figure}[htbp]
\epsfysize 300pt
\centerline{\epsffile{CvStack.eps}}
\caption{Output of the \protect\verb:CvStack: shell program.}
\label{fig:cvstack}
\end{figure}

\item {\bf sunull} is a program I (Jack) wrote to create all-zero traces
to enhance displays of the sort produced by \verb:CvStack:.
Actually, I had written this program many times, but this was the first
time I did it on purpose.  (Yes, that was an attempt at humor.)

\item An arcane calculation to get velocity labeling
on the trace axis.  Very impressive!  I wonder what it means?
(See last item.)

\item The \verb:exit: statement is useful because you might want
to save some ``spare parts'' for future use.  If so, just put them
after the \verb:exit: statement and they won't be executed.
\end{enumerate}

\noindent Figure~\ref{fig:cvstack} shows an output generated by \verb:CvStack:.

\chapter{Answers to Frequently Asked Questions}

This chapter addresses questions often asked by new {\small\sf SU} users.
Some answers refer to the directory {\tt CWPROOT}.  We use this
symbolic name for the directory that contains the {\sf CWP/SU} source code, include files, libraries, and executables.  You are asked to specify
this directory name during the {\small\sf SU} installation procedure.

\section{Installation questions}
Complete information about the installation
process is found in the {\sf README}
files supplied with the distribution.
Here we discuss only some commonly found installation problems.

\begin{question}
I get error messages about missing {\tt fgetpos} and {\tt fsetpos} routines, 
even though I am using the {\sf GCC} compiler.
How do I get around this problem?
\end{question}

\begin{rmans}
We've seen this problem most often with older 
{\sf SUN OS} 4.xx (pre-{\sf SOLARIS}). 
These {\sf SUN} systems may not have the {\tt fgetpos} and {\tt fsetpos} subroutines defined.
Because these two routines are not currently used in the {\small\sf SU} package,
we have modified the installation process to permit the
user to define a compile-time flag to circumvent this problem.
Please uncomment the {\sf OPTC} line in the paragraph in Makefile.config
that looks like this:
\begin{verbatim}
# For SUN installing with GCC compiler but without GLIBC libraries
#OPTC = -O -DSUN_A -DSUN
\end{verbatim}
and do a "make remake".
\end{rmans}

\begin{question}
I get error messages regarding missing
{\tt strtoul}, and/or {\tt strerror} routines, even though I am using
the {\sf GCC} compiler. How do I get around this problem?
\end{question}

\begin{rmans}
Again, this is  most often seen with the older {\sf SUN OS}. 
The fix is the same as for the previous question.
\end{rmans}

\begin{question}
\label{SU:q:gcc}
Why do I get missing subroutine messages about {\sf ANSI C} routines?
Isn't the {\sf GCC} compiler supposed to be an {\sf ANSI} compiler?
\end{question}

\begin{rmans}
The {\sf GCC} compiler is just that, a compiler. It
draws on the libraries that are present on the machine.
If the {\sf GNU} libraries (this is the "glibc" package)
have not been installed, then the
{\sf GCC} compiler will use the libraries that are native to the machine
you are running on. Because the four routines listed above are
not available in the {\sf SUN 4. OS}, {\sf GCC} does not recognize them.
However, installing the {\sf GNU} libraries will make the {\sf GCC} compiler
behave as a full {\sf ANSI C} compiler.
\end{rmans}

\begin{question}
\label{SU:q:bugs}
Why do I get missing subroutine messages about {\sf ANSI C} routines?
I can't get the code to compile because my compiler can't find
"bzero" or "bcopy", how can  I fix this?
\end{question}

\begin{rmans}
You really shouldn't be having this problem, because
we try to keep to the ANSI standard, but sometimes old
style function calls creep in. The problem of rooting these things out
is exacerbated because many systems still support the old style calls.

If you have trouble installing
because your compiler can't find "bcopy" or "bzero"
make the following replacements.

Replace all statements of the form

{\small \begin{verbatim}
bzero( a, b);
\end{verbatim}} \noindent

with statements of the form:

{\small \begin{verbatim}
memset( (void *) a , (int) '\0', b );
\end{verbatim}} \noindent

Please replace all instances
of statements of the form of:
{\small \begin{verbatim}
bcopy ( a , b, c);
\end{verbatim}} \noindent
with a statements of the form:
{\small \begin{verbatim}
memcpy( (void *) b, (const void *) a, c );
\end{verbatim}} \noindent
\end{rmans}

\section{Data format questions}

In this section, we address questions about converting data
that are in various formats into {\small\sf SU} format.

\begin{question}
What is the data format that {\small\sf SU} programs expect?
\end{question}

\begin{rmans}
The {\small\sf SU} data format is based on, (but is not exactly the same as)
the {\sf SEG-Y} format. The {\small\sf SU} format
consists of data traces each of which has a header.
The {\small\sf SU} trace header is identical to {\sf SEG-Y} trace header.
Both the header and the trace data are written in the
native binary format of your machine.
You will need to use {\bf segyread\/} to convert SEGY data to SU data.

\noindent{\bf Caution}: The optional fields
in the {\sf SEG-Y} trace header are used for different purposes
at different sites.  {\small\sf SU} itself makes use of certain of these fields.
Thus, you may need to use {\tt segyclean}---see the answer to
Question~\ref{SU:q:segyclean}.
{\small\sf SU} format does not have the binary and ebcdic tape headers that
are part of the {\sf SEG-Y} format.

After installing the package, you can get more information on the
{\sf SEG-Y}/{\small\sf SU} header by typing: 
{\small \begin{verbatim}
% sukeyword -o
\end{verbatim}}\noindent
This lists the include file {\tt segy.h} that defines the {\small\sf SU} trace header.
\end{rmans}

\begin{question}
Is there any easy way of adding necessary 
{\sf SEG-Y} information to our own modeled data to prepare
our data for processing using the {\small\sf SU} package?
\end{question}

\begin{rmans}
It depends on the details of how your data was written to the file:
\begin{enumerate}
\item If you have a `datafile'
that is in the form of binary floating point numbers of the type
that would be created by a C program, then use {\tt suaddhead} to
put {\small\sf SU} ({\sf SEG-Y}) trace headers on the data. Example:
{\small \begin{verbatim}
% suaddhead < datafile  ns=N_SAMP > data.su
\end{verbatim}}\noindent
Here, \verb:N_SAMP: is the (integer) number of samples per
trace in the data.

\item If your data are Fortran-style floats, then you would use:
{\small \begin{verbatim}
% suaddhead < datafile ftn=1 ns=NS > data.su
\end{verbatim}}\noindent
See also, Question~\ref{SU:q:fortran}.

\item If your data are {\sf ASCII}, then use:
{\small \begin{verbatim}
% a2b n1=N1 < data.ascii | suaddhead ns=NS > data.su
\end{verbatim}}\noindent
Here \verb:N1: is the number of floats per line in the file
{\tt data.ascii}.

\item If you have some other data type, then you may use:
{\small \begin{verbatim}
% recast < data.other in=IN out=float | suaddhead ns=NS > data.su
\end{verbatim}}\noindent
where \verb:IN: is the type (int, double, char, etc...) 
\end{enumerate}

\noindent
For further information, consult the self-docs of the programs
{\tt suaddhead}, {\tt a2b}, and~{\tt recast}.
\end{rmans}

\begin{question}
\label{SU:q:segyclean}
I used {\tt segyread} to read a {\sf SEG-Y} tape.
Everything seems to work fine,
but when I plot my data with suximage, the window is black.
What did I do wrong?
\end{question}

\begin{rmans}
When you read an {\sf SEG-Y} tape, you need to pipe the data through
{\tt segyclean} to zero the optional {\sf SEG-Y} trace header field.
If the {\small\sf SU} programs see nonzero values in certain parts
of the optional field, they try
to display the data as ``nonseismic data,'' using those values
to set the plot parameters.

Another possibility is that there are a few data values that are so
large that they are overwhelming the 256 gray scale levels in the
graphics.
The way to get around this problem is to set {\bf perc=99} in the
graphics program. For example:
{\small \begin{verbatim}
% suximage < sudata  perc=99 &
\end{verbatim}} \noindent This will clip data values with size in
the top 1 percentile of the total data.
\end{rmans}

\begin{question}
I am trying to plot data with the {\tt pswigb}
(or {\tt pswigp}, or {\tt xwigb}, or  \ldots)
program.  I know that I have data with
\verb:n1=NSAMP: and \verb:n2=NTRACES:,
but when I plot, I find that I have to set \verb:n1=NSAMP+60: for the plot
to look even remotely correct. Why is this?
\end{question}

\begin{rmans}
It is likely that you are trying to plot with the wrong tool.
The input data format of the programs,
{\tt pswigb}, {\tt pswigp}, {\tt pscontour}, {\tt pscube}, {\tt psmovie},
{\tt xwigb}, {\tt xgraph}, and~{\tt xmovie},
expect data to consist of simple floating point numbers.
If your data are {\small\sf SU} data ({\sf SEG-Y}) traces,
then there is an additional
header at the beginning of each trace,
which, on most computer architectures,
is the same number (240) of bytes
as the storage for 60 floats.
 
\sloppypar{To plot these data, use respectively:
{\tt supswigb}, {\tt supswigp}, {\tt supscontour}, {\tt supscube},
{\tt supsmovie}, {\tt suxwigb}, {\tt suxgraph}, or~{\tt suxmovie}.}

Also, it is not necessary to specify the dimensions of the data for these
latter programs.  The {\tt su}-versions of the codes determine
the necessary information from the appropriate header values.
(In fact, that is {\em all} they do---the actual graphics is
handled by the version without the {\tt su} prefix.)
\end{rmans}

\begin{question}
I want to check the size of a file to see if it has the right number
of values, but I am not sure how to take the header into account.
How is this done?
\end{question}

\begin{rmans}
If the file consists of simple floating point numbers, then the
size in bytes equals the size of a float times the number of
samples (\verb:SIZE = 4 * N_SAMP:).
The {\small\sf SU} data ({\sf SEG-Y} traces)
also have a header (240 bytes per trace)
giving the total number of bytes as:\\
\verb:(240 + 4 N_SAMP ) N_TRACES:.\\
\noindent
The byte count computed in this way
is the number that the UNIX command {\tt ls -l} shows.

{\bf Caveats}: The above calculations assume that you have
the conventional architecture and that the header definition
in {\tt segy.h} has not been altered.  Watch out as machines
with 64 bit word size become common!
\end{rmans}

\begin{question}
\label{SU:q:fortran}
I have some data in Fortran form and tried to convert it to {\small\sf SU} data
via the following:
{\small \begin{verbatim}
% suaddhead < data.fortran ns=N_SAMP ftn=1 > data.su
\end{verbatim}}\noindent
but this did not work properly. I am sure that my fortran data
are in unformatted binary floats. What should I do?
\end{question}

\begin{rmans}
There are different ways of interpreting the term ``unformatted''
with regard to fortran data.  Try:
{\small \begin{verbatim}
% ftnstrip < data.fortran | suaddhead ns=N_SAMP > data.su
\end{verbatim}}\noindent

The program {\tt ftnstrip} can often succeed in converting
your fortran data into C-like binary data, even when the
\verb:ftn=1: option in {\tt suaddhead} fails.
(Note: the program {\bf ftnunstrip\/} may be used to take C-style
binary data and convert it to Fortran form.)
\end{rmans} 

\begin{question}
I just successfully installed the {\sf CWP/SU} package, but when I
try to run the demo scripts, I get many error messages describing
programs that the shell script cannot find. How do I fix this?
\end{question}

\begin{rmans}
You need to put {\tt CWPROOT/bin} (where {\tt CWPROOT}
is {\tt /your/root/path} that
contains the {\sf CWP/SU} source code, include files,
libraries, and executables)
in your shell {\tt PATH}. This is done in your {\tt .cshrc} file
if you run under
{\tt csh} or {\tt tcsh}.
In Bourne shell ({\tt sh}), Born Again shell ({\tt bash}), or Korn shell
({\tt ksh}) the {\tt PATH} variable is in your {\tt .profile} file.
You also need
to type
{\small\begin{verbatim}
% rehash
\end{verbatim}}\noindent
if you are running C-shell {\tt /bin/csh} or  TC-shell {\tt /bin/tcsh}
as your working shell environment, if you have not relogged since 
you compiled the codes. 
\end{rmans} 

\begin{question}
How do I transfer data between {\small\sf SU} and a commercial package, such
as Promax.
\end{question}

\begin{rmans}
The short answer is that you make a SEGY tape on disk file.
To do convert a file called, say, "data.su" to a segy file
do the following:

{\small \begin{verbatim}
% segyhdrs < data.su
% segywrite tape=data.segy < data.su
\end{verbatim}} \noindent

Now use Promax to read data.segy. This file is a
"Promax tape-on-disk file in IBM Real format."
Choose Promax menus accordingly.

\noindent For other commercial packages, use the appropriate
commands to read a SEGY tape on disk file.

\noindent To go from the commercial package to {\small\sf SU} follow
the reverse steps. Create a file that is a SEGY tape image
and then use 
{\small \begin{verbatim}
% segyread tape=data.segy | segyclean > data.su
\end{verbatim}} \noindent
\end{rmans}
 
\begin{question}
I would like to strip the trace headers off of some SU data, perform
an operation of some type on the bare traces and put the headers
back on without losing any of the header information. How do I do this?
\end{question}

\begin{rmans}
Do the following:

{\small \begin{verbatim}
% sustrip < data.su head=headers > data.binary
\end{verbatim}} \noindent

(Do whatever was desired to data.binary to make data1.binary)

{\small \begin{verbatim}
% supaste < data1.binary head=headers > data1.su
\end{verbatim}} \noindent
\end{rmans}

\begin{question}
I have made some data on an IBM RS6000 and have transferred it to
my Linux-based PC system. The data looks ok on the RS6000,
but when I try to work with it on the PC, none of the SU programs seem
to work. What is wrong?
\end{question}

\begin{rmans}
The problem you have encountered is that there are two IEEE binary
formats called respectively `big endian` and `little endian` or,
alternately `high byte` and `low byte`. These terms refer to the
order of the bytes that represent the data. IBM RS6000, Silicon
Graphics, NeXT (black hardware), SUN, HP, PowerPC, any Motorola
chip-based platforms are `big endian` machines, whereas, Intel-based
PCs and Dec and Dec Alpha products are `little endian` platforms.

Two programs are supplied in the CWP/SU package for swapping
the bytes for data transfer. These are  {\bf swapbytes} and {\bf suswapbytes}.

The program {\bf swapbytes} is designed to permit the user to swap
the bytes on binary data that are all one type of data (floats, doubles,
shorts, unsigned shorts, longs, unsigned longs, and ints).

For data that are in the {\small\sf SU} format, the program {\bf suswapbytes} is
provided.

Furthermore, within the programs {\bf segyread} and {\bf segywrite}
there are ``swap='' flags that permit the user to specify whether
the platform they are working on are ``big endian'' or ``little endian''
platforms.

In older releases of {\small\sf SU} there were problems with the bitwise operations
that would be encountered in the wiggle-trace drawing routines. However,
these problems have been fixed via the ENDIANFLAG that appears in
Makefile.config.
\end{rmans}

\begin{question}
How do I convert data that are in the SEG-2 format to SEGY?
\end{question}

\begin{rmans}
In \$CWPROOT/src/Third\_Party/seg2segy   there are two programs
that have been made available to us by the University of Pau
in France, for this purpose. These should be easy to install
on any system where {\small\sf SU} has been installed.

Once you have converted   data.seg2  to  data.segy, you may
read it into the {\small\sf SU} format via:

{\small \begin{verbatim}
% segyread tape=data.segy > data.su
\end{verbatim}} \noindent
\end{rmans}

\section{Tape reading and writing}
This section contains frequently asked questions about reading
and writing {\sf SEG-Y} tapes with {\small\sf SU}.

\noindent Tape reading/writing is more of an art than a science.
Here are a few tips. 
\begin{enumerate}
\item Make sure your tape drive is set to be variable block
    length. If you are on an {\sf IBM RS6000}, this means you
    will need to use {\tt smit} to set {\tt blocksize=0} on your tape
    device. Having the tape drive set to some default
    constant blocksize (say blocksize=1024 or 512)
    will foil all attempts to read an {\sf SEG-Y} tape.
\item To read multiple tape files on a tape, use the non
     rewinding device. On an {\tt RS6000} this would be
      something like {\tt /dev/rmtx.1}, see {\tt man mt} for details.
\item If this still doesn't work, then try:
{\small \begin{verbatim}
% dd if=/dev/rmtx of=temps bs=32767 conv=noerror
\end{verbatim}}\noindent
Here, {\tt /dev/rmtx} (not the real name of the device,
it varies from system
to system) is your regular (rewinding) tape device.
In the option, {\tt bs=32767}, we gave the right blocksize ($2^{16}+1$)
for an {\tt IBM/RS6000}.  Try
\verb:bs=32765:  ($2^{16}-1$) on a {\sf SUN}. 
This will dump the entire contents of the tape onto
a single file.
\end{enumerate}


\begin{question}
How do I write multiple SEG-Y files onto a tape?
\end{question}


\begin{rmans}
Here is a shell script for writing multiple files on a tape:
{\small \begin{verbatim}
#! /bin/sh

DEV=/dev/nrxt0  # non rewinding tape device

mt -f $DEV rewind

j=0
jmax=40

while test "$j" -ne "$jmax"
do
        j=`expr $j + 1`
        echo "writing tape file  $j"
        segywrite tape=$DEV bfile=b.$j hfile=h.$j verbose=1 buff=0 < ozdata.$j
done

exit 0
\end{verbatim}}\noindent
\end{rmans}

\section{Geometry Setting}
\begin{question}
How do I do ``geometry setting'' in SU?
\end{question}
\begin{rmans}

There is a common seismic data manipulation task that often is 
called "geometry setting" in commercial packages in which the
user converts information in the survey observers' logs
into values in the trace headers.

\noindent The CWP/SU package does indeed, have provisions for getting and
setting header fields, as well as computing a third header field
from one or two other header fields. The programs that you need
to use for this are:


\vspace{1ex}
\indent sugethw    ("SU get header word") \\
\indent sushw      ("SU set header word") \\
\indent suchw      ("SU change or compute header word")
\vspace{1ex}

\noindent Type the name of each program to see the self
documentation of that code.

\vspace{1ex}
\noindent In addition, to find out what the header field "keywords"
mentioned in these programs are:  type:    sukeyword -o

\vspace{1ex}
\noindent You may have the information in a variety of forms.
The most common and least complicated assumptions of that form 
will be made here.

\vspace{1ex}
\noindent The task requires the following basic steps.

\vspace{1ex}
\begin{enumerate}
\item Get your data into SU format. The SU format is not exactly SEGY,
   but it does preserve the SEGY header information. If you are
   starting with SEGY data (either on tape, or on in the form of
   a diskfile) then you use "segyread" to read the data into an
   su file format.

   For tape:

{\small \begin{verbatim}
      % segyread tape=/dev/rmt0 bfile=data.1 header=h.1 | segyclean > data.su
\end{verbatim}}\noindent

   For diskfile

{\small \begin{verbatim}
      %  segyread tape=data.segy bfile=data.1 header=h.1 | segyclean > data.su
\end{verbatim}} \noindent
   The file   data.segy is assumed here to be a "tape image" of segy data.
   You have to be careful because some commercial software will write
   SEGY-like data, by mimicking the layout of the SEGY format, but 
   this format will not be in the true IBM tape format that SEGY is defined
   to be.  In Promax, if you write a SEGY file in IBM Real format, then this
   will be true SEGY tape image.
   working on.

\item If you have your data in the SU format, then you may view the
   ranges of the SEGY headers (headers that are not set will not
   be shown) via:
{\small \begin{verbatim}
   % surange < data.su
\end{verbatim}}

\item Data often comes with some fields already set. To dump these
   fields in a format that is convenient for geometry setting,
   you would use    sugethw  in the following way:

{\small \begin{verbatim}
   % sugethw < data.su  output=geom  key=key1,key2,... > hfile.ascii
\end{verbatim}}

   The strings "key1,key2,..." are the keywords representing the desired
   SEGY trace header fields. These keywords may be listed via:

{\small \begin{verbatim}
   % sukeyword -o
\end{verbatim}}

\item Once you have dumped the desired header fields  into  hfile.ascii
   then you may edit them with the editor of your choice. The point
   is that you may create a multi-column ascii file that lists the
   values of specific header fields (trace by trace, as they appear
   in data.su) by *any* method you wish. Each column will contain
   the value of a specific header field to be set.

\item Now that you have created the ascii file containing your header values,
   you may load these values into data.su via:

{\small \begin{verbatim}
   % a2b < hfile.ascii n1=N_columns > hfile.bin
\end{verbatim}} \noindent
   Here,  N\_columns is the number of columns in   hfile.ascii.
   This is to convert hfile.ascii to a binary file.

   Now use:
{\small \begin{verbatim}
   % sushw < data.su key=key1,key2,...  infile=hfile.bin > data1.su
\end{verbatim}} \noindent
   Here   key1,key2,... are the appropriate keywords representing
   the fields being set, listed in the exact order the values appear,
   column by column in hfile.ascii.

\item If you want to compute a third header field from two given header
   field values, then you may use: {\bf suchw} for this.
   Also, if the header fields that you want to set are
   systematic in some way (are constant for each trace or vary
   linearly across a gather), then you don't have to use the
   "infile=" option. You may simply give the
   necessary values to   sushw.   See the selfdocs for   sushw and
   suchw  for examples of these.
\end{enumerate}
\end{rmans}

\section{Technical Questions}
\begin{question}
I want to resample my data so that I have half as many traces, and
half as many samples. How do I do that?
\end{question}

\begin{rmans}
To resample data, you must do the following: 

\begin{enumerate}
\item Check that you won't have aliasing. Do this by viewing the amplitude
spectra of your data. Do this with {\bf suspecfx\/}
{\small \begin{verbatim}
    suspecfx < data.su | suxwigb
\end{verbatim}} \noindent
\item If the bandwidth of your data extends beyond the new nyquist frequency
of your data (which, in this example, will be half of the original nyquist
frequency) then you will have to filter your data to fit within
its new nyquist bandwidth. Do this with {\bf sufilter\/}
{\small \begin{verbatim}
     sufilter < data.su f=f1,f2,f3,f4  amps=0,1,1,0 > data.filtered.su
\end{verbatim}} \noindent
Here, the {\bf f1 f2 f3 f4\/} are the filter corner frequencies and
{\bf amps=0,1,1,0\/} indicate that the filter is a bandpass filter.
\item Now you may resample your data with suresamp:
{\small \begin{verbatim}
    suresamp < data.filtered.su  nt=NTOUT dt=DTOUT > data.resampled.su
\end{verbatim}} \noindent
\end{enumerate}
For your case, NTOUT is 1/2 of the original number of samples, and
DTOUT is twice the time sampling interval (in seconds) of that in
the original data.  Your output data should look quite similar to 
your input data, with the exception that the bandwidth will change.
\end{rmans}

\section{General}
This section addresses general questions about the {\small\sf SU} package.

\begin{question}
What are these funny words gelev, selev, fldr, etc. that I see
in various places?
\end{question}

\begin{rmans}
These are the "keywords" that are required for many of the codes.
They refer to SU (Segy) header fields.

\begin{verbatim}
   Type:   sukeyword -o                to see the whole list 
   Type:   sukeyword keyword           to see the listing for an individual
                                       keyword
\end{verbatim}
\end{rmans}

\begin{question}
What do the terms ``little endian'' and ``big endian'' and  mean?
\end{question}

\begin{rmans}
There are two IEEE binary formats, called respectively
'little endian' and 'big endian'. These are also called
'high byte' and 'low byte', respectively.
These refer to the byte order in the bitwise representation of
binary data. The following platforms are 'little endian': DEC and
Intel-based PC's. The other common platforms are "big endian":
IBM RS6000, Silicon Graphics, NeXT (black hardware), SUN,
HP, PowerPC, any Motorola chip-based platform.
\end{rmans}

\begin{question}
Why are {\sf CWP/SU} releases given by integers (22, 23, 24, etc...)
instead of the more familiar decimal release numbers (1.1, 1.3, etc...)?
\end{question}

\begin{rmans}
The {\sf CWP/SU} release numbers are chosen to correspond
to the {\tt SU NEWS} email messages.
The individual codes in the package have traditional decimal
release numbers (assigned by {\sf RCS}), but these are all different.
The package changes in incremental, but non-uniform ways, so the standard
notation seems inappropriate. However, the user may view 24 to be
2.4. We may adopt this convention in the future.

{\bf Remark}:  In the early days, we {\em did} use {\sf RCS} to
simultaneously update all the codes to 2.1, 3.1, \ldots .  This
practice died a natural death somewhere along the way.
\end{rmans}

\begin{question}
How often are the codes updated?
\end{question}

\begin{rmans}
The {\sf CWP/SU\/} package is updated at roughly 3-6 month intervals.
We mail announcements of these releases to all known users.  Since
we do not provide support for outdated versions, we urge you to remain current.
\end{rmans}

\begin{question}
I have a complicated collection of input parameters for a {\sf CWP/SU}
program.  I want to run the command from the command line of a terminal
window, but I don't want to retype the entire string of input parameters.
What do I do?
\end{question}

\begin{rmans}
{\sf CWP/SU} programs that take their input parameters from the command
line also have the feature of being able to read from a
``parameter file.''   This is invoked by setting
the parameter \verb:par=parfile:, where {\tt parfile} is a file containing
the desired commandline string.

For example:
{\small \begin{verbatim}
suplane ntr=20 nt=40 dt=.001 | ...
\end{verbatim}}\noindent
is completely equivalent to the command:
{\small \begin{verbatim}
suplane par=parfile | ...
\end{verbatim}}\noindent
if the string
{\small \begin{verbatim}
ntr=20 nt=40 dt=.001
\end{verbatim}}\noindent
is contained  in `parfile.'
\end{rmans}

\begin{question}
I can't find an {\bf sudoc\/} entry for the function "ints8r," yet the
SU manual says that all library functions have online documentation?
What am I doing wrong?
\end{question}

\begin{rmans}
The proper search procedure for a library function (such as ints8r) is:
{\small \begin{verbatim}
% sufind ints8r
\end{verbatim}} \noindent
Which yields:

\begin{verbatim}
INTSINC8 - Functions to interpolate uniformly-sampled data via 8-coeff. sinc
                approximations:

ints8c  interpolation of a uniformly-sampled complex function y(x) via an


For more information type: "sudoc program_name <CR>"
\end{verbatim}

The name INTSINC8 is the name of the file that contains the
library function ins8c. You may now use {\bf sudoc\/} to find out more
information via:

{\small \begin{verbatim}
% sudoc intsinc8
\end{verbatim}} \noindent

Which yields:

\begin{verbatim}
In /usr/local/cwp/src/cwp/lib: 
INTSINC8 - Functions to interpolate uniformly-sampled data via 8-coeff. sinc
                approximations:

ints8c  interpolation of a uniformly-sampled complex function y(x) via an
         8-coefficient sinc approximation.
ints8r  Interpolation of a uniformly-sampled real function y(x) via a
                table of 8-coefficient sinc approximations

Function Prototypes:
void ints8c (int nxin, float dxin, float fxin, complex yin[], 
        complex yinl, complex yinr, int nxout, float xout[], complex yout[]);
void ints8r (int nxin, float dxin, float fxin, float yin[], 
        float yinl, float yinr, int nxout, float xout[], float yout[]);

Input:
nxin            number of x values at which y(x) is input
dxin            x sampling interval for input y(x)
fxin            x value of first sample input
yin             array[nxin] of input y(x) values:  yin[0] = y(fxin), etc.
yinl            value used to extrapolate yin values to left of yin[0]
yinr            value used to extrapolate yin values to right of yin[nxin-1]
nxout           number of x values a which y(x) is output
xout            array[nxout] of x values at which y(x) is output

Output:
yout            array[nxout] of output y(x):  yout[0] = y(xout[0]), etc.

Notes:
Because extrapolation of the input function y(x) is defined by the
left and right values yinl and yinr, the xout values are not restricted
to lie within the range of sample locations defined by nxin, dxin, and
fxin.

The maximum error for frequiencies less than 0.6 nyquist is less than
one percent.

Author:  Dave Hale, Colorado School of Mines, 06/02/89

\end{verbatim}
\end{rmans}

\begin{question}
I have written my own SU programs and would like them to appear 
in the {\bf suname\/} and {\bf sudoc\/} listings. How do I do this?
\end{question}

\begin{rmans}
Run {\bf updatedocall\/} (source code located in CWPROOT/par/shell).
If you have put this code under a new path, then you must add
this path to the list of paths in the updatedoc script.
For the selfdoc information to be captured by the updatedoc script,
you will need to have the following marker lines at the beginning
and end of the selfdoc and additional information portion of the 
source code of your program.
\begin{verbatim}
/*********************** self documentation **********************/
/**************** end self doc ********************************/
\end{verbatim}
Be sure to clone these directly out of an existing SU program, rather
than typing them yourself, so that the pattern is the exact one
expected by the updatedoc script.
\end{rmans}

\begin{question}
I have a gray scale (not color) PostScript file made with psimage
and would like to convert it to a color PostScript format, but do
not have the original binary data that I made the file from. How
do I do this?
\end{question}

\begin{rmans}
You have to restore the binary file to make the new color PostScript
file.  Here is how you do it. (Here, we are assuming a bit-mapped
graphic as would be produced by psimage or supsimage).
\begin{enumerate}
\item Make a backup of your PostScript file.
\item edit the PostScript file removing everything but the
    hexidecimal binary image that makes up the majority of
    the file.
\item use    h2b   to convert the hexidecimal file to binary
\item You will find that the file is flipped from the original
    input file.  Use   transp   to flip the data. Note that the
    n1 and n2 values that are used by transp are the dimensions
    of the input data, which are the reverse of the output data.
\item You now have a 0-255 representation of your binary data
    which you should be able to plot again any way you desire.
\end{enumerate}

This method may be used to convert scanned images to {\small\sf SU} format,
as well, with the next step in the procedure to be putting {\small\sf SU}
headers on the data with  {\bf suaddhead}.
\end{rmans}


\chapter{How to Write an SU Program}

\section{Setting up the Makefile}

The CWP/SU package uses a sophisticated Makefile structure, that you may also
use when you develop new code. You should begin any new code writing project by
creating a local directory in your working area.
You should then copy the Makefile from \$CWPROOT/src/su/main into that directory
and make the following changes
\begin{verbatim}
Change:

D = $L/libcwp.a $L/libpar.a $L/libsu.a

LFLAGS= $(PRELFLAGS) -L$L -lsu -lpar -lcwp -lm $(POSTLFLAGS)

to:

D = $L/libcwp.a $L/libpar.a $L/libsu.a

B = .

OPTC = -g

LFLAGS= $(PRELFLAGS) -L$L -lsu -lpar -lcwp -lm $(POSTLFLAGS)

Change:


PROGS =                 \
         $B/bhedtopar    \
         $B/dt1tosu      \
         $B/segyclean    \
         $B/segyhdrs     \
         $B/segyread     \
        ...
         ...

to:

PROGS =                 \
         $B/yourprogram
\end{verbatim}
where the source code of your program is called  ``yourprogram.c'' and resides in
this directory.


You should then  be able to  simply type ``make'' and ``yourprogram'' will be compiled.

As a test you can try copying one of the existing SU programs, from \$CWPROOT/src/su/main
into your local working directory, modifying the Makefile accordingly and typing: make.

Indeed, because all new SU programs may be viewed as beginning as clones of existing SU
programs of a similar structure, this is perhaps the best way to begin any new coding
venture.


\section {A template SU program\label{SU:sec:template}}
Although variations are usually needed, a template for a typical {\small\sf SU} program
looks like the program listing below (we excerpted lines from the program {\tt sumute} to 
build this template).  The numbers in square brackets at the end of the lines in the 
listing are not part of the listing---we added them to facilitate discussion of the 
template.  The secret to efficient {\small\sf SU} coding is finding an existing program 
similar to the one you want to write.  If you have trouble locating the right code or 
codes to ``clone,'' ask us---this can be the toughest part of the job!
 
{\small\begin{verbatim}
/* SUMUTE: $Revision: 1.19 $ ; $Date: 2001/06/06 18:31:42 $      */  [1]

#include "su.h"                                                     [2]
#include "segy.h"

/*********************** self documentation **********************/ [3]
char *sdoc[] = {
"                                                                ",
" SUMUTE - ......                                                ",
"                                                                ",
" sumute <stdin >stdout                                          ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ...                                                     ",
"                                                                ",
" Trace header fields accessed: ns                               ",
" Trace header fields modified: none                             ",
"                                                                ",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *
 *        CWP: Jack Cohen, John Stockwell
 */


segy tr;                                                             [4]

main(int argc, char **argv)
{
        int ns;                /* number of samples          */      [5]
        ...


        /* Initialize */                 
        initargs(argc, argv);                                        [6]
        requestdoc(1);                                               [7]

        /* Get parameters */
        if (!getparint("ntaper", &ntaper))        ntaper = 0;        [8]

                                                
        /* Get info from first trace */
        if (!gettr(&tr)) err("can't read first trace");              [9]
        if (!tr.dt) err("dt header field must be set");              [10]

        /* Loop over traces */
        do {                                                         [11]
                int nt     = (int) tr.ns;                            [12]

                if (below == 0) {                                    [13]
                        nmute = NINT((t - tmin)/dt);
                        memset((void *) tr.data, (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[i+nmute] *= taper[i];
                } else {
                        nmute = NINT((nt*dt - t)/dt);
                        memset((void *) (tr.data+nt-nmute),
                                        (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[nt-nmute-1-i] *= taper[i];
                }
                puttr(&tr);                                           [14]
        } while (gettr(&tr));                                         [15]
        
        return EXIT_SUCCESS;                                          [16]
}
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item We maintain the internal versions of the codes with the UNIX utility {\sf RCS}.  This item shows the string template for {\sf RCS}.
\item The file {\tt su.h} includes (directly or indirectly) all our locally defined 
macros and prototypes.  The file {\tt segy.h} has the definitions for the trace header 
fields.
\item The starred lines delimit the ``self-doc'' information---include them exactly as 
you find them in the codes since they are used by the automatic documentation shells.  
The style of the self-doc shown is typical except that often additional usage information 
is shown at the bottom and, of course, often there are more options.  Look at some 
existing codes for ideas.
\item This is an external declaration of an {\small\sf SU} ({\sf SEG-Y}) trace buffer.  
It is external to avoid wasting stack space.
\item We usually describe the global variables at the time of declaration.
Examine codes related to yours to increase consistency of nomenclature
(there is no official {\small\sf SU} naming standard).
\item The {\tt initargs} subroutine sets {\small\sf SU}'s command line passing 
facility (see page~\pageref{SU:page:getpar}).
\item The {\tt requestdoc} subroutine call specifies the circumstances under which 
self-doc will be echoed to the user.  The argument `1' applies to the  typical program that uses only standard input (i.e. \verb+<+) to read an {\small\sf SU} trace file.  Use `0' for 
codes that create synthetic data (like {\tt suplane}) and `2' for codes that require 
two input files (we could say ``et cetera,'' but there are no existing {\small\sf SU} 
mains that require {\em three} or more input files).
\item This is typical code for reading `parameters from the command line.  
Interpret it like this: ``If the user did not specify a value, then use the default value.''  The subroutine must be type-specific, here we are getting an {\em integer} parameter.
\item Read the first trace, exit if empty.  The subroutine {\tt fgettr} ``knows about'' 
the {\small\sf SU} trace format.  Usually the trace file is read from standard input and then we use {\tt gettr} which is a macro based on {\tt fgettr} defined in {\tt su.h}.  Note 
that this code implies that the first trace is read into the trace buffer (here 
called {\tt tr}), therefore we will have to process this trace before the next 
call to {\tt fgettr}.
\item We've read that first trace because, we need to get some trace parameters from the 
first trace header.  Usually these are items like the number of samples ({\tt tr.ns}) 
and/or the sampling interval ({\tt tr.dt}) that, by the {\sf SEGY-Y} standard, are the 
same for all traces.
\item Since the first trace has been (typically) read before the main processing loop starts, we use a ``do-while'' that reads a new trace at the {\em bottom} of the loop.
\item We favor using {\em local} variables where permitted.
\item This is the seismic algorithm--here incomplete.  We've left in some of the actual {\tt sumute} code because it happens to contains lines that will be useful in the new code, we'll be writing below.  You may want to call a subroutine here to do the real work.
\item {\tt fputtr} and {\tt puttr} are the output analogs of {\tt fgettr} and {\tt gettr}.
\item The loop end.  {\tt gettr} returns a 0 when the trace file is exhausted and the processing then stops.
\item This is an {\sf ANSI-C} macro conventionally used to indicate successful program termination.
\end{enumerate}

\section{Writing a new program: {\tt suvlength}}

A user asked about {\small\sf SU} processing for variable length traces.  At his
institute, data are collected from time of excitation to a variable
termination time.  The difficulty is that {\small\sf SU} processing is based on
the {\sf SEG-Y} standard which mandates that all traces in the data set
be of the same length.  Rather than contemplating changing all of {\small\sf SU},
it seems to us that the solution is to provide a program that converts
the variable length data to fixed length data by padding with zeroes
where necessary at the end of the traces---let's name this new program
{\tt suvlength}.  We can make the length of the output traces a user
parameter.  If there is a reasonable choice, it makes sense to provide
a default value for parameters.  Here, using the length of the first
trace seems the best choice since that value can be ascertained before
the main processing loop starts.

So far, so good.  But now our plan runs into a serious snag: the
fundamental trace getting facility, {\tt gettr}, itself assumes fixed
length traces (or perhaps we should say that {\tt gettr} deliberately
enforces the fixed length trace standard).  But, if you think about
it, you'll realize that {\tt gettr} itself has to take special measures
with the {\em first} trace to figure out its length.  All we have to do
is make a new trace getting routine that employs that first trace
logic for {\em every} trace.  Here, we'll suppress the details of
writing the ``fvgettr'' subroutine and turn to converting the
template above into the new {\tt suvlength} code:

{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.19 $ ; $Date: 2001/06/06 18:31:42 $   */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                ",
" SUVLENGTH - Adjust variable length traces to common length     ",
"                                                                ",
" suvlength <variable_length_traces >fixed_length_traces         ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ns      output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;        /* number of samples on output traces  */


        /* Initialize */                 
        initargs(argc, argv);
        requestdoc(1);
 
        /* Get parameters */
        ...
        
        /* Get info from first trace */
        ...

        ...

        return EXIT_SUCCESS;                                          [16]
}

/* fvgettr code goes here */
        ...

\end{verbatim}}\noindent
Now we run into a small difficulty.  Our only parameter has a default
value that is obtained only after we read in the first trace.  The
obvious solution is to reverse the parameter getting and the trace
getting in the template.  Thus we resume:
{\small\begin{verbatim}
        /* Get info from first trace and set ns */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;

        /* Loop over the traces */
        do {
                int nt = tr.ns;
\end{verbatim}}\noindent
Now comes the actual seismic algorithm---which is rather trivial in
the present case:  add zeroes to the end of the input trace if the
output length is specified greater than the input length.  We could
write a simple loop to do the job, but the task is done most
succinctly by using the {\sf ANSI-C} routine {\tt memset}.  However, we
confess that unless we've used it recently, we usually forget how to
use this routine.  One solution is to {\tt cd} to the {\tt su/main}
directory and use {\tt grep} to find other uses of {\tt memset}.  When
we did this, we found that {\tt sumute} had usage closest to what we
needed and that is why we started from a copy of that code.  Here is
the complete main for {\tt suvlength}:
{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.19 $ ; $Date: 2001/06/06 18:31:42 $        */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                 ",
" SUVLENGTH - Adjust variable length traces to common length      ",
"                                                                 ",
" suvlength <vdata >stdout                                        ",
"                                                                 ",
" Required parameters:                                            ",
"         none                                                    ",
"                                                                 ",
" Optional parameters:                                            ",
"          ns     output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 *
 * Trace header fields accessed:  ns
 * Trace header fields modified:  ns
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;                /* samples on output traces        */


        /* Initialize */
        initargs(argc, argv);
        requestdoc(1);


        /* Get info from first trace */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;


        /* Loop over the traces */
        do {
                int nt = tr.ns;
                                
                if (nt < ns) /* pad with zeros */
                        memset((void *)(tr.data + nt), '\0', (ns-nt)*FSIZE);
                tr.ns = ns;
                puttr(&tr);
        } while (fvgettr(stdin, &tr));
        
        return EXIT_SUCCESS;
}


#include "header.h"

/* fvgettr - get a segy trace from a file by file pointer (nt can vary)
 *
 * Returns:
 *        int: number of bytes read on current trace (0 after last trace)
 *
 * Synopsis:
 *        int fvgettr(FILE *fp, segy *tp)
 *
 * Credits:
 *        Cloned from .../su/lib/fgettr.c
 */

int fvgettr(FILE *fp, segy *tp)
   ...
\end{verbatim}}\noindent
{\bf Remark}: In the actual {\small\sf SU}, the subroutine {\tt fvgettr} has been
extracted as a library function and we also made a convenience macro
{\tt vgettr} for the case of standard input.  But these are secondary
considerations that don't arise for most applications.

For any new {\small\sf SU} code, one should provide an example shell program to show how
the new code is to be used.  Here is such a program for X Windows graphics:
{\small\begin{verbatim}
#! /bin/sh
# Trivial test of suvlength with X Windows graphics

WIDTH=700
HEIGHT=900
WIDTHOFF=50
HEIGHTOFF=20

>tempdata
>vdata
suplane >tempdata  # default is 32 traces with 64 samples per trace
suplane nt=72 >>tempdata
suvlength <tempdata ns=84 |
sushw key=tracl a=1 b=1 >vdata

# Plot the data 
suxwigb <vdata \
        perc=99 title="suvlength test"\
        label1="Time (sec)" label2="Traces" \
        wbox=$WIDTH hbox=$HEIGHT xbox=$WIDTHOFF ybox=$HEIGHTOFF &

# Remove #comment sign on next line to test the header
#sugethw <vdata tracl ns | more
\end{verbatim}}\noindent

\appendix
\chapter{Obtaining and Installing SU  \label{app:A}}
The {\small\sf SU} package contains seismic processing programs along with
libraries of scientific routines, graphics routines and
routines supporting the {\small\sf SU} coding conventions.
The package is available by anonymous ftp at the site
ftp.cwp.mines.edu (138.67.12.4). The directory path is pub/cwpcodes.
The package may also be obtained on the World Wide Web at
http://www.cwp.mines.edu/cwpcodes.
Take the files:
\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item untar\_me\_first.xx.tar.Z
\item cwp.su.all.xx.tar.Z
\end{enumerate}
Here the {\tt xx} denotes the number of the current release.
An incremental update is also available for updating the
previous release {\tt yy} to the current release {\tt xx}.  Take the files:

\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item README\_UPDATE
\item untar\_me\_first.xx.tar.Z
\item update.yy.to.xx.tar.Z
\item update.list
\end{enumerate}

\noindent If you find that {\tt ftp} times out during the transmission of the
files, the package is available in smaller pieces in the subdirectory
{\tt outside\_usa}.

\noindent For readers who are not familiar with anonymous ftp,
an annotated transaction listing follows in section~\ref{SU:sec:anonftp}.

\section{Obtaining files by anonymous ftp\label{SU:sec:anonftp}}
\begin{tabular}{lll}
Type: & &  \\
\% ftp 138.67.12.4        & --- &  138.67.12.4 is our ftp site  \\
username: anonymous       & --- &   your username is ``anonymous''     \\
password: yourname@@your.machine.name   & --- &  type anything here  \\
& &  \\
ftp$>$     & --- & this is the prompt you see \\
& & when you are in ftp 
\end{tabular}

\indent You are now logged in via ftp to the CWP anonymous ftp site.
You may type:

\begin{tabular}{lll}
ftp$>$ ls             & --- & to see the contents of the directories \\
ftp$>$ cd dirname     & --- & to change directories to ``dirname'' \\
ftp$>$ binary         & --- & to set ``binary mode'' for transferring files \\
        & &            You must do this before you try to transfer any \\
        & &            binary file. This includes all files with the form \\
        & &            some\_name.tar.Z extension. \\
ftp$>$ get filename   & --- & to transfer  ``filename'' from our site to your machine \\
ftp$>$ mget pattern*  & --- & to transfer all files with names of the ``pattern*'' \\
For example: & & \\
&& \\
ftp$>$ mget *.tar.Z   & --- & will transfer all files with the form of name.tar.Z \\
& &              to your machine. You will be asked whether you  \\
& &              really want each file of this name pattern transferred, \\
& &               before ftp actually does it.  \\
ftp$>$ bye            & --- & to exit from ftp 
\end{tabular}

\section{Requirements for installing the package}
The only requirements for installing the package are:
\begin{enumerate}
\item A machine running the UNIX operating system.
\item An {\sf ANSI C} compiler.
\item A version of make which supports include files
\item 16-60 megabytes (depending on system) of disk space for
the source and compiled binary. If space is an issue, then the
compiled binaries may be ``stripped'' by cd'ing to \$CWPROOT/bin
and typing "strip *".
\end{enumerate}

\noindent
The package has been successfully installed on:
\begin{itemize}
%update
\item IBM RS6000
\item SUN SPARC STATIONS
\item HP 9000 series machines
\item HP Apollo
\item NeXT
\item Convex
\item DEC
\item Silicon Graphics
\item PC's running LINUX, PRIME TIME, SCO, FREE BSD, ESIX, and NeXTSTEP 486
\end{itemize}

There are README files in the distribution with special notes about some
of these platforms.  We depend on the {\small\sf SU} user community to alert us to 
installation problems, so if you run into difficulties, please let us know.

The distribution contains a series of files that detail the installation 
process.  Read them in the following order:

{\small\begin{verbatim}
LEGAL_STATEMENT --- license,  legal statement
README_BEFORE_UNTARRING --- initial information
README_FIRST --- general information
README_TO_INSTALL --- installation instructions
Portability/README_*    --- portability information for various platforms
README_GETTING_STARTED --- how to begin using the codes
\end{verbatim}}\noindent
Many of these files are contained within untar\_me\_first.xx.tar.Z.

\section{A quick test}
\begin{figure}
\epsfxsize 250pt
\centerline{\epsffile{suplane.eps}}
\caption{Output of the \protect\verb:suplane: pipeline.}
\label{fig:suplane}
\end{figure}

Once you have completed the installation, here is a quick test you can make
to see if you have a functioning seismic system.
For an X-windows machine, the ``pipeline''
\begin{verbatim}
suplane | suxwigb &
\end{verbatim}
should produce the graphic shown in Figure~\ref{fig:suplane}. 
If you have a PostScript printer, then you should get a hard copy version
with the pipeline
\begin{verbatim}
suplane | supswigb | lpr
\end{verbatim}
If you have Display PostScript, or a PostScript previewer, then to get
a screen display, replace the \verb:lpr: command in the pipeline by
the command that opens a PostScript file, for example:
\begin{verbatim}
suplane | supswigb | ghostview -
\end{verbatim}

Another set of test pipelines are
\begin{verbatim}
susynlv | supsimage | lpr
\end{verbatim}
\begin{verbatim}
susynlv | supswigb | ghostview -
\end{verbatim}

\begin{figure}
\epsfxsize 300pt
\centerline{\epsffile{susynlv.eps}}
\caption{Output of the \protect\verb:susynlv: pipeline.}
\label{fig:susynlv}
\end{figure}

\chapter{Help Facililties \label{app:B}}
\section{Suhelp}
The full text of the output from:
\begin{verbatim}
% suhelp
\end{verbatim} \noindent

\input suhelp.tex

\section{Suname}
The full text of the output from:
\begin{verbatim}
% suname
\end{verbatim}\noindent
\input suname.tex

\section{Suhelp.html}
This is the text listing of Chris Liner's SU Help page.

\input suhelphtml.tex

\section{SUKEYWORD - list the SU datatype (SEGY) Keywords}
The following is the listing put out by

\input sukeyword.tex

\chapter{DEMOS - a brief descriptions of the demos}

\input demos.tex


\end{document}
@


1.19
log
@*** empty log message ***
@
text
@d1 1
a1 1
% copyright 1999 Colorado School of Mines, all rights reserved
d13 2
a14 2
\def\releasenumber{34\ }
\def\currentyear{2000\ }
d35 4
a38 4
The Seismic Unix project is by the Society of Exploration Geophysicists
(SEG), and by the Center for Wave Phenomena (CWP), Department,
of Mathematical and Computer Sciences, Colorado School of Mines.
Past support for SU has included these groups, as well as the Gas Research
d41 1
a41 1
That you SEG, and CWP for your continued support!
d48 1
a48 1
period when SU was ported to the modern workstation from its
d65 1
a65 1
Chris Liner, while a student at the Center, wrote most of the graphics
d98 1
a98 1
Rutty in Australia, Jens Hartmann in Germany, Alexander Koek, Michelle Miller
d113 1
a113 1
SU pamplets, and other materials, which we distribute at public meetings.
d115 1
a115 1
\section*{In Memorium}
d154 1
a154 1
In 1983, Jack K. Cohen and Shuki Ronen of the Center for Wave Phenomena (CWP)
d180 3
a182 3
Earlier versions of SU had been ported to CWP sponsor companies,
even providing the basis for in-house seismic processing packages developed
by some of those companies!
d190 3
a192 1
environment.
d199 2
a200 1
this manual.
d345 1
a345 1
\item SUNAME - get name line from self-docs 
d561 2
a623 1
you will see the same output as above, with the additional paragraphs
d625 3
a940 64
\section{SUKEYWORD - List SU Header Field Key Words}
Many of the SU programs that draw on header field information
have the parameter ``key='' listed in their selfdocs, with
the reference to ``keywords.''

The SU keywords are based on the SEGY trace header fields.
(This will be explained later, in the sections on tape reading
and data format conversion.)
To find out what these header fields are, and what they stand
for in the SU data type, type:

{ \small\begin{verbatim} 
% sukeyword -o
typedef struct {	/* segy - trace identification header */

	int tracl;	/* trace sequence number within line */

	int tracr;	/* trace sequence number within reel */

	int fldr;	/* field record number */

	int tracf;	/* trace number within field record */

	int ep;	/* energy source point number */
....
\end{verbatim}}\noindent
Please type {\bf sukeyword -o\/}' or see Appendix~{\ref{app:B} for the full text.

To find an individual keyword (for example ``cdp'') type:
{ \small\begin{verbatim} 
% sukeyword  cdp
\end{verbatim}}\noindent

{\small\begin{verbatim}
        int ep; /* energy source point number */

        int cdp;        /* CDP ensemble number */

        int cdpt;       /* trace number within CDP ensemble */

        short trid;     /* trace identification code:
                        1 = seismic data
                        2 = dead
                        3 = dummy
                        4 = time break
                        5 = uphole
                        6 = sweep
                        7 = timing
                        8 = water break
                        9---, N = optional use (N = 32,767)

                        Following are CWP id flags:

                         9 = autocorrelation

                        10 = Fourier transformed - no packing
                             xr[0],xi[0], ..., xr[N-1],xi[N-1]
....
\end{verbatim} } \noindent
Please see Appendix~\ref{app:B} for the full text of the output. 

A number of programs sort data, window data, and display data
by making use of the values in the SU header fields, making
{\bf sukeyword\/} an oft-used utility.
d983 18
d1017 1
a1017 11
%and 400 dots per inch PostScript format:\\
%\verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or\\
%\verb:pub/samizdat/texts/imaging/imaging_400dpi.ps.Z:.
%\ \ The exercises in this\\
%text make extensive use of {\small\sf SU}.

\item You should not hesitate to look at the source code itself.
  Section~\ref{SU:sec:template} explains the key {\small\sf SU} coding idioms.
  Please let us know if you discover any inconsistencies between the
  source and our documentation of it.  We also welcome suggestions for
  improving the comments and style of our codes.
d1019 5
a1023 4
\item Direct email to:  john@@dix.mines.edu 
if you have comments, questions, or suggestions regarding {\small\sf SU}.

\item The source code, itself, is a help mechanism of sorts. The main
d1027 5
a1031 1
\end{itemize}
d1851 1
a1851 1
\subsection{SUKEYWORD - See SU Keywords}
d5550 1
a5550 1
/* SUMUTE: $Revision: 1.18 $ ; $Date: 1998/09/29 15:28:51 $      */  [1]
d5693 1
a5693 1
/* SUVLENGTH: $Revision: 1.18 $ ; $Date: 1998/09/29 15:28:51 $   */
d5771 1
a5771 1
/* SUVLENGTH: $Revision: 1.18 $ ; $Date: 1998/09/29 15:28:51 $        */
d6004 1
a6004 1
suplane | suximage &
@


1.18
log
@sept 1998
@
text
@d1 1
d13 2
a14 2
\def\releasenumber{32\ }
\def\currentyear{1998\ }
d16 1
d35 1
a35 2
The Seismic Unix project is partially supported by the Gas Research
Institute (GRI), by the Society of Exploration Geophysicists
d38 2
d41 1
a41 2

That you GRI, SEG, and CWP for your continued and expanded support!
d47 1
a47 1
Geoscientific Computing at the Colorado School of Mines during the
d85 1
a85 1
This text is available from the CWP anonymous ftp site.
d98 9
a106 2
Rutty in Australia, Jens Hartmann in Germany, Alexander Koek in Delft,
Wenying Cai at the University of Utah, and Torsten Shoenfelder of Germany.
d124 1
a124 1
In the 3 years that have elapsed since the first version of
d129 1
a129 1
to the many contributions of code, bug fixes, extentions,
d132 1
a132 1
much discussion with members of the worldwide SU user commuinity,
d139 1
a139 1
have been have been moved to appendicies.
d149 1
a149 1
source code of the package as examples.
d163 2
a164 2
and Ronen, while Ronen was a student at Jon Claerbout's Stanford 
Exploration Project (SEP).
d166 3
a168 3
that was available at the time,
because the industry standard at the time was to use Fortran programs on
VAX VMS based systems.
d171 1
a171 1
SU (c. 1984) the sponsors of CWP had already begun showing interest
d179 5
a183 5
Until September of 1993, SU was used primarily in-house at CWP. 
Earlier versions of SU which had been ported to CWP sponsor companies
became the foundation for in-house seismic processing packages at
those companies.
Once the package was generally available on the Internet, it began
d185 1
a185 1
The package used by exploration geophysicists,
d202 1
a202 1
can alter and extend its capabilities.  The philosophy behind the package
d208 3
a210 1
however.  SU is intended as an extension of the Unix operating system,
d228 1
a228 1
making Unix, itself, a language. Seismic Unix benefits from all
d233 1
a233 1
Of course, it may be that no Unix or Seismic Unix program will fulfil
d245 4
a248 3
Because most commmercial seismic processing packages are GUI-based,
it is unavoidable that users will expect SU to be similar.
However, it is not a fair comparision, for several reasons. 
d254 2
a255 2
interface, if and when one is developed.   At most, any SU interface
will give limited access to the capabilities of the package.
d260 2
a261 2
If you do commercial level processing, and have dedicated, or plan
to dedicate funds to purchasing one or more license of such commercial
d265 1
a265 1
However, SU can be an important adjunct to the commercial package
d270 4
d276 1
a276 1
in geophysical and signal processing applications. It certainly
d279 2
d286 3
a288 2
be 3D applications in future releases of SU, though these may not be
the most efficient of codes.
d297 4
a300 1
programs on most systems.
d314 2
a315 2
as a language. As with any language, a certain amount of
vocabulary must be mastered before useful operations may be
d320 1
d328 1
a328 1
the majority of the programs contain a {\bf selfdoc\/}---a
d348 1
a348 1
For nonexecutables (library routines) and for programs without the
d687 2
a688 1
Usage: sufind [-v -n] string
d692 4
d770 1
a770 1
{\bf sufind\/} to look for DMO programs:
d844 1
a844 1
By using {\bf sufind\/} in conjuction with the selfdoc feature (or {\bf sudoc}),
d868 1
a868 1
http://douze.utulsa.edu/~cll/suhelp/suhelp.html
d874 1
a874 1
                          Version 31 (October 1997)
d908 21
a928 1
    * Alphabetical name list            * 258 items
d1047 1
a1047 1
  to frequenty asked questions about {\small\sf SU}, including detailed information
d1442 1
a1442 1
Future plans include encorporating each of these codes into the main
d1496 1
a1496 1
and end-of-record delimeters. Binary data created by C programs do not have
d1504 1
a1504 1
The program assumes that each record of fortran data is preceeded
d1515 1
a1515 1
called ``infile'' via, open and read statments that look like:
d1608 1
a1608 1
there is a progam called ``recast'' that will do the job for a
d1747 1
a1747 1
Having perfomed an operation on the binary data, we may want to
d2021 1
a2021 1
ouputs the values sequentially in order of keyword given, trace
d2150 1
a2150 1
As before, piping {\bf suplane\/} data into {\bf sushw\/} yeilds the following
d2216 1
a2216 1
processing sequences in the controled environment of shell scripts.
d2219 1
a2219 1
There is an often upleasant task called ``setting geometry''
d3788 4
a3791 1
%  suxcor < plane.vib.su sufile=junk.vib.su | suwind itmin=2500 itmax=2563 | sushw key=delrt a=0.0 > data.su
d3828 1
a3828 1
we would probalbly only use a single pass of the filter to remove
d3838 1
a3838 1
Also, an added feature of the Release~32 version of {\bf supef\}
d3847 1
a3847 1
the Wiener shaping filter {\bf sushape\}.
d3909 1
a3909 1
\chapter{Background Wavespeed Profiles}
d3941 1
a3941 1
\chapter{Synthetic Data Generators}
d4164 2
a4165 2
the program {\bf sufilter} with the \verb:<: operator, and similarly, the output data set \verb:outdata: receives the data because of the \verb:>: operator.
The output of {\bf sufilter} is connected to the input of {\bf sugain} by use of the \verb:|: operator.
d4168 1
a4168 1
how parameters are passed to {\small\sf SU} programs.  The program {\bf sugain}
d4170 1
a4170 1
the program {\bf sufilter} receives the assigned four component {\em vector}
d4283 1
a4283 1
{\bf WARNING!}  Spaces are significant to the UNIX shell---it  uses
d4289 1
a4289 1
\item The main pipeline of this shell code selects a certain set of cmp gathers with {\bf suwind}, gains this subset with {\bf sugain} and pipes the result into
d4295 1
a4295 1
\item The lines within the {\bf suximage} command are continued by the
d4298 1
a4298 1
\noindent{\bf WARNING!}  The line continuation backslash must be the {\em final}
d4307 1
a4307 1
plotting programs (e.g., supsimage).  For example, with {\bf supsimage} in
d4312 1
a4312 1
accepted by {\bf suximage}
d5528 56
d5586 6
a5591 1
looks like the program listing below (we excerpted lines from the program {\tt sumute} to build this template).  The numbers in square brackets at the end of the lines in the listing are not part of the listing---we added them to facilitate discussion of the template.  The secret to efficient {\small\sf SU} coding is finding an existing program similar to the one you want to write.  If you have trouble locating the right code or codes to ``clone,'' ask us---this can be the toughest part of the job!
d5594 1
a5594 1
/* SUMUTE: $Revision: 1.17 $ ; $Date: 1998/08/10 21:49:03 $      */  [1]
d5670 10
a5679 3
\item The file {\tt su.h} includes (directly or indirectly) all our locally defined macros and prototypes.  The file {\tt segy.h} has the definitions for the trace header fields.
\item The starred lines delimit the ``self-doc'' information---include them exactly as you find them in the codes since they are used by the automatic documentation shells.  The style of the self-doc shown is typical except that often additional usage information is shown at the bottom and, of course, often there are more options.  Look at some existing codes for ideas.
\item This is an external declaration of an {\small\sf SU} ({\sf SEG-Y}) trace buffer.  It is external to avoid wasting stack space.
d5683 18
a5700 5
\item The {\tt initargs} subroutine sets {\small\sf SU}'s command line passing facility (see page~\pageref{SU:page:getpar}).
\item The {\tt requestdoc} subroutine call specifies the circumstances under which self-doc will be echoed to the user.  The argument `1' applies to the  typical program that uses only standard input (i.e. \verb+<+) to read an {\small\sf SU} trace file.  Use `0' for codes that create synthetic data (like {\tt suplane}) and `2' for codes that require two input files (we could say ``et cetera,'' but there are no existing {\small\sf SU} mains that require {\em three} or more input files).
\item This is typical code for reading `parameters from the command line.  Interpret it like this: ``If the user did not specify a value, then use the default value.''  The subroutine must be type-specific, here we are getting an {\em integer} parameter.
\item Read the first trace, exit if empty.  The subroutine {\tt fgettr} ``knows about'' the {\small\sf SU} trace format.  Usually the trace file is read from standard input and then we use {\tt gettr} which is a macro based on {\tt fgettr} defined in {\tt su.h}.  Note that this code implies that the first trace is read into the trace buffer (here called {\tt tr}), therefore we will have to process this trace before the next call to {\tt fgettr}.
\item We've read that first trace because, we need to get some trace parameters from the first trace header.  Usually these are items like the number of samples ({\tt tr.ns}) and/or the sampling interval ({\tt tr.dt}) that, by the {\sf SEGY-Y} standard, are the same for all traces.
d5737 1
a5737 1
/* SUVLENGTH: $Revision: 1.17 $ ; $Date: 1998/08/10 21:49:03 $   */
d5815 1
a5815 1
/* SUVLENGTH: $Revision: 1.17 $ ; $Date: 1998/08/10 21:49:03 $        */
d6104 3
a6107 27
  The Making\_Data demos shows the basics of making synthetic data
  shot gathers and common offset sections using susynlv.  Particular
  attention is paid to illustrating good display labeling.

  The Filtering/Sufilter demo illustrates some real data processing to
  eliminate ground roll and first arrivals.  The demos in the
  Filtering subdirectories give instructions for accessing the data
  from the CWP ftp site.

  The Deconvolution demo uses simple synthetic spike traces to
  illustrate both dereverberation and spiking decon using supef and
  other tools.  The demos include commands for systematically
  examining the effect of the filter parameters using loops.

  The Sorting\_Traces Tutorial is an interactive script that
  reinforces some of the basic UNIX and {\small\sf SU} lore discussed in this
  document.  The interactivity is limited to allowing you to set the
  pace.  Such tutorials quickly get annoying, but we felt that one
  such was needed to cover some issues that didn't fit into our
  standard demo format.  There is also a standard, but less complete,
  demo on this topic.

  The next step is to activate the Selecting\_Traces Demo.  Then
  proceed to the NMO Demo.  Beyond that, visit the Demo directories
  that interest you.  The {\bf demos} directory tree is still under
  active development---please let us know if the demos are helpful and
  how they can be improved.
@


1.17
log
@Release 32 version
@
text
@d305 1
a305 1
the majority of the programs contain a {\bf selfdoc}---a
d363 1
a363 1
Please type {\bf suhelp\/}or see Appendix~{\ref{app:B} for the full text.
d771 1
a771 1
Now use the self-doc facility to get more information about {\bf sudmofk}:
d985 1
a985 1
that interest you.  The {\bf demos} directory tree is still under
d993 1
a993 1
  to carry out coordinated data processing.  The {\bf su/examples}
d998 1
a998 1
\item The {\bf faq} directory contains a growing collection of answers
d2546 1
a2546 1
\subsubsection{Makeing a movie with SUXMOVIE}
d3758 1
a3758 1
doesn't work properly, when in fact, it simply being used properly.
d3762 1
a3762 1
of {\bf maxlag\/}. This may be determined by first establishing the
d3765 1
a3765 1
As the preprocessing step, you will probably need to us {\bf sugain\/}
d3784 1
a3784 1
the parameters for the postprocessing filtering (using {\bf sufilter\/}
d3790 2
a3791 1
the prediction error filter computation.
d5220 35
d5482 1
a5482 1
/* SUMUTE: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $      */  [1]
d5605 1
a5605 1
/* SUVLENGTH: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $   */
d5683 1
a5683 1
/* SUVLENGTH: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $        */
@


1.16
log
@15 Jan 1998
@
text
@d2 1
d12 2
a13 2
\def\releasenumber{31\ }
\def\currentyear{1997\ }
d21 1
a21 17
%\begin{titlepage}
%\pagenumbering{roman}
%\vspace*{2.5 in}
%\centerline{\Huge\bf The SU User's Manual}
%\vspace{0.5in}
%\centerline{\Large Jack K. Cohen \& John W. Stockwell, Jr.}
%\vspace{0.5in}
%\centerline{\Large Version 1.6: Jan, \currentyear}
%
%\vfill
%
%\centerline{\epsfysize=0.675in \epsffile{cwplogo.eps}}
%\vspace{1ex}
%\centerline{Center for Wave Phenomena}
%\centerline{Colorado School of Mines}
%
%\end{titlepage}
a24 1

d34 2
a35 2
Institute ({\sf GRI}), by the Society of Exploration Geophysicists
({\sf SEG}), and by the Center for Wave Phenomena ({\sf CWP}) Department
d41 2
a42 2
The sponsors of the {\small\sf CWP} Consortium Project have long been partners
in the {\small\sf SU} project and we are pleased to explicitly acknowledge that
d44 1
a44 1
supplied in the past by {\sf IBM} Corporation and by the Center for
d46 1
a46 1
period when {\small\sf SU} was ported to the modern workstation from its
d50 2
a51 2
contributed to {\small\sf SU}, that it is impossible to list them all here.
However, certain people have made such signal contributions that they
d54 3
a56 2
Einar Kjartansson wrote the first draft of what is now called SU in
cooperation with Shuki Ronen while both were at Stanford University.
d61 1
a61 1
turning {\small\sf SU} into a supportable and exportable product.
d64 1
a64 1
codes used in the pre-workstation (i.e, graphics terminal) age of {\small\sf SU}.
d66 1
a66 1
him to make a positive and continuing influence on the {\small\sf SU} coding
d81 3
a83 3
John Scales showed how to use {\small\sf SU} effectively in the classroom in his
electronic text, {\em Theory of Seismic Imaging}, Samizdat Press, 1994.
This text is available from the {\small\sf CWP} anonymous ftp site.
d86 3
a88 1
portability of the package.
d90 2
a91 1
We also have had extensive help from the worldwide SU community.
d93 2
a94 2
Memorial University in Newfoundland, Toralf Foerster of the Institut
of Baltic Research in Warnemuende Germany, Stewart A. Levin, John
d97 2
a98 1
and Wenying Cai at the University of Utah.  Our apologies in advance
d100 1
a100 1
{\small\sf SU}---we promise to include you in future updates of this manual!
d103 2
a104 1
and also for her excellent design of this document.
d114 27
d144 33
a176 2
Seismic Unix ({\small\sf SU}) is a self-contained software environment for
seismic research and data processing used by exploration geophysicists,
d178 2
a179 2
and others. It is used by scientific staff in both small companies
and major oil and gas companies, and by academics and government
d183 3
a185 1
The {\small\sf SU} package is {\em free software}, meaning that you may have
d187 5
a191 3
under the license that appears at the beginning of this manual.
The package is maintained and expanded periodicall, with
each new release appearing at 3 to 6 month intervals.
d198 149
a346 43
The package permits the exchange of data according to the industry
protocol ({\sf SEG-Y}).  It provides a standard environment for the
testing of new processing algorithms. It is easy to use because it
does not require learning a special language---its application uses
only the standard facilities afforded by the UNIX operating system.
Once UNIX shell-redirecting and pipes are mastered, there is no
further artificial language to learn.  The seismic commands and
options can be used as readily as other UNIX commands.  In
particular, the user can write ordinary UNIX shell scripts to
combine frequent command combinations into meta-commands (i.e.,
processing flows).  These scripts can be thought of as ``job files.''

The seismic processing programs in the package
assume that the data are written in {\sf SEG-Y} format
with each trace preceded by an appropriate header.  This allows
the data information to be read by each program
in the processing stream in a consistent manner.  The package includes
facilities for converting data in several other formats
to the {\sf SEG-Y} format.

The {\small\sf SU} user community accesses the software over the Internet using
the commonly available ``anonymous ftp'' facility.  In this way, users
obtain the software with no constraints on its use.  During
installation, the user is given the option of sending an electronic
mail request to add them to our user group.  Members of the user group
receive announcements when updates of the package become available.
The {\small\sf SU} community is worldwide, spanning six continents, 35 countries
and over 600 known installations on a variety of hardware platforms
ranging from mainframes to workstations and {\sf PC}'s.

Parts of {\small\sf SU} originated from software developed by students working
with the Stanford Exploration Project at Stanford University.  The
present package was developed and is maintained at the Center for Wave
Phenomena ({\small \sf CWP}) at the Colorado School of Mines.  {\small\sf CWP} is an
interdisciplinary (geophysics, mathematics) research and educational
program in seismic exploration, supported by 34 companies in the oil
and gas industry.

Another growing source of new applications, as well as extensions and
bug fixes to existing {\small\sf SU} programs is our worldwide community of users. 
Without their enthusiasm, patience, thoughtful suggestions, and
valuable software contributions, the package would not be nearly as
powerful.
d348 3
a350 17
\chapter{How to Get a Copy of SU}
The {\small\sf SU} package contains seismic processing programs along with
libraries of scientific routines, graphics routines and
routines supporting the {\small\sf SU} coding conventions.
The package is available by anonymous ftp at the site
ftp.cwp.mines.edu (138.67.12.4). The directory path is pub/cwpcodes.
The package may also be obtained on the World Wide Web at
http://www.cwp.mines.edu/cwpcodes.
Take the files:
\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item untar\_me\_first.xx.tar.Z
\item cwp.su.all.xx.tar.Z
\end{enumerate}
Here the {\tt xx} denotes the number of the current release.
An incremental update is also available for updating the
previous release {\tt yy} to the current release {\tt xx}.  Take the files:
d352 6
a357 7
\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item README\_UPDATE
\item untar\_me\_first.xx.tar.Z
\item update.yy.to.xx.tar.Z
\item update.list
\end{enumerate}
a358 3
\noindent If you find that {\tt ftp} times out during the transmission of the
files, the package is available in smaller pieces in the subdirectory
{\tt outside\_usa}.
d360 4
a363 2
\noindent For readers who are not familiar with anonymous ftp,
an annotated transaction listing follows in section~\ref{SU:sec:anonftp}.
d365 5
a369 10
\section{Obtaining files by anonymous ftp\label{SU:sec:anonftp}}
\begin{tabular}{lll}
Type: & &  \\
\% ftp 138.67.12.4        & --- &  138.67.12.4 is our ftp site  \\
username: anonymous       & --- &   your username is ``anonymous''     \\
password: yourname@@your.machine.name   & --- &  type anything here  \\
& &  \\
ftp$>$     & --- & this is the prompt you see \\
& & when you are in ftp 
\end{tabular}
a370 2
\indent You are now logged in via ftp to the CWP anonymous ftp site.
You may type:
d372 1
a372 17
\begin{tabular}{lll}
ftp$>$ ls             & --- & to see the contents of the directories \\
ftp$>$ cd dirname     & --- & to change directories to ``dirname'' \\
ftp$>$ binary         & --- & to set ``binary mode'' for transferring files \\
        & &            You must do this before you try to transfer any \\
        & &            binary file. This includes all files with the form \\
        & &            some\_name.tar.Z extension. \\
ftp$>$ get filename   & --- & to transfer  ``filename'' from our site to your machine \\
ftp$>$ mget pattern*  & --- & to transfer all files with names of the ``pattern*'' \\
For example: & & \\
&& \\
ftp$>$ mget *.tar.Z   & --- & will transfer all files with the form of name.tar.Z \\
& &              to your machine. You will be asked whether you  \\
& &              really want each file of this name pattern transferred, \\
& &               before ftp actually does it.  \\
ftp$>$ bye            & --- & to exit from ftp 
\end{tabular}
d374 4
a377 7
\section{Requirements for installing the package}
The only requirements for installing the package are:
\begin{enumerate}
\item A machine running the UNIX operating system.
\item An {\sf ANSI C} compiler.
\item Ten megabytes of disk space for the source and compiled binary.
\end{enumerate}
d379 2
a380 14
\noindent
The package has been successfully installed on:
\begin{itemize}
%update
\item IBM RS6000
\item SUN SPARC STATIONS
\item HP 9000 series machines
\item HP Apollo
\item NeXT
\item Convex
\item DEC
\item Silicon Graphics
\item PC's running LINUX, PRIME TIME, SCO, FREE BSD, ESIX, and NeXTSTEP 486
\end{itemize}
d382 1
a382 3
There are README files in the distribution with special notes about some
of these platforms.  We depend on the {\small\sf SU} user community to alert us to 
installation problems, so if you run into difficulties, please let us know.
d384 9
a392 2
The distribution contains a series of files that detail the installation 
process.  Read them in the following order:
d394 16
a409 7
{\small\begin{verbatim}
LEGAL_STATEMENT --- license,  legal statement
README_BEFORE_UNTARRING --- initial information
README_FIRST --- general information
README_TO_INSTALL --- installation instructions
Portability/README_*    --- portability information for various platforms
README_GETTING_STARTED --- how to begin using the codes
a410 1
Many of these files are contained within untar\_me\_first.xx.tar.Z.
d412 1
a412 7
\section{A quick test}
\begin{figure}
\epsfxsize 250pt
\centerline{\epsffile{suplane.eps}}
\caption{Output of the \protect\verb:suplane: pipeline.}
\label{fig:suplane}
\end{figure}
a413 18
Once you have completed the installation, here is a quick test you can make
to see if you have a functioning seismic system.
For an X-windows machine, the ``pipeline''
\begin{verbatim}
suplane | suximage &
\end{verbatim}
should produce the graphic shown in Figure~\ref{fig:suplane}. 
If you have a PostScript printer, then you should get a hard copy version
with the pipeline
\begin{verbatim}
suplane | supswigb | lpr
\end{verbatim}
If you have Display PostScript, or a PostScript previewer, then to get
a screen display, replace the \verb:lpr: command in the pipeline by
the command that opens a PostScript file, for example:
\begin{verbatim}
suplane | supswigb | ghostview -
\end{verbatim}
d415 1
a415 7
Another set of test pipelines are
\begin{verbatim}
susynlv | supsimage | lpr
\end{verbatim}
\begin{verbatim}
susynlv | supswigb | ghostview -
\end{verbatim}
d417 5
a421 6
\begin{figure}
\epsfxsize 300pt
\centerline{\epsffile{susynlv.eps}}
\caption{Output of the \protect\verb:susynlv: pipeline.}
\label{fig:susynlv}
\end{figure}
d423 4
d428 1
a428 5
\chapter{Help Facilities}
\section{self-documentation (selfdoc)}
Virtually all {\small\sf SU} programs give information about themselves
when you type the name of the program without any arguments.
For example,
d434 1
a434 1
 sustack <input >output key=cdp normpow=1.0 verbose=0           
d447 1
a447 1
        Sushw can be used afterwards if this is not acceptable.                                                              
d450 39
a488 3
 
\section{suhelp} 
{\bf suhelp} lists the names of the programs in the distribution.  The first ``page'' of the output looks something like this:
d490 42
a531 1
% suhelp
d533 5
d539 49
a587 3
CWP PROGRAMS: (no self-documentation)
ctrlstrip       fcat            maxints         t  
downfort        isatty          pause           upfort  
d589 1
a589 6
PAR PROGRAMS: (programs with self-documentation)
a2b             kaperture       resamp          transp          vtlvz  
b2a             makevel         smooth2         unif2           wkbj  
farith          mkparfile       smoothint2      unisam  
ftnstrip        prplot          subset          unisam2  
h2b             recast          swapbytes       velconv  
d591 1
d593 2
a594 1
press return key to continue
d596 5
d602 3
a604 5
\section{suname}
{\bf suname} is a program that lists the names and a one-line description
of all the programs and libraries.  Here is the first ``page'' of the output:
{\small\begin{verbatim}
% suname
d606 3
a608 1
 -----  CWP Free Programs -----   
d610 5
a614 1
Mains: 
d616 8
a623 9
In CWPROOT/cwp/main:
* CTRLSTRIP - Strip non-graphic characters
* DOWNFORT - change Fortran programs to lower case, preserving strings
* FCAT - fast cat with 1 read per file 
* ISATTY - pass on return from isatty(2)
* MAXINTS - Compute maximum and minimum sizes for integer types 
* PAUSE - prompt and wait for user signal to continue
* T - time and date for non-military types
* UPFORT - change Fortran programs to upper case, preserving strings
d625 2
a626 18
In CWPROOT/par/main:
A2B - convert ascii floats to binary                            
B2A - convert binary floats to ascii                            
FARITH - File ARITHmetic -- perform simple arithmetic with binary files
FTNSTRIP - convert a file of floats plus record delimiters created      
H2B - convert 8 bit hexidecimal floats to binary                
KAPERTURE - generate the k domain of a line scatterer for a seismic array
MAKEVEL - MAKE a VELocity function v(x,y,z)                             
MKPARFILE - convert ascii to par file format                            
--More--(5%)
\end{verbatim}}\noindent
Note that the directory paths are shown as an aid to the {\small\sf SU} programmer.

\section{sudoc}
{\bf sudoc} is a utility that lists the self-documentation
of all programs in the package.  Subroutines and the few
mains and shell scripts that do not show a selfdoc will have an
sudoc entry.
d628 2
a629 3
For example,
{\small\begin{verbatim}
% sudoc fgettr 
d631 3
a633 2
In CWPROOT/src/su/lib: 
FGETTR - Routines to get an SU trace from a file 
d635 3
a637 5
fgettr          get a segy trace from a file by file pointer
gettr           macro using fgettr to get a trace from stdin
 
Function Prototype:
int fgettr(FILE *fp, segy *tp);
d639 2
a640 5
Returns:
int: number of bytes read on current trace (0 after last trace)
 
The function gettr(x) is a macro defined in su.h
define gettr(x) fgettr(stdin, (x))
d642 2
a643 8
Usage example:
        segy tr;
        ...
        while (gettr(&tr)) {
                tr.offset = abs(tr.offset);
                puttr(&tr);
        }
        ...
a644 1
Authors: SEP: Einar Kjartansson, Stew Levin CWP: Shuki Ronen, Jack Cohen
d647 15
d663 13
a675 3
\section{sufind}
{\bf sufind} is a program that searches the self-documentations
for a given string.  For example,
a728 1

d731 9
a739 1
The final line of this output ends with a symbol meant to indicate that the user is to type a carriage return.\footnote{The phrase ``carriage return'' refers to an older technology, the typewriter.  Ask your parents for further details.}
d741 2
a742 5
\section{sukeyword}
{\small\sf SU} programs that manipulate the trace headers
use specific names called ``keywords'' to identify the 
header fields.  The {\bf sukeyword} program enables the user to list
the definition file for the keywords.  For example,
d744 3
a746 1
% sukeyword fldr
d748 4
a751 2
...skipping
        int tracr;     /* trace sequence number within reel */
d753 1
a753 1
        int fldr;      /* field record number */
d755 1
a755 1
        int tracf;     /* trace number within field record */
d757 1
a757 1
        int ep;        /* energy source point number */
d759 1
a759 1
        int cdp;       /* CDP ensemble number */
d761 164
a924 1
        int cdpt;      /* trace number within CDP ensemble */
a935 2
--More--(13%)
\end{verbatim}}\noindent
d937 1
a937 7
\section{Other help mechanisms}
\begin{itemize}
\item {\bf gendocs} is a program that creates the LaTeX document,
  {\bf selfdocs.tex}, that contains a complete set of all the
  self-documentations in the distribution (over 300 pages!).  A
  PostScript version of this document is available in our anonymous
  ftp site\\ (\verb:pub/cwpcodes/documentation.xx.tar.Z:).
d939 1
a939 2
\noindent
Here, {\tt xx} denotes the number of the current release.
d941 13
a953 4
\item The top level {\bf demos} directory contains a number of tutorial
  shell scripts.  Its subdirectories contain \verb:README: files that
  give detailed information.  Assuming that you start in the
  {\bf demos} directory, here is a roadmap to get you started:
d955 2
a956 3
  The Making\_Data demos shows the basics of making synthetic data
  shot gathers and common offset sections using susynlv.  Particular
  attention is paid to illustrating good display labeling.
d958 22
a979 4
  The Filtering/Sufilter demo illustrates some real data processing to
  eliminate ground roll and first arrivals.  The demos in the
  Filtering subdirectories give instructions for accessing the data
  from the CWP ftp site.
d981 7
a987 4
  The Deconvolution demo uses simple synthetic spike traces to
  illustrate both dereverberation and spiking decon using supef and
  other tools.  The demos include commands for systematically
  examining the effect of the filter parameters using loops.
d989 1
a989 13
  The Sorting\_Traces Tutorial is an interactive script that
  reinforces some of the basic UNIX and {\small\sf SU} lore discussed in this
  document.  The interactivity is limited to allowing you to set the
  pace.  Such tutorials quickly get annoying, but we felt that one
  such was needed to cover some issues that didn't fit into our
  standard demo format.  There is also a standard, but less complete,
  demo on this topic.

  The next step is to activate the Selecting\_Traces Demo.  Then
  proceed to the NMO Demo.  Beyond that, visit the Demo directories
  that interest you.  The {\bf demos} directory tree is still under
  active development---please let us know if the demos are helpful and
  how they can be improved.
d991 1
d1023 83
d1108 39
a1146 10
\chapter{Using SU}
\section{SU and UNIX}
You need not learn a special seismic language to use
{\small\sf SU}.  If you know how
to use UNIX shell-redirecting and pipes, you are ready to start
using {\small\sf SU}---the seismic commands and options can be used just as you
would use the built-in UNIX commands.  In particular, you
can write ordinary UNIX shell scripts to combine frequent
command combinations into meta-commands (i.e., processing flows).
These scripts can be thought of as ``job files.''
d1148 16
a1163 11
\begin{table}[htbp]
\label{SU:tab:unix}
\caption{UNIX Symbols}
\begin{tabular}{||l||l||}  \hline\hline
process1 $<$ file1 & process1 takes input from file1 \\
process2 $>$ file2 & process2 writes on (new) file2 \\
process3 $>>$ file3 & process3 appends to file3  \\
process4 $|$ process5 & output of process4 is input to process5  \\
process6 $<<$ text & take input from following lines  \\ \hline \hline
\end{tabular}
\end{table}
d1165 5
a1169 9
So let's begin with a capsule review of the basic UNIX operators
as summarized in Table~\ref{SU:tab:unix}.
The symbols $<$, $>$, and $>>$ are known as ``redirection operators,''
since they redirect input and output into or out of the command
(i.e., process).
The symbol $|$ is called a ``pipe,'' since we can picture
data flowing from one process to another through the ``pipe.''
Here is a simple {\small\sf SU} ``pipeline'' with input ``indata'' and
output ``outdata'':
d1171 1
d1173 1
a1173 2
sufilter f=4,8,42,54 <indata |
sugain tpow=2.0 >outdata
d1175 1
a1175 4
This example shows a band-limiting operation being ``piped'' into
a gaining operation.  The input data set \verb:indata: is directed into
the program {\bf sufilter} with the \verb:<: operator, and similarly, the output data set \verb:outdata: receives the data because of the \verb:>: operator.
The output of {\bf sufilter} is connected to the input of {\bf sugain} by use of the \verb:|: operator.
d1177 54
a1230 6
\label{SU:page:getpar}The strings with the \verb:=: signs illustrate
how parameters are passed to {\small\sf SU} programs.  The program {\bf sugain}
receives the assigned value 2.0 to its parameter \verb:tpow:, while
the program {\bf sufilter} receives the assigned four component {\em vector}
to its parameter \verb:f:.  To find out what the valid parameters are
for a given program, we use the self-doc facility.
d1232 6
a1237 4
By the way, space around the UNIX
redirection and pipe symbols is optional---the example shows
one popular style.  On the other hand, spaces around the \verb:=:
operator are {\em not} permitted.
d1239 14
a1252 7
The first four symbols in
Table~\ref{SU:tab:unix} are the basic grammar of UNIX;
the final $<<$ entry
is the symbol for the less commonly used ``here document'' redirection.
Despite its rarity in interactive use,
{\small\sf SU} shell programs are significantly enhanced by
appropriate use of the $<<$ operator---we will illustrate this below.
d1254 2
a1255 3
Many built-in UNIX commands do not have a self-documentation
facility like {\small\sf SU}'s---instead, most do have ``man'' pages.
For example,
d1257 28
d1286 5
a1290 1
% man cat
d1292 5
a1296 1
CAT(1)              UNIX Programmer's Manual               CAT(1)
d1298 1
d1300 1
d1302 1
a1302 2
NAME
     cat - catenate and print
d1304 1
a1304 2
SYNOPSIS
     cat [ -u ] [ -n ] [ -s ] [ -v ] file ...
d1306 1
a1306 3
DESCRIPTION
     Cat reads each file in sequence and displays it on the stan-
     dard output.  Thus
d1308 1
a1308 1
                    cat file
d1310 4
a1313 1
     displays the file on the standard output, and
d1315 15
a1329 2
                    cat file1 file2 >file3
--More--
a1330 3
You need to know a bit more UNIX lore
to use {\small\sf SU} efficiently---we'll introduce these tricks of the trade in
the context of the examples discussed later in this chapter.
d1332 28
d1361 42
a1402 9
\section{Exploring SU}
This section is a simulated example of an interactive session with {\small\sf SU}.
\subsection{Looking for DMO programs}
Later we will discuss the construction of {\small\sf SU} processing streams, our
present purpose is just to illustrate how to look around
for the raw materials for such streams.
Let's by using {\bf sufind} to see if there are any {\sf DMO} programs:
{\small\begin{verbatim}
% sufind dmo
d1404 17
a1420 1
 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
d1422 20
a1441 1
 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
d1443 25
d1469 2
a1470 1
 SUDMOTX - DMO via T-X domain (Kirchhoff) method for common-offset gathers
d1472 15
a1486 1
 sudmotx <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [optional parms]
d1488 2
d1491 2
a1492 1
 SUFDMOD2 - Finite-Difference MODeling (2nd order) for acoustic wave equation
d1494 61
a1554 1
 sufdmod2 <vfile >wfile nx= nz= tmax= xs= zs= [optional parameters]     
d1556 1
d1558 4
a1561 1
 SUSTOLT - Stolt migration for stacked data or common-offset gathers    
d1563 2178
a3740 1
 sustolt <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
d3742 364
a4105 4
The last two ``hits'' are spurious,
but we see that two {\sf DMO} programs have been found.
\subsection{Getting information about SU programs}
Use the self-doc facility to get more information about {\bf sudmofk}:
d4107 2
a4108 42
% sudmofk
                                                                        
 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
                                                                        
 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
                                                                        
 Required Parameters:                                                   
 cdpmin                  minimum cdp (integer number) for which to apply DMO
 cdpmax                  maximum cdp (integer number) for which to apply DMO
 dxcdp                   distance between adjacent cdp bins (m) 
 noffmix                 number of offsets to mix (see notes)           
                                                                        
 Optional Parameters:                                                   
 tdmo=0.0                times corresponding to rms velocities in vdmo (s)
 vdmo=1500.0             rms velocities corresponding to times in tdmo (m/s)
 sdmo=1.0                DMO stretch factor; try 0.6 for typical v(z)   
 fmax=0.5/dt             maximum frequency in input traces (Hz) 
 verbose=0               =1 for diagnostic print                        
                                                                        
 Notes:                                                         
 Input traces should be sorted into common-offset gathers.  One common- 
 offset gather ends and another begins when the offset field of the trace
 headers changes.                                                       
                                                                        
 The cdp field of the input trace headers must be the cdp bin NUMBER, NOT
 the cdp location expressed in units of meters or feet.         
                                                                        
 The number of offsets to mix (noffmix) should typically equal the ratio of
 the shotpoint spacing to the cdp spacing.  This choice ensures that every
 cdp will be represented in each offset mix.  Traces in each mix will   
 contribute through DMO to other traces in adjacent cdps within that mix.
                                                                        
 The tdmo and vdmo arrays specify a velocity function of time that is   
 used to implement a first-order correction for depth-variable velocity.
 The times in tdmo must be monotonically increasing.                    
                                                                        
 For each offset, the minimum time at which a non-zero sample exists is 
 used to determine a mute time.  Output samples for times earlier than this
 mute time will be zeroed.  Computation time may be significantly reduced
 if the input traces are zeroed (muted) for early times at large offsets.
                                                                        
 Trace header fields accessed:  ns, dt, delrt, offset, cdp.             
d4110 4
d4115 11
a4125 7
\subsection{Viewing header field definitions}
Note that the end of the last program description referred to
``header fields'';
these names are {\em not} standard and, as mentioned earlier, you can
get definitions by using {\bf sukeyword}.  For example,
{\small\begin{verbatim}
% sukeyword delrt
d4127 7
a4133 2
...skipping
                           may be positive or negative */
d4135 3
a4137 5
        short delrt;    /* delay recording time, time in ms between
                           initiation time of energy source and time
                           when recording of data samples begins
                           (for deep water work if recording does not
                           start at zero time) */
d4139 2
a4140 1
        short muts;     /* mute time--start */
d4142 1
a4142 1
        short mute;     /* mute time--end */
a4143 1
        unsigned short ns;      /* number of samples in this trace */
a4144 1
        unsigned short dt;      /* sample interval; in micro-seconds */
d4146 2
a4147 5
        short gain;     /* gain type of field instruments code:
                                1 = fixed
                                2 = binary
                                3 = floating point
                                4 ---- N = optional use */
d4149 2
a4150 2
--More--(53%)
\end{verbatim}}\noindent
d4152 3
a4154 4
\subsection{Viewing program names}
{\small\sf SU} program names are often obscure (we aren't proud of this).
Here's how to get help with remembering the exact name of a program
when you recall a fragment of the name:
d4156 1
a4156 2
{\small\begin{verbatim}
% sufind -n head
d4158 1
a4158 2
 SUADDHEAD - put headers on bare traces and set the tracl and ns fields
 UPDATEHEAD - update ../doc/Headers/Headers.all
d4160 2
a4161 1
For more information type: "program_name <CR>"
d4163 3
a4165 2
Recall also that {\bf suhelp} and {\bf suname} give comprehensive listings
of the {\small\sf SU} programs.
a4166 3
Note that we used the {\tt -n} option of the  {\bf sufind} command.  Using the self-doc facility, we can get the full story:
{\small\begin{verbatim}
% sufind
a4167 6
sufind - get info from self-docs about SU programs
Usage: sufind [-v -n] string
"sufind string" gives brief synopses
"sufind -v string" verbose hunt for relevant items
"sufind -n name_fragment" searches for command name
\end{verbatim}}\noindent
d4616 1
a4616 82
\section{Some core programs}

Reading the self-documentation and trying out the following {\small\sf SU} programs
will give you a good start in learning {\small\sf SU}. 

\subsection{Examining the trace headers}
\begin{description}
\item{\bf surange} --- print minimum and maximum values of trace header fields
\item{\bf sugethw} --- print values of selected header fields
\item{\bf suascii} --- print header and data values
\item{\bf suxedit} --- interactively examine headers and traces
\end{description}

\subsection{Some common processing programs}
\begin{description}
\item{\bf suacor} --- compute autocorrelations
\item{\bf sufilter} --- multipurpose zero phase filter (includes bandpass)
\item{\bf sugain} --- gain (with lots of options)
\item{\bf sumute} --- zero samples before a time that depends on offset
\item{\bf sunmo} --- normal-moveout correction
\item{\bf supef} --- prediction error filtering
\item{\bf susort} --- sort traces by values of trace header fields
\item{\bf sustack} --- stack (sum) traces
\item{\bf suvelan} --- velocity analysis
\item{\bf suwind} --- window (i.e., get a subset of) traces
\end{description}

\subsection{Some common plotting programs}
\begin{description}
\item{\bf suximage} --- gray scale X Windows plotting
\item{\bf suxwigb} --- bit mapped wiggle trace X Windows plotting
\item{\bf supsimage} --- gray scale PostScript plotting
\item{\bf supswigb} --- bit mapped wiggle trace PostScript plotting
\end{description}

\section{A brief tour of the source directories}
The {\small\sf SU} software is a layered product.  The layers correspond to the
following directories:
\begin{description}
\item{\bf cwp} Library of scientific routines (e.g. fft routines)
written in ``vanilla'' C. Utility mains and shells.
\item{\bf par} Library supporting the {\small\sf CWP} programming
style (i.e., self-doc, error reporting, parameter passing).
Mains that use (only) these facilities.  Shells for maintaining the
online documentation database.
\item{\bf su} Seismic processing codes that use the
{\sf SEG-Y} trace structure.    Subroutines that manage this
structure. Codes that buffer the generic graphics
routines listed below.  Shells that provide backward compatibility with
earlier releases.
\item{graphics libraries}
        \begin{enumerate}
        \item {\bf psplot}---PostScript graphics:
                \begin{enumerate}
                \item pscontour: contour plots
                \item pscube: 3D data cube
                \item psgraph: curve plotting
                \item psimage: raster plotting
                \item psmovie: supports frames
                \item pswigb: bit mapped wiggle traces (fast)
                \item pswigp: polygon wiggle traces (slow)
                \item PostScript support programs
                \end{enumerate}
        \item {\bf xplot}---xlib based X Windows graphics
                \begin{enumerate}
                \item ximage: raster plotting
                \item xwigb: bit mapped wiggle traces
                \item X Windows support programs
                \end{enumerate}
        \item \bf{Xtcwp}---toolkit based X Windows graphics
                \begin{enumerate}
                \item xgraph: curve plotting
                \item xmovie: supports frames
                \item X Windows resource files
                \end{enumerate}
        \end{enumerate}
\end{description}
These are only the highlights.  If you intend to add your
own C mains to the package, it is worthwhile
spending a few hours browsing through the source code.

\chapter{Frequently Asked Questions}
d4730 2
a4731 1
The {\small\sf SU} data format is based on the {\sf SEG-Y} format. The {\small\sf SU} format
d4736 1
d4903 2
a4912 1

d5277 1
a5277 1
The {\sf CWP/SU} package is updated at roughly 3-6 month intervals.
d5279 1
a5279 2
we do not provide support for outdated versions,
we urge you to remain current.
d5312 1
a5312 1
I can't find an sudoc entry for the function "ints8r," yet the
d5335 1
a5335 1
library function ins8c. You may now use "sudoc" to find out more
d5389 1
a5389 1
in the "suname" and "sudoc" listings. How do I do this?
d5393 1
a5393 1
Run {\bf updatedocall} (source code located in CWPROOT/par/shell).
d5446 1
a5446 1
/* SUMUTE: $Revision: 1.15 $ ; $Date: 1997/10/14 20:15:53 $      */  [1]
d5569 1
a5569 1
/* SUVLENGTH: $Revision: 1.15 $ ; $Date: 1997/10/14 20:15:53 $   */
d5647 1
a5647 1
/* SUVLENGTH: $Revision: 1.15 $ ; $Date: 1997/10/14 20:15:53 $        */
d5759 166
d5926 2
d5929 1
d5931 27
a5957 31
\end{document}
-------------------------
% Printed separately for ``inside the title page'' as legal.tex
\appendix
\section{Legal Matters}
{\bf Disclaimer}:
There are no guarantees, explicit or implicit, made by the Center for 
Wave Phenomena, the Colorado School of Mines, or any member of the 
aforesaid organizations, past, present, or future, regarding the accuracy,
safety, usefulness, or any other quality or aspect of this software.

{\bf License}:
Copyright (c) Colorado School of Mines, 1994,1995,1996\\
All rights reserved.

Permission to use, copy, and modify this software for any purpose and
without fee is hereby granted, provided that the above copyright notice
and this permission notice appear in all copies, and the name of the
Colorado School of Mines ({\sf CSM}) not be used in advertising or
publicity pertaining to this software without specific, written prior
permission. CSM makes no representations about the suitability of this
software for any purpose. It is provided ``as is'' without express or
implied warranty.

{\bf Referencing Seismic Unix:}
In publications, please reference SU in the following fashion. \\

{\bf Example reference:
   Cohen, J. K. and Stockwell, Jr. J. W., (1996), CWP/SU: Seismic \\
      Unix Release \releasenumber: a free package for seismic research and processing, \\
      Center for Wave Phenomena, Colorado School of Mines.
d5959 5
a5963 2
Of course, 1996  and 29 should be replaced with date and number
of the release you are using.
d5965 1
@


1.15
log
@october 1997 fixes
@
text
@d11 2
a12 2
\def\releasenumber{31}
\def\currentyear{1997}
d75 2
a76 2
two-year stay here and, during this time, aided Cohen in turning {\small\sf SU} 
into a supportable and exportable product.
d116 8
d2219 1
a2219 1
/* SUMUTE: $Revision: 1.14 $ ; $Date: 1997/06/17 21:43:13 $      */  [1]
d2342 1
a2342 1
/* SUVLENGTH: $Revision: 1.14 $ ; $Date: 1997/06/17 21:43:13 $   */
d2420 1
a2420 1
/* SUVLENGTH: $Revision: 1.14 $ ; $Date: 1997/06/17 21:43:13 $        */
@


1.14
log
@17 June version
@
text
@d11 4
a14 5
\newcommand{\Tt}[1]{{\tt #1}}
\newcommand{\Cb}[1]{\centerline{\bf #1}}
\newcommand{\SU}{{\small\sf SU}}
\newcommand{\CWP}{{\small\sf CWP}}
\newcommand{\unix}{{\small\sf UNIX}}
d27 1
a27 1
%\centerline{\Large Version 1.6: Jan, 1997}
d49 4
a52 3
We would like to express our thanks to the Society of Exploration 
Geophysicists ({\sf SEG}) for their generous grant, which has
made Release 29 of the {\sf CWP/SU} package possible.
d54 5
a58 2
The sponsors of the \CWP\ Consortium Project have long been partners
in the \SU\ project and we are pleased to explicitly acknowledge that
d62 1
a62 1
period when \SU\ was ported to the modern workstation from its
d66 1
a66 1
contributed to \SU, that it is impossible to list them all here.
d75 1
a75 1
two-year stay here and, during this time, aided Cohen in turning \SU\ 
d79 1
a79 1
codes used in the pre-workstation (i.e, graphics terminal) age of \SU.
d81 1
a81 1
him to make a positive and continuing influence on the \SU\ coding
d96 1
a96 1
John Scales showed how to use \SU\ effectively in the classroom in his
d98 4
a101 1
This text is available from the \CWP\ anonymous ftp site.
d111 1
a111 1
\SU---we promise to include you in future updates of this manual!
d116 1
d119 1
a119 1
Seismic Unix (\SU) is a self-contained software environment for
d127 1
a127 1
The \SU\ package is {\em free software}, meaning that you may have
d142 2
a143 2
only the standard facilities afforded by the \unix\ operating system.
Once \unix\ shell-redirecting and pipes are mastered, there is no
d145 2
a146 2
options can be used as readily as other \unix\ commands.  In
particular, the user can write ordinary \unix\ shell scripts to
d158 1
a158 1
The \SU\ user community accesses the software over the Internet using
d164 1
a164 1
The \SU\ community is worldwide, spanning six continents, 35 countries
d168 1
a168 1
Parts of \SU\ originated from software developed by students working
d171 1
a171 1
Phenomena (\CWP) at the Colorado School of Mines.  \CWP\ is an
d177 1
a177 1
bug fixes to existing \SU\ programs is our worldwide community of users. 
d183 1
a183 1
The \SU\ package contains seismic processing programs along with
d185 1
a185 1
routines supporting the \SU\ coding conventions.
d196 1
a196 1
Here the \Tt{xx} denotes the number of the current release.
d198 1
a198 1
previous release \Tt{yy} to the current release \Tt{xx}.  Take the files:
d208 1
a208 1
\noindent If you find that \Tt{ftp} times out during the transmission of the
d210 1
a210 1
\Tt{outside\_usa}.
d250 1
a250 1
\item A machine running the \unix\ operating system.
d271 1
a271 1
of these platforms.  We depend on the \SU\ user community to alert us to 
d332 1
a332 1
Virtually all \SU\ programs give information about themselves
d409 1
a409 1
Note that the directory paths are shown as an aid to the \SU\ programmer.
d511 1
a511 1
\SU\ programs that manipulate the trace headers
d553 1
a553 1
Here, \Tt{xx} denotes the number of the current release.
d575 1
a575 1
  reinforces some of the basic \unix\ and \SU\ lore discussed in this
d588 1
a588 1
\item The essence of \SU\ usage is the construction of shell programs
d592 1
a592 1
  ``shells,'' are used interchangeably in the \unix\ literature.
d595 1
a595 1
  to frequenty asked questions about \SU, including detailed information
d603 1
a603 1
  extensive use of \SU.
d608 1
a608 1
%text make extensive use of \SU.
d611 1
a611 1
  Section~\ref{SU:sec:template} explains the key \SU\ coding idioms.
d617 1
a617 1
if you have comments, questions, or suggestions regarding \SU.
d624 5
a628 5
\SU.  If you know how
to use \unix\ shell-redirecting and pipes, you are ready to start
using \SU---the seismic commands and options can be used just as you
would use the built-in \unix\ commands.  In particular, you
can write ordinary \unix\ shell scripts to combine frequent
d644 1
a644 1
So let's begin with a capsule review of the basic \unix\ operators
d651 1
a651 1
Here is a simple \SU\ ``pipeline'' with input ``indata'' and
d664 1
a664 1
how parameters are passed to \SU\ programs.  The program {\bf sugain}
d670 1
a670 1
By the way, space around the \unix\
d676 1
a676 1
Table~\ref{SU:tab:unix} are the basic grammar of \unix;
d680 1
a680 1
\SU\ shell programs are significantly enhanced by
d683 2
a684 2
Many built-in \unix\ commands do not have a self-documentation
facility like \SU's---instead, most do have ``man'' pages.
d711 2
a712 2
You need to know a bit more \unix\ lore
to use \SU\ efficiently---we'll introduce these tricks of the trade in
d717 1
a717 1
This section is a simulated example of an interactive session with \SU.
d719 1
a719 1
Later we will discuss the construction of \SU\ processing streams, our
d829 1
a829 1
\SU\ program names are often obscure (we aren't proud of this).
d842 1
a842 1
of the \SU\ programs.
d844 1
a844 1
Note that we used the \Tt{-n} option of the  {\bf sufind} command.  Using the self-doc facility, we can get the full story:
d856 2
a857 2
The essence of good \SU\ usage is constructing (or cloning!)
\unix\ shell programs to create and record processing flows.
d861 1
a861 1
Most \SU\ programs read from standard input and write to standard output.
d863 2
a864 2
connecting \SU\ programs with \unix\ pipes.
Most flows will end with one of the \SU\ plotting programs.
d866 1
a866 1
parameter settings, it is convenient to put the \SU\ commands in a
d869 1
a869 1
{\bf Remark}: All the \unix\ shells, Bourne (sh), Cshell (csh),
d899 1
a899 1
of the line is not executed by the \unix\ shell.  The combination
d911 1
a911 1
path name is a \unix\ maintained environment variable
d917 1
a917 1
{\bf WARNING!}  Spaces are significant to the \unix\ shell---it  uses
d920 1
a920 1
(Somewhere around 1977, one author's (Jack) first attempt to learn \unix\ was
d934 1
a934 1
backslash is one of the most common and frustrating bugs in \unix\
d944 1
a944 1
The \SU\ plotting programs are special---their self-doc doesn't
d948 1
a948 1
generic \CWP\ plotting program {\bf ximage}.  This apparent flaw
d950 1
a950 1
effect of a key \SU\ design decision.  The \SU\ graphics
d955 1
a955 1
but it implies a basic limitation in \SU's graphical capabilities.
d959 1
a959 1
through the self-documentation for {\em both} the ``\SU\ jacket'' programs
d966 1
a966 1
The simplest way to execute a \unix\ shell program is to give
d1025 1
a1025 1
\item Set a debugging mode that asks \unix\
d1039 1
a1039 1
documentation of the processing parameters used.  (\SU\ does not have
d1046 1
a1046 1
of \SU\ parameter vectors: comma-separated strings with no spaces.
d1088 1
a1088 1
greatly extend the reach of \SU\ without writing C code.
d1092 1
a1092 1
It is a sad fact that the \unix\ shell is not
d1096 1
a1096 1
common \unix\ shell programming idioms.
d1102 1
a1102 1
not contained in any single \SU\ program.
d1208 1
a1208 1
data set, we only need it once.  The solution is to use the \unix\ stream
d1220 1
a1220 1
arithmetic.  Where this is needed, we use the \unix\ built-in
d1224 1
a1224 1
but as seconds in \SU\ codes.  Note carefully the {\em back}quotes
d1259 1
a1259 1
{\bf WARNING!}  OK, now you know that there is a \unix\ command
d1304 2
a1305 2
Reading the self-documentation and trying out the following \SU\ programs
will give you a good start in learning \SU. 
d1338 1
a1338 1
The \SU\ software is a layered product.  The layers correspond to the
d1343 1
a1343 1
\item{\bf par} Library supporting the \CWP\ programming
d1385 2
a1386 2
This chapter addresses questions often asked by new \SU\ users.
Some answers refer to the directory \Tt{CWPROOT}.  We use this
d1388 1
a1388 1
this directory name during the \SU\ installation procedure.
d1397 1
a1397 1
I get error messages about missing \Tt{fgetpos} and \Tt{fsetpos} routines, 
d1405 2
a1406 2
These {\sf SUN} systems may not have the \Tt{fgetpos} and \Tt{fsetpos} subroutines defined.
Because these two routines are not currently used in the \SU\ package,
d1420 1
a1420 1
\Tt{strtoul}, and/or \Tt{strerror} routines, even though I am using
d1490 1
a1490 1
that are in various formats into \SU\ format.
d1493 1
a1493 1
What is the data format that \SU\ programs expect?
d1497 1
a1497 1
The \SU\ data format is based on the {\sf SEG-Y} format. The \SU\ format
d1499 1
a1499 1
The \SU\ trace header is identical to {\sf SEG-Y} trace header.
d1505 2
a1506 2
at different sites.  \SU\ itself makes use of certain of these fields.
Thus, you may need to use \Tt{segyclean}---see the answer to
d1508 1
a1508 1
\SU\ format does not have the binary and ebcdic tape headers that
d1512 1
a1512 1
{\sf SEG-Y}/\SU\ header by typing: 
d1516 1
a1516 1
This lists the include file \Tt{segy.h} that defines the \SU\ trace header.
d1522 1
a1522 1
our data for processing using the \SU\ package?
d1530 2
a1531 2
that would be created by a C program, then use \Tt{suaddhead} to
put \SU\ ({\sf SEG-Y}) trace headers on the data. Example:
d1549 1
a1549 1
\Tt{data.ascii}.
d1560 1
a1560 1
\Tt{suaddhead}, \Tt{a2b}, and~\Tt{recast}.
d1565 1
a1565 1
I used \Tt{segyread} to read a {\sf SEG-Y} tape.
d1573 2
a1574 2
\Tt{segyclean} to zero the optional {\sf SEG-Y} trace header field.
If the \SU\ programs see nonzero values in certain parts
d1591 2
a1592 2
I am trying to plot data with the \Tt{pswigb}
(or \Tt{pswigp}, or \Tt{xwigb}, or  \ldots)
d1602 2
a1603 2
\Tt{pswigb}, \Tt{pswigp}, \Tt{pscontour}, \Tt{pscube}, \Tt{psmovie},
\Tt{xwigb}, \Tt{xgraph}, and~\Tt{xmovie},
d1605 1
a1605 1
If your data are \SU\ data ({\sf SEG-Y}) traces,
d1613 2
a1614 2
\Tt{supswigb}, \Tt{supswigp}, \Tt{supscontour}, \Tt{supscube},
\Tt{supsmovie}, \Tt{suxwigb}, \Tt{suxgraph}, or~\Tt{suxmovie}.}
d1617 1
a1617 1
latter programs.  The \Tt{su}-versions of the codes determine
d1620 1
a1620 1
handled by the version without the \Tt{su} prefix.)
d1633 1
a1633 1
The \SU\ data ({\sf SEG-Y} traces)
d1639 1
a1639 1
is the number that the \unix\ command \Tt{ls -l} shows.
d1643 1
a1643 1
in \Tt{segy.h} has not been altered.  Watch out as machines
d1649 1
a1649 1
I have some data in Fortran form and tried to convert it to \SU\ data
d1665 1
a1665 1
The program \Tt{ftnstrip} can often succeed in converting
d1667 1
a1667 1
\verb:ftn=1: option in \Tt{suaddhead} fails.
d1678 2
a1679 2
You need to put \Tt{CWPROOT/bin} (where \Tt{CWPROOT}
is \Tt{/your/root/path} that
d1682 1
a1682 1
in your shell \Tt{PATH}. This is done in your \Tt{.cshrc} file
d1684 3
a1686 3
\Tt{csh} or \Tt{tcsh}.
In Bourne shell (\Tt{sh}), Born Again shell (\Tt{bash}), or Korn shell
(\Tt{ksh}) the \Tt{PATH} variable is in your \Tt{.profile} file.
d1692 1
a1692 1
if you are running C-shell \Tt{/bin/csh} or  TC-shell \Tt{/bin/tcsh}
d1698 1
a1698 1
How do I transfer data between \SU\ and a commercial package, such
d1719 1
a1719 1
\noindent To go from the commercial package to \SU\ follow
d1770 1
a1770 1
For data that are in the \SU\ format, the program {\bf suswapbytes} is
d1778 1
a1778 1
In older releases of \SU\ there were problems with the bitwise operations
d1792 1
a1792 1
on any system where \SU\ has been installed.
d1795 1
a1795 1
read it into the \SU\ format via:
d1804 1
a1804 1
and writing {\sf SEG-Y} tapes with \SU.
d1811 1
a1811 1
    will need to use \Tt{smit} to set \Tt{blocksize=0} on your tape
d1816 2
a1817 2
     rewinding device. On an \Tt{RS6000} this would be
      something like \Tt{/dev/rmtx.1}, see \Tt{man mt} for details.
d1822 1
a1822 1
Here, \Tt{/dev/rmtx} (not the real name of the device,
d1825 2
a1826 2
In the option, \Tt{bs=32767}, we gave the right blocksize ($2^{16}+1$)
for an \Tt{IBM/RS6000}.  Try
d1984 1
a1984 1
This section addresses general questions about the \SU\ package.
d2024 1
a2024 1
to the \Tt{SU NEWS} email messages.
d2058 1
a2058 1
the parameter \verb:par=parfile:, where \Tt{parfile} is a file containing
d2199 2
a2200 2
This method may be used to convert scanned images to \SU\ format,
as well, with the next step in the procedure to be putting \SU\
d2207 2
a2208 2
Although variations are usually needed, a template for a typical \SU\ program
looks like the program listing below (we excerpted lines from the program \Tt{sumute} to build this template).  The numbers in square brackets at the end of the lines in the listing are not part of the listing---we added them to facilitate discussion of the template.  The secret to efficient \SU\ coding is finding an existing program similar to the one you want to write.  If you have trouble locating the right code or codes to ``clone,'' ask us---this can be the toughest part of the job!
d2211 1
a2211 1
/* SUMUTE: $Revision: 1.13 $ ; $Date: 1996/09/20 21:33:50 $      */  [1]
d2286 2
a2287 2
\item We maintain the internal versions of the codes with the \unix\ utility {\sf RCS}.  This item shows the string template for {\sf RCS}.
\item The file \Tt{su.h} includes (directly or indirectly) all our locally defined macros and prototypes.  The file \Tt{segy.h} has the definitions for the trace header fields.
d2289 1
a2289 1
\item This is an external declaration of an \SU\ ({\sf SEG-Y}) trace buffer.  It is external to avoid wasting stack space.
d2292 3
a2294 3
(there is no official \SU\ naming standard).
\item The \Tt{initargs} subroutine sets \SU's command line passing facility (see page~\pageref{SU:page:getpar}).
\item The \Tt{requestdoc} subroutine call specifies the circumstances under which self-doc will be echoed to the user.  The argument `1' applies to the  typical program that uses only standard input (i.e. \verb+<+) to read an \SU\ trace file.  Use `0' for codes that create synthetic data (like \Tt{suplane}) and `2' for codes that require two input files (we could say ``et cetera,'' but there are no existing \SU\ mains that require {\em three} or more input files).
d2296 2
a2297 2
\item Read the first trace, exit if empty.  The subroutine \Tt{fgettr} ``knows about'' the \SU\ trace format.  Usually the trace file is read from standard input and then we use \Tt{gettr} which is a macro based on \Tt{fgettr} defined in \Tt{su.h}.  Note that this code implies that the first trace is read into the trace buffer (here called \Tt{tr}), therefore we will have to process this trace before the next call to \Tt{fgettr}.
\item We've read that first trace because, we need to get some trace parameters from the first trace header.  Usually these are items like the number of samples (\Tt{tr.ns}) and/or the sampling interval (\Tt{tr.dt}) that, by the {\sf SEGY-Y} standard, are the same for all traces.
d2300 3
a2302 3
\item This is the seismic algorithm--here incomplete.  We've left in some of the actual \Tt{sumute} code because it happens to contains lines that will be useful in the new code, we'll be writing below.  You may want to call a subroutine here to do the real work.
\item \Tt{fputtr} and \Tt{puttr} are the output analogs of \Tt{fgettr} and \Tt{gettr}.
\item The loop end.  \Tt{gettr} returns a 0 when the trace file is exhausted and the processing then stops.
d2306 1
a2306 1
\section{Writing a new program: \Tt{suvlength}}
d2308 1
a2308 1
A user asked about \SU\ processing for variable length traces.  At his
d2310 1
a2310 1
termination time.  The difficulty is that \SU\ processing is based on
d2312 1
a2312 1
be of the same length.  Rather than contemplating changing all of \SU,
d2316 1
a2316 1
\Tt{suvlength}.  We can make the length of the output traces a user
d2323 2
a2324 2
fundamental trace getting facility, \Tt{gettr}, itself assumes fixed
length traces (or perhaps we should say that \Tt{gettr} deliberately
d2326 1
a2326 1
it, you'll realize that \Tt{gettr} itself has to take special measures
d2331 1
a2331 1
template above into the new \Tt{suvlength} code:
d2334 1
a2334 1
/* SUVLENGTH: $Revision: 1.13 $ ; $Date: 1996/09/20 21:33:50 $   */
d2404 1
a2404 1
succinctly by using the {\sf ANSI-C} routine \Tt{memset}.  However, we
d2406 3
a2408 3
use this routine.  One solution is to \Tt{cd} to the \Tt{su/main}
directory and use \Tt{grep} to find other uses of \Tt{memset}.  When
we did this, we found that \Tt{sumute} had usage closest to what we
d2410 1
a2410 1
the complete main for \Tt{suvlength}:
d2412 1
a2412 1
/* SUVLENGTH: $Revision: 1.13 $ ; $Date: 1996/09/20 21:33:50 $        */
d2490 1
a2490 1
{\bf Remark}: In the actual \SU, the subroutine \Tt{fvgettr} has been
d2492 1
a2492 1
\Tt{vgettr} for the case of standard input.  But these are secondary
d2495 1
a2495 1
For any new \SU\ code, one should provide an example shell program to show how
d2556 1
a2556 1
      Unix Release 29: a free package for seismic research and processing, \\
@


1.13
log
@Minor change in history per Shuki.
@
text
@a10 3
\newcommand{\Em}[1]{{\em #1\/}}
\newcommand{\Bf}[1]{{\bf #1}}
\newcommand{\Sf}[1]{{\small\sf #1}}
d21 18
a38 17
\begin{titlepage}
\pagenumbering{roman}
\vspace*{2.5 in}
\centerline{\Huge\bf The SU User's Manual}
\vspace{0.5in}
\centerline{\Large Jack K. Cohen \& John W. Stockwell, Jr.}
\vspace{0.5in}
\centerline{\Large Version 1.5: August, 1996}

\vfill

\centerline{\epsfysize=0.675in \epsffile{cwplogo.eps}}
\vspace{1ex}
\centerline{Center for Wave Phenomena}
\centerline{Colorado School of Mines}

\end{titlepage}
d51 2
a52 2
Geophysicists (\Sf{SEG}) for their generous grant, which has
made Release 29 of the \Sf{CWP/SU} package possible.
d57 1
a57 1
supplied in the past by \Sf{IBM} Corporation and by the Center for
d94 1
a94 1
electronic text, \Em{Theory of Seismic Imaging}, Samizdat Press, 1994.
a109 1

d120 1
a120 1
The \SU\ package is \Em{free software}, meaning that you may have
d132 1
a132 1
protocol (\Sf{SEG-Y}).  It provides a standard environment for the
d144 1
a144 1
assume that the data are written in \Sf{SEG-Y} format
d149 1
a149 1
to the \Sf{SEG-Y} format.
d159 1
a159 1
ranging from mainframes to workstations and \Sf{PC}'s.
d244 1
a244 1
\item An \Sf{ANSI C} compiler.
d260 1
a260 1
\item PC's running LINUX, PRIME TIME, ESIX, and NeXTSTEP 486
d263 6
a268 1
There are README files in the distribution with special notes about some of these platforms.  We depend on the \SU\ user community to alert us to installation problems, so if you run into difficulties, please let us know.
a269 1
The distribution contains a series of files that detail the installation process.  Read them in the following order:
d271 1
a271 1
LICENSE --- legal statement
a274 1
README_X --- X-windows install information
d278 1
d280 7
a287 1
\section{A quick test}
d292 1
a292 1
suplane | suxwigb &
d294 3
a296 1
should produce the graphic shown in Figure~\ref{fig:suplane}.  If you have a PostScript printer, then you should get a hard copy version with the pipeline
d300 14
a313 1
If you have display PostScript, then to get a screen display, replace the \verb:lpr: command in the pipeline by the command that opens a PostScript file.
d315 5
a319 5
\begin{figure}[htbp]
\epsfysize 240pt
\centerline{\epsffile{suplane.eps}}
\caption{Output of the \protect\verb:suplane: pipeline.}
\label{fig:suplane}
d351 1
a351 1
\Bf{suhelp} lists the names of the programs in the distribution.  The first ``page'' of the output looks something like this:
d372 1
a372 1
\Bf{suname} is a program that lists the names and a one-line description
d405 1
a405 1
\Bf{sudoc} is a utility that lists the self-documentation
d443 1
a443 1
\Bf{sufind} is a program that searches the self-documentations
d506 1
a506 1
header fields.  The \Bf{sukeyword} program enables the user to list
d539 2
a540 2
\item \Bf{gendocs} is a program that creates the LaTeX document,
  \Bf{selfdocs.tex}, that contains a complete set of all the
d548 1
a548 1
\item The top level \Bf{demos} directory contains a number of tutorial
d551 1
a551 1
  \Bf{demos} directory, here is a roadmap to get you started:
d577 1
a577 1
  that interest you.  The \Bf{demos} directory tree is still under
d582 1
a582 1
  to carry out coordinated data processing.  The \Bf{su/examples}
d587 5
a591 1
\item The text book, \Em{Theory of Seismic Imaging}, by John A.
d608 4
d653 2
a654 2
the program \Bf{sufilter} with the \verb:<: operator, and similarly, the output data set \verb:outdata: receives the data because of the \verb:>: operator.
The output of \Bf{sufilter} is connected to the input of \Bf{sugain} by use of the \verb:|: operator.
d657 1
a657 1
how parameters are passed to \SU\ programs.  The program \Bf{sugain}
d659 1
a659 1
the program \Bf{sufilter} receives the assigned four component \Em{vector}
d666 1
a666 1
operator are \Em{not} permitted.
d715 1
a715 1
Let's by using \Bf{sufind} to see if there are any \Sf{DMO} programs:
d739 1
a739 1
but we see that two \Sf{DMO} programs have been found.
d741 1
a741 1
Use the self-doc facility to get more information about \Bf{sudmofk}:
d790 2
a791 2
these names are \Em{not} standard and, as mentioned earlier, you can
get definitions by using \Bf{sukeyword}.  For example,
d834 1
a834 1
Recall also that \Bf{suhelp} and \Bf{suname} give comprehensive listings
d837 1
a837 1
Note that we used the \Tt{-n} option of the  \Bf{sufind} command.  Using the self-doc facility, we can get the full story:
d862 1
a862 1
\Bf{Remark}: All the \unix\ shells, Bourne (sh), Cshell (csh),
d866 1
a866 1
Our first example is a simple shell program called \Bf{Plot}.
d888 1
a888 1
\Bf{Discussion of numbered lines:}
d910 1
a910 1
\Bf{WARNING!}  Spaces are significant to the \unix\ shell---it  uses
d912 1
a912 1
making code easy to read, do \Em{not} put spaces next to the \verb:=: symbol.
d916 2
a917 2
\item The main pipeline of this shell code selects a certain set of cmp gathers with \Bf{suwind}, gains this subset with \Bf{sugain} and pipes the result into
the plotting program \Bf{suximage}.  As indicated in the Usage comment,
d922 1
a922 1
\item The lines within the \Bf{suximage} command are continued by the
d925 1
a925 1
\noindent\Bf{WARNING!}  The line continuation backslash must be the \Em{final}
d933 3
a935 3
usage---the \verb:&: should \Em{not} be used with the analogous PostScript
plotting programs (e.g., supsimage).  For example, with \Bf{supsimage} in
place of \Bf{suximage}, the \verb:&: might be replaced by \verb:| lpr:.
d939 1
a939 1
accepted by \Bf{suximage}
d941 1
a941 1
generic \CWP\ plotting program \Bf{ximage}.  This apparent flaw
d952 3
a954 3
through the self-documentation for \Em{both} the ``\SU\ jacket'' programs
(\Bf{suximage}, \Bf{suxwigb}, \ldots) and the generic plotting
programs (\Bf{ximage}, \Bf{xwigb}, \ldots).
d960 1
a960 1
it ``execute permission.''  For example, to make our above \Bf{Plot} shell
d982 1
a982 1
Suppose you want to use \Bf{sudmofk}.  You've read the self-doc, but
d984 3
a986 3
the directory \Bf{su/examples}.  In this case, we are lucky and find
the shell program, \Bf{Dmo}.  Again, the numbers in square brackets at the
end of the lines shown below are \Em{not} part of the listing.
d1008 1
a1008 1
\Bf{Discussion of numbered lines:}
d1040 1
a1040 1
The nmo program (\Bf{sunmo}) will give an error message and abort
d1043 1
a1043 1
\item Note that \Bf{susort} allows the use of \Em{secondary}
d1076 2
a1077 2
Related shell programs are \Bf{su/examples/Nmostack} and
\Bf{su/examples/Mig}.
d1082 2
a1083 2
See, for example, \Bf{CvStack}, \Bf{FilterTest}, \Bf{FirstBreak}, and
\Bf{Velan} in \Bf{su/examples}.
d1091 1
a1091 1
We use \Bf{CvStack} as an
d1094 1
a1094 1
\Em{velocity panels}---a concept
d1097 2
a1098 2
\Bf{Remark}:  For most of us,
writing a shell like \Bf{CvStack} from scratch is a time-consuming affair.
d1107 1
a1107 1
but instead focus on the new features used in \Bf{CvStack}.
d1177 1
a1177 1
\Bf{Discussion of numbered lines:}
d1181 1
a1181 1
from the first trace header of the data set.  The program \Bf{sugethw}
d1200 1
a1200 1
Although \Bf{sugethw} is eager to give the values for every trace in the
d1202 2
a1203 2
editor (\Bf{sed}).  In fact, we use it twice.  By default, \Bf{sed} passes
along its input to its output.  Our first use is merely to tell \Bf{sed}
d1205 2
a1206 2
\Bf{sed} strips off the unwanted material before the integer.
In detail, the second \Bf{sed} command reads: replace (or substitute)
d1214 1
a1214 1
\Bf{bc} calculator program with the ``here document'' facility.
d1216 2
a1217 2
is given in micro-seconds in the \Sf{SEG-Y} header,
but as seconds in \SU\ codes.  Note carefully the \Em{back}quotes
d1226 1
a1226 1
characters again.  For more information about \Bf{bc}:
d1235 1
a1235 1
operator!  If you intend to \Em{append}, then, as mentioned earlier, use
d1242 1
a1242 1
\Bf{Caveat}: The bracket notation is a nice
d1247 1
a1247 1
Because the bracket notation is not documented on the typical \Bf{sh} manual
d1249 1
a1249 1
all modern \Bf{sh} commands support it---please let us know
d1252 1
a1252 1
\Bf{WARNING!}  OK, now you know that there is a \unix\ command
d1264 1
a1264 1
the \Bf{sh} manual page says that the termination should contain
d1268 1
a1268 1
\Bf{bc} for an integer arithmetic calculation even though
d1279 1
a1279 1
\item \Bf{sunull} is a program I (Jack) wrote to create all-zero traces
d1341 1
a1341 1
\Sf{SEG-Y} trace structure.    Subroutines that manage this
d1347 1
a1347 1
        \item \Bf{psplot}---PostScript graphics:
d1358 1
a1358 1
        \item \Bf{xplot}---xlib based X Windows graphics
d1380 1
a1380 1
symbolic name for the directory that contains the \Sf{CWP/SU} source code, include files, libraries, and executables.  You are asked to specify
d1385 1
a1385 1
process is found in the \Sf{README}
d1391 1
a1391 1
even though I am using the \Sf{GCC} compiler.
d1397 2
a1398 2
\Sf{SUN OS} 4.xx (pre-\Sf{SOLARIS}). 
These \Sf{SUN} systems may not have the \Tt{fgetpos} and \Tt{fsetpos} subroutines defined.
d1402 1
a1402 1
Please uncomment the \Sf{OPTC} line in the paragraph in Makefile.config
d1414 1
a1414 1
the \Sf{GCC} compiler. How do I get around this problem?
d1418 1
a1418 1
Again, this is  most often seen with the older \Sf{SUN OS}. 
d1424 2
a1425 2
Why do I get missing subroutine messages about \Sf{ANSI C} routines?
Isn't the \Sf{GCC} compiler supposed to be an \Sf{ANSI} compiler?
d1429 1
a1429 1
The \Sf{GCC} compiler is just that, a compiler. It
d1431 1
a1431 1
If the \Sf{GNU} libraries (this is the "glibc" package)
d1433 1
a1433 1
\Sf{GCC} compiler will use the libraries that are native to the machine
d1435 43
a1477 3
not available in the \Sf{SUN 4. OS}, \Sf{GCC} does not recognize them.
However, installing the \Sf{GNU} libraries will make the \Sf{GCC} compiler
behave as a full \Sf{ANSI C} compiler.
d1490 1
a1490 1
The \SU\ data format is based on the \Sf{SEG-Y} format. The \SU\ format
d1492 1
a1492 1
The \SU\ trace header is identical to \Sf{SEG-Y} trace header.
d1496 2
a1497 2
\noindent\Bf{Caution}: The optional fields
in the \Sf{SEG-Y} trace header are used for different purposes
d1502 1
a1502 1
are part of the \Sf{SEG-Y} format.
d1505 1
a1505 1
\Sf{SEG-Y}/\SU\ header by typing: 
d1514 1
a1514 1
\Sf{SEG-Y} information to our own modeled data to prepare
d1524 1
a1524 1
put \SU\ (\Sf{SEG-Y}) trace headers on the data. Example:
d1537 1
a1537 1
\item If your data are \Sf{ASCII}, then use:
d1558 1
a1558 1
I used \Tt{segyread} to read a \Sf{SEG-Y} tape.
d1565 2
a1566 2
When you read an \Sf{SEG-Y} tape, you need to pipe the data through
\Tt{segyclean} to zero the optional \Sf{SEG-Y} trace header field.
d1571 10
d1598 1
a1598 1
If your data are \SU\ data (\Sf{SEG-Y}) traces,
d1612 1
a1612 1
(In fact, that is \Em{all} they do---the actual graphics is
d1626 1
a1626 1
The \SU\ data (\Sf{SEG-Y} traces)
d1634 1
a1634 1
\Bf{Caveats}: The above calculations assume that you have
d1664 1
a1664 1
I just successfully installed the \Sf{CWP/SU} package, but when I
d1673 1
a1673 1
contains the \Sf{CWP/SU} source code, include files,
d1685 3
a1687 2
under \Tt{.csh} or \Tt{.tcsh}, if you
have not relogged since you compiled the codes. 
d1690 104
d1797 1
a1797 1
and writing \Sf{SEG-Y} tapes with \SU.
d1799 1
a1799 1
Tape reading/writing is more of an art than a science.
d1803 1
a1803 1
    length. If you are on an \Sf{IBM RS6000}, this means you
d1807 1
a1807 1
    will foil all attempts to read an \Sf{SEG-Y} tape.
d1820 1
a1820 1
\verb:bs=32765:  ($2^{16}-1$) on a \Sf{SUN}. 
d1865 1
a1865 1
The CWP/SU package does indeed, have provisions for getting and
d1870 1
d1875 1
d1877 2
a1878 1
Type the name of each program to see the self documentation of that code.
d1880 3
a1882 3
\noindent
In addition, to find out what the header field "keywords" mentioned
in these programs are:  type:    sukeyword -o
d1884 4
a1887 3
\noindent
You may have the information in a variety of forms. The most common
and least complicated assumptions of that form will be made here.
d1889 2
a1890 2
\noindent
The task requires the following basic steps.
d1892 1
d1902 3
a1904 1
        segyread tape=/dev/rmt0 bfile=data.1 header=h.1 | segyclean > data.su
d1908 3
a1910 2
        segyread tape=data.segy bfile=data.1 header=h.1 | segyclean > data.su

d1922 1
a1922 1

d1924 1
d1930 3
a1932 1
   % sugethw < data.su  output=geom  key=key1,key2,... > hfields.ascii
d1937 1
d1939 1
d1941 1
a1941 1
\item Once you have dumped the desired header fields  into  hfields.ascii
d1951 1
a1951 1
\begin{verbatim}
d1953 2
a1954 2

   Here,  N_columns is the number of columns in   hfile.ascii.
a1955 1
\end{verbatim}
a1956 1
\begin{verbatim}
d1958 1
d1960 1
a1960 1
\end{verbatim}
d1966 2
a1967 5
   field values, then you may use:
\begin{verbatim}
   suchw
\end{verbatim}
   for this. Also, if the header fields that you want to set are
d1976 7
d1984 10
d1995 3
a1997 1
\section{General}
d1999 10
a2008 1
This section addresses general questions about the \SU\ package.
d2011 1
a2011 1
Why are \Sf{CWP/SU} releases given by integers (22, 23, 24, etc...)
d2016 1
a2016 1
The \Sf{CWP/SU} release numbers are chosen to correspond
d2019 1
a2019 1
release numbers (assigned by \Sf{RCS}), but these are all different.
d2024 1
a2024 1
\Bf{Remark}:  In the early days, we \Em{did} use \Sf{RCS} to
d2034 1
a2034 1
The \Sf{CWP/SU} package is updated at roughly 3-6 month intervals.
d2041 1
a2041 1
I have a complicated collection of input parameters for a \Sf{CWP/SU}
d2048 1
a2048 1
\Sf{CWP/SU} programs that take their input parameters from the command
d2069 128
d2204 1
a2204 1
/* SUMUTE: $Revision: 1.12 $ ; $Date: 1996/09/20 21:16:54 $      */  [1]
d2276 1
a2276 1
\Bf{Discussion of numbered lines:}
d2279 1
a2279 1
\item We maintain the internal versions of the codes with the \unix\ utility \Sf{RCS}.  This item shows the string template for \Sf{RCS}.
d2282 1
a2282 1
\item This is an external declaration of an \SU\ (\Sf{SEG-Y}) trace buffer.  It is external to avoid wasting stack space.
d2287 2
a2288 2
\item The \Tt{requestdoc} subroutine call specifies the circumstances under which self-doc will be echoed to the user.  The argument `1' applies to the  typical program that uses only standard input (i.e. \verb+<+) to read an \SU\ trace file.  Use `0' for codes that create synthetic data (like \Tt{suplane}) and `2' for codes that require two input files (we could say ``et cetera,'' but there are no existing \SU\ mains that require \Em{three} or more input files).
\item This is typical code for reading `parameters from the command line.  Interpret it like this: ``If the user did not specify a value, then use the default value.''  The subroutine must be type-specific, here we are getting an \Em{integer} parameter.
d2290 3
a2292 3
\item We've read that first trace because, we need to get some trace parameters from the first trace header.  Usually these are items like the number of samples (\Tt{tr.ns}) and/or the sampling interval (\Tt{tr.dt}) that, by the \Sf{SEGY-Y} standard, are the same for all traces.
\item Since the first trace has been (typically) read before the main processing loop starts, we use a ``do-while'' that reads a new trace at the \Em{bottom} of the loop.
\item We favor using \Em{local} variables where permitted.
d2296 1
a2296 1
\item This is an \Sf{ANSI-C} macro conventionally used to indicate successful program termination.
d2304 1
a2304 1
the \Sf{SEG-Y} standard which mandates that all traces in the data set
d2320 1
a2320 1
with the \Em{first} trace to figure out its length.  All we have to do
d2322 1
a2322 1
logic for \Em{every} trace.  Here, we'll suppress the details of
d2327 1
a2327 1
/* SUVLENGTH: $Revision: 1.12 $ ; $Date: 1996/09/20 21:16:54 $   */
d2397 1
a2397 1
succinctly by using the \Sf{ANSI-C} routine \Tt{memset}.  However, we
d2405 1
a2405 1
/* SUVLENGTH: $Revision: 1.12 $ ; $Date: 1996/09/20 21:16:54 $        */
d2483 1
a2483 1
\Bf{Remark}: In the actual \SU, the subroutine \Tt{fvgettr} has been
d2516 3
d2525 1
a2525 1
\Bf{Disclaimer}:
d2531 1
a2531 1
\Bf{License}:
d2538 1
a2538 1
Colorado School of Mines (\Sf{CSM}) not be used in advertising or
d2544 1
a2544 1
\Bf{Referencing Seismic Unix:}
d2547 1
a2547 1
\Bf{Example reference:
@


1.12
log
@release 29 changes
@
text
@d69 7
a75 7
Shuki Ronen wrote the first draft of what is now called \SU\ in
cooperation with Einar Kjartansson while both were graduate students
at Stanford University.  In turn, some of the fundamental concepts
they implemented were formulated by their mentor, Jon Claerbout,
Director of the Stanford Exploration Project.  Ronen brought this work
to our Center during a two-year stay here and, during this time, aided
Cohen in turning \SU\ into a supportable and exportable product.
d1712 1
a1712 1
	segyread tape=/dev/rmt0 bfile=data.1 header=h.1 | segyclean > data.su
d1716 1
a1716 1
	segyread tape=data.segy bfile=data.1 header=h.1 | segyclean > data.su
d1854 1
a1854 1
/* SUMUTE: $Revision: 1.11 $ ; $Date: 1996/08/05 20:14:36 $      */  [1]
d1977 1
a1977 1
/* SUVLENGTH: $Revision: 1.11 $ ; $Date: 1996/08/05 20:14:36 $   */
d2055 1
a2055 1
/* SUVLENGTH: $Revision: 1.11 $ ; $Date: 1996/08/05 20:14:36 $        */
@


1.11
log
@added discussion of referencing SU, geometry setting, removed old
references to hilbert, updated IP address to 138.67.12.4, expanded
About SU section a small amount
@
text
@d4 1
a4 1
\scrollmode
d9 1
a9 1
%\topmargin -.5in
d48 2
a49 1
\addtocontents{toc}{{\bf Acknowledgments}}
d206 1
a206 1
\Tt{outside_usa}.
d1854 1
a1854 1
/* SUMUTE: $Revision: 1.10 $ ; $Date: 1996/02/07 21:50:06 $      */  [1]
d1977 1
a1977 1
/* SUVLENGTH: $Revision: 1.10 $ ; $Date: 1996/02/07 21:50:06 $   */
d2055 1
a2055 1
/* SUVLENGTH: $Revision: 1.10 $ ; $Date: 1996/02/07 21:50:06 $        */
@


1.10
log
@Removed GRI acks and logo, updated suvlength discussion.
@
text
@d31 1
a31 1
\centerline{\Large Version 1.4: Feb, 1996}
d51 4
a113 1

d119 13
a131 1
researchers.
a132 2
The \SU\ package is \Em{free software}, distributed quarterly with the
full source code, so that users can alter and extend its capabilities.
d159 2
a160 2
The \SU\ community is worldwide, spanning six continents, 29 countries
and over 400 known installations on a variety of hardware platforms
d168 1
a168 1
program in seismic exploration, supported by 25 companies in the oil
d171 5
d182 3
a184 1
hilbert.mines.edu (138.67.12.63). The directory path is pub/cwpcodes.
d203 4
d213 1
a213 1
\% ftp 138.67.12.63        & --- &  138.67.12.63 is our ftp site  \\
d246 1
a299 1

d1005 1
a1005 1
the subsidiary nmo processes.  Since these picks must be consistent
d1216 1
a1216 1
Since the bracket notation is not documented on the typical \Bf{sh} manual
d1367 11
a1377 7
These \Sf{SUN} systems may not have the \Tt{fgetpos} and \Tt{fsetpos} subroutines defined.  Since these two routines are not
currently used in the \SU\ package,
the easiest fix when these two functions are missing
functions is to comment out references
to the functions \Tt{efgetpos} and \Tt{efsetpos} in
\Tt{CWPROOT/src/par/include/par.h} and \Tt{CWPROOT/src/par/lib/subcalls.c}.
See also the answer to Question~\ref{SU:q:gcc}.
d1388 1
a1388 14
To fix the problem of a missing \Tt{strtoul} routine, replace:\\ \Tt{CWPROOT/src/par/lib/atopkge.c}\\
with the file:\\
\Tt{CWPROOT/src/par/lib/Portability/atopkge.c}.

\vspace{1ex}
\noindent
For a missing \Tt{strerror} routine, replace:\\
\Tt{CWPROOT/src/par/lib/errpkge.c}\\
with the file:\\
\Tt{CWPROOT/src/par/lib/Portability/errpkge.c}.

Each of these replacements, of the regular file by the \Tt{Portability}
version, entails a small loss of functionality.  See also the answer to
Question~\ref{SU:q:gcc}.
d1428 2
a1429 1
Thus, you may need to use \Tt{segyclean}---see the answer to Question~\ref{SU:q:segyclean}.
d1668 114
d1811 1
a1811 1
The \Sf{CWP/SU} package is updated at roughly 3 month intervals.
d1853 1
a1853 1
/* SUMUTE: $Revision: 1.9 $ ; $Date: 1996/01/30 15:33:03 $      */  [1]
d1976 1
a1976 1
/* SUVLENGTH: $Revision: 1.9 $ ; $Date: 1996/01/30 15:33:03 $   */
d2054 1
a2054 1
/* SUVLENGTH: $Revision: 1.9 $ ; $Date: 1996/01/30 15:33:03 $        */
d2178 1
a2178 1
Copyright (c) Colorado School of Mines, 1994\\
d2189 12
@


1.9
log
@fixed cwp logo
@
text
@d3 2
a4 1
\nofiles
d31 1
a31 1
\centerline{\Large Version 1.3: July, 1995}
d35 4
a38 5
\hspace{0.5in} \epsfysize=0.675in \epsffile{cwplogo.eps}
\hfill\epsfysize=.5in \epsffile{grilogo.eps}\hspace{0.5in} \\
\vspace{1ex} \\
$\qquad$Center for Wave Phenomena\hfill Gas Research Institute \\
$\qquad$Colorado School of Mines\hfill Chicago, Illinois
a50 5
We are pleased to acknowledge that \SU\ is partially supported by the
Gas Research Institute.  This welcome support comes at a time when
\SU\ is achieving worldwide distribution and thus is making greater
demands on our resources.

d97 6
a102 6
of Baltic Research in Warnemuende Germany, Stew Levin and Joe Oravetz
at Mobil Oil, Joe Dellinger at Amoco, Matthew Rutty in Australia, Jens
Hartmann in Germany, and Wenying Cai at the University of Utah.  Our
apologies in advance for undoubtedly omitting mention of other
deserving contributors to \SU---we promise to include you in future
updates of this manual!
d1723 1
a1723 1
/* SUMUTE: $Revision: 1.8 $ ; $Date: 1995/07/26 18:58:22 $      */  [1]
d1820 24
a1843 3
A user asked about \SU\ processing for variable length traces.  At his institute, data are collected from time of excitation to a variable termination time.  The difficulty is that \SU\ processing is based on the \Sf{SEG-Y} standard which mandates that all traces in the data set be of the same length.  Rather than contemplating changing all of \SU, it seems to us that the solution is to provide a program that converts the variable length data to fixed length data by padding with zeroes where necessary at the end of the traces---let's name this new program \Tt{suvlength}.  We can make the length of the output traces a user parameter.  If there is a reasonable choice, it makes sense to provide a default value for parameters.  Here, using the length of the first trace seems the best choice since that value can be ascertained before the main processing loop starts.

So far, so good.  But now our plan runs into a serious snag: the fundamental trace getting facility, \Tt{gettr}, itself assumes fixed length traces (or perhaps we should say that \Tt{gettr} deliberately enforces the fixed length trace standard).  But, if you think about it, you'll realize that \Tt{gettr} itself has to take special measures with the \Em{first} trace to figure out its length.   All we have to do is make a new trace getting routine that employs that first trace logic for \Em{every} trace!  Fortunately, we don't even have to do that since the same problem arose a few years ago and we wrote \Tt{fvgettr} at that time for the requirements of the \Tt{suvcat} program.  So as a first draft solution, we'll just copy in \Tt{fvgettr} as an in-code subroutine for our new program.  Let's begin converting our above template into the new \Tt{suvlength} code:
d1846 1
a1846 1
/* SUVLENGTH: $Revision: 1.8 $ ; $Date: 1995/07/26 18:58:22 $   */
a1862 4
"                                                                ",
" Trace header fields accessed: ns                               ",
" Trace header fields modified: ns                               ",
"                                                                ",
a1866 1
 *
d1895 1
a1895 1
/* fvgettr code from suvcat goes here */
d1899 4
a1902 1
Now we run into a small difficulty.  Our only parameter has a default value that is obtained only after we read in the first trace.  The obvious solution is to reverse the parameter getting and the trace getting in the template.  Thus we resume: 
d1912 11
a1922 1
Now comes the actual seismic algorithm---which is rather trivial in the present case:  add zeroes to the end of the input trace if the output length is specified greater than the input length.  We could write a simple loop to do the job, but the task is done most succinctly by using the \Sf{ANSI-C} routine \Tt{memset}.  However, we confess that unless we've used it recently, we usually forget how to use this routine.  One solution is to \Tt{cd} to the \Tt{su/main} directory and use \Tt{grep} to find other uses of \Tt{memset}.  When we did this, we found that \Tt{sumute} had usage closest to what we needed and that is why we started from a copy of that code.  Here is the complete main for \Tt{suvlength}:
d1924 1
a1924 1
/* SUVLENGTH: $Revision: 1.8 $ ; $Date: 1995/07/26 18:58:22 $        */
a1940 4
"                                                                 ",
" Trace header fields accessed:  ns                               ",
" Trace header fields modified:  ns                               ",
"                                                                 ",
d1946 3
a1994 1
 *
a1996 1
 *
a1998 1

d2002 4
a2005 1
Of course, now that \Tt{fvgettr} has been used in two codes, it should be extracted as a library function and we should make a convenience macro \Tt{vgettr} for the case of standard input.  But these are secondary considerations that don't arise for most applications.
a2011 1
# Use same graphics set-up as in demos (e.g. demos/Sorting_Traces/Demo/Xsort)
a2012 3
GRAPHER=xgraph
IMAGER=suximage
WIGGER=suxwigb
d2026 1
a2026 1
$WIGGER <vdata \
a2028 1
        -geometry ${WIDTH}x${HEIGHT}+${WIDTHOFF}+${HEIGHTOFF} \
a2030 1

a2032 5

# Clean up
rm tempdata vdata


a2033 32
and here is the PostScript equivalent:
{\small\begin{verbatim}
#! /bin/sh
# Trivial test of suvlength with PostScript graphics
# Use same graphics set-up as in demos (e.g. demos/Sorting_Traces/Demo/PSsort)
# set PostScript Previewer here if environment variable PSPREVIEWER not set
VIEWER=$PSPREVIEWER

GRAPHER=psgraph
IMAGER=supsimage
WIGGER=supswigp

>tempdata
>vdata
suplane >tempdata  # default is 32 traces with 64 samples per trace
suplane nt=72 >>tempdata
suvlength <tempdata ns=84 |
sushw key=tracl a=1 b=1 >vdata

# Plot the data 
$WIGGER <vdata perc=99 title="suvlength test"\
        label1="Time (sec)" label2="Traces" | $VIEWER

# Remove #comment sign on next line to test the header
#sugethw <vdata tracl ns | more

# Clean up
rm tempdata vdata
exit

\end{verbatim}}\noindent

@


1.8
log
@Added ack for Joe Dellinger
Updated internet address
Revised discussion of demos
Eliminated some Edit-induced wrapped lines, but many more to do!
@
text
@d8 1
a8 1
\topmargin -.5in
d34 1
a34 1
\hspace{0.5in} \epsfysize=0.675in \epsffile{/usr/local/lib/tex/inputs/cwplogo.eps}
d1728 1
a1728 1
/* SUMUTE: $Revision: 1.7 $ ; $Date: 1995/07/26 11:07:48 $      */  [1]
d1830 1
a1830 1
/* SUVLENGTH: $Revision: 1.7 $ ; $Date: 1995/07/26 11:07:48 $   */
d1900 1
a1900 1
/* SUVLENGTH: $Revision: 1.7 $ ; $Date: 1995/07/26 11:07:48 $        */
@


1.7
log
@*** empty log message ***
@
text
@d30 1
a30 1
\centerline{\Large Version 1.2: March, 1995}
d51 40
a90 1
We are pleased to acknowledge that \SU\ is partially supported by the Gas Research Institute.  This welcome support comes at a time when \SU\ is achieving worldwide distribution and thus is making greater demands on our resources.
d92 2
a93 12
The sponsors of the \CWP\ Consortium Project have long been partners in the \SU\ project and we are pleased to explicitly acknowledge that relationship
here.  In addition, we wish to acknowledge extra support supplied in the past by \Sf{IBM} Corporation and by the Center for Geoscientific Computing at the Colorado School of Mines during the period when \SU\ was ported to the modern workstation from its previous incarnation on graphics terminals.
 
There are so many faculty and students, both at our Center and elsewhere, who have contributed to \SU, that it is impossible to list them all here.  However, certain people have made such signal contributions that they deserve explicit mention.

Shuki Ronen wrote the first draft of what is now called \SU\ in cooperation with Einar Kjartansson while both were graduate students at Stanford University.  In turn, some of the fundamental concepts they implemented were formulated by their mentor, Jon Claerbout, Director of the Stanford Exploration Project.  Ronen brought this work to our Center during a two-year stay here and, during this time, aided Cohen in turning \SU\ into a supportable and exportable product.

Chris Liner, while a student at the Center, wrote most of the graphics codes used in the pre-workstation (i.e, graphics terminal) age of \SU.  Liner's broad knowledge of seismology and seismic processing enabled him to make a positive and continuing influence on the \SU\ coding philosophy.

Craig Artley, now at Golden Geophysical, made major contributions to the graphics codes while a student at CWP and continues to make significant contributions to the general package.

Dave Hale wrote several of the ``heavy duty'' processing codes as well as most of the core scientific and graphics libraries.  His knowledge of good C-language coding practice helped make our package a good example for applied computer scientists.
d95 2
a96 3
Ken Larner contributed many user interface ideas based on his extensive knowledge of seismic processing in the ``real world.''

John Scales showed how to use \SU\ effectively in the classroom in his electronic text, \Em{Theory of Seismic Imaging}, Samizdat Press, 1994.
d99 9
a107 1
We also have had extensive help from the worldwide SU community.  Among those who should be singled out for mention are Tony Kocurko at Memorial University in Newfoundland, Toralf Foerster of the Institut of Baltic Research in Warnemuende Germany, Stew Levin and Joe Oravetz at Mobil Oil,  Matthew Rutty in Australia, Jens Hartmann in Germany, and Wenying Cai at the University of Utah.  Our apologies in advance for undoubtedly omitting mention of other deserving contributors to \SU---we promise to include you in future updates of this manual!
d123 2
a124 2
The \SU\ package is \Em{free software}, distributed quarterly with the full
source code, so that users can alter and extend its capabilities.
d127 9
a135 9
testing of new processing algorithms. It is easy to use because it does not require
learning a special language---its application uses only the standard
facilities afforded by the \unix\ operating system.  Once \unix\ shell-redirecting and pipes are mastered,
there is no further artificial language to learn.
The seismic commands and options can be used as readily as
other \unix\ commands.  In particular, the user
can write ordinary \unix\ shell scripts to combine frequent
command combinations into meta-commands (i.e., processing flows).
These scripts can be thought of as ``job files.''
d145 17
a161 16
The \SU\ user community accesses the software over the Internet
using the commonly available ``anonymous ftp'' facility.
In this way, users obtain the software with no constraints on its use.
During installation, the user is given the option of
sending an electronic mail request to add them to our user group.
Members of the user group receive announcements when
updates of the package become available.
The \SU\ community is now worldwide, spanning six continents, more than
26 countries and over 325 known installations on a variety of hardware platforms ranging from mainframes to workstations and \Sf{PC}'s.

Parts of \SU\ originated from software developed by students
working with the Stanford Exploration Project at Stanford University.
The present package was developed and is maintained at the Center for Wave Phenomena (\CWP) at the Colorado School of Mines.
\CWP\ is an interdisciplinary (geophysics, mathematics) research
and educational program in seismic exploration, supported by 25
companies in the oil and gas industry.
d169 2
a170 1
hilbert.mines.colorado.edu (138.67.12.63). The directory path is pub/cwpcodes.  Take the files:
d199 1
a199 1
& & when you are in ftp	
d497 5
a501 1
\item \Bf{gendocs} is a program that creates the LaTeX document, \Bf{selfdocs.tex}, that contains a complete set of all the self-documentations in the distribution (over 300 pages!).  A PostScript version of this document is available in our anonymous ftp site\\ (\verb:pub/cwpcodes/documentation.xx.tar.Z:).
d506 32
a537 18
\item The top level \Bf{demos} directory contains a number of tutorial shell scripts.  Its subdirectories contain \verb:README: files that give
detailed information.  Assuming that you start in the \Bf{demos} directory,
here is a roadmap to get you started:

The Sorting\_Traces Tutorial is an interactive script that
reinforces some of the basic \unix\ and \SU\ lore discussed in
this document.  The interactivity is limited to allowing you to
set the pace.  Such tutorials quickly get annoying, but we felt
that one such was needed.  The Sorting\_Traces Demo is the format
used in the rest of the \Bf{demos} directory---it just pops
up a bunch of windows and assumes you'll read the script if
you want to see how a particular window is evoked.

The next step is to activate the Selecting\_Traces Demo.  Then proceed to
the Deconvolution Demo and the NMO Demo.  Beyond that, visit the Demo
directories that interest you.  The \Bf{demos} directory tree is
still under active development---please let us know if the demos are
helpful and how they can be improved.
d540 11
a550 11
to carry out coordinated data processing.  The \Bf{su/examples} directory
contains a number of such programs.  By the way, the terms
``shell scripts,'' ``shell programs,'' ``shell files,'' and ``shells,''
are used interchangeably in the \unix\ literature.

\item The text book, \Em{Theory of Seismic Imaging}, by John A. Scales,
Samizdat Press, 1994, is available in our anonymous ftp site in both 300
and 400 dots per inch PostScript format:
\verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or
\verb:imaging_400dpi.ps.Z:.  The exercises
in this text make extensive use of \SU.
d558 4
a561 4
Section~\ref{SU:sec:template} explains the key \SU\ coding idioms.
Please let us know if you discover any inconsistencies between the source
and our documentation of it.  We also welcome suggestions for improving
the comments and style of our codes. 
d1104 2
a1105 2
	v=`bc -l <<END
        	$v + $dv
d1296 25
a1320 25
	\begin{enumerate}
	\item \Bf{psplot}---PostScript graphics:
		\begin{enumerate}
		\item pscontour: contour plots
		\item pscube: 3D data cube
		\item psgraph: curve plotting
		\item psimage: raster plotting
		\item psmovie: supports frames
		\item pswigb: bit mapped wiggle traces (fast)
		\item pswigp: polygon wiggle traces (slow)
		\item PostScript support programs
		\end{enumerate}
	\item \Bf{xplot}---xlib based X Windows graphics
		\begin{enumerate}
		\item ximage: raster plotting
		\item xwigb: bit mapped wiggle traces
		\item X Windows support programs
		\end{enumerate}
	\item \bf{Xtcwp}---toolkit based X Windows graphics
		\begin{enumerate}
		\item xgraph: curve plotting
		\item xmovie: supports frames
		\item X Windows resource files
		\end{enumerate}
	\end{enumerate}
d1648 3
a1650 3
	j=`expr $j + 1`
	echo "writing tape file  $j"
	segywrite tape=$DEV bfile=b.$j hfile=h.$j verbose=1 buff=0 < ozdata.$j
d1728 1
a1728 1
/* SUMUTE: $Revision: 1.6 $ ; $Date: 95/03/01 10:46:57 $	*/  [1]
d1830 1
a1830 1
/* SUVLENGTH: $Revision: 1.6 $ ; $Date: 95/03/01 10:46:57 $	*/
d1875 1
a1875 1
	
d1900 1
a1900 1
/* SUVLENGTH: $Revision: 1.6 $ ; $Date: 95/03/01 10:46:57 $        */
@


1.6
log
@Tweak about demos directory location per senior group.
@
text
@d2 1
a3 1
% make text fill more of the page
a9 1
\scrollmode
d41 1
a41 1
\input{legal.tex}
d1672 1
a1672 1
/* SUMUTE: $Revision: 1.5 $ ; $Date: 95/03/01 10:33:05 $	*/  [1]
d1774 1
a1774 1
/* SUVLENGTH: $Revision: 1.5 $ ; $Date: 95/03/01 10:33:05 $	*/
d1844 1
a1844 1
/* SUVLENGTH: $Revision: 1.5 $ ; $Date: 95/03/01 10:33:05 $        */
@


1.5
log
@Tweaks by Jack and additional of external contributor acknowledgements.
@
text
@d465 1
a465 1
\item The \Bf{demos} directory contains a number of tutorial shell scripts.  Its subdirectories contain \verb:README: files that give
d1673 1
a1673 1
/* SUMUTE: $Revision: 1.4 $ ; $Date: 94/10/31 09:53:45 $	*/  [1]
d1775 1
a1775 1
/* SUVLENGTH: $Revision: 1.4 $ ; $Date: 94/10/31 09:53:45 $	*/
d1845 1
a1845 1
/* SUVLENGTH: $Revision: 1.4 $ ; $Date: 94/10/31 09:53:45 $        */
@


1.4
log
@Cvstack --> CvStack
@
text
@d31 1
a31 1
\centerline{\Large Version 1.1: October, 1994}
d57 1
a57 1
There are so many faculty and students, both at our Center and elsewhere, who have contributed to \SU, that it is impossible to list them all here.  However, certain people have made such signal contributions that they deserve mention whenever the package is discussed.
d63 2
d72 3
a74 1
We thank Barbara McLenon for her detailed suggestions on the text
d118 1
a118 1
26 countries and over 290 known installations on a variety of hardware platforms ranging from mainframes to workstations and \Sf{PC}'s.
d124 1
a124 1
and educational program in seismic exploration, supported by 23
d1641 1
a1641 1
what do I do?
d1659 1
a1659 1
The string
d1673 1
a1673 1
/* SUMUTE: $Revision: 1.3 $ ; $Date: 94/10/13 09:17:39 $	*/  [1]
d1775 1
a1775 1
/* SUVLENGTH: $Revision: 1.3 $ ; $Date: 94/10/13 09:17:39 $	*/
d1845 1
a1845 1
/* SUVLENGTH: $Revision: 1.3 $ ; $Date: 94/10/13 09:17:39 $        */
@


1.3
log
@Corrected small lacuna in acknowledgements.
Checking in mainly to benchmark this version.
@
text
@d973 1
a973 1
See, for example, \Bf{Cvstack}, \Bf{Filtertest}, \Bf{Firstbreak}, and
d982 1
a982 1
We use \Bf{Cvstack} as an
d989 1
a989 1
writing a shell like \Bf{Cvstack} from scratch is a time-consuming affair.
d998 1
a998 1
but instead focus on the new features used in \Bf{Cvstack}.
d1165 2
a1166 2
\centerline{\epsffile{Cvstack.eps}}
\caption{Output of the \protect\verb:Cvstack: shell program.}
d1171 1
a1171 1
to enhance displays of the sort produced by \verb:Cvstack:.
d1184 1
a1184 1
\noindent Figure~\ref{fig:cvstack} shows an output generated by \verb:Cvstack:.
d1669 1
a1669 1
/* SUMUTE: $Revision: 1.2 $ ; $Date: 94/10/13 09:14:16 $	*/  [1]
d1771 1
a1771 1
/* SUVLENGTH: $Revision: 1.2 $ ; $Date: 94/10/13 09:14:16 $	*/
d1841 1
a1841 1
/* SUVLENGTH: $Revision: 1.2 $ ; $Date: 94/10/13 09:14:16 $        */
@


1.2
log
@A number of small changes by John while the file was sitting in
Barbara's area and not being maintained under RCS.
@
text
@d57 1
a57 1
There are faculty and students, both at our Center and elsewhere, who have contributed to \SU, that it is impossible to list them all here.  However, certain people have made such signal contributions that they deserve mention whenever the package is discussed.
d1669 1
a1669 1
/* SUMUTE: $Revision: 1.13 $ ; $Date: 93/11/24 15:50:02 $	*/  [1]
d1771 1
a1771 1
/* SUVLENGTH: $Revision: 1.1 $ ; $Date: 94/07/06 11:19:53 $	*/
d1841 1
a1841 1
/* SUVLENGTH: $Revision: 1.1 $ ; $Date: 94/07/06 11:19:53 $        */
@


1.1
log
@Initial revision
@
text
@d2 1
a2 1

d25 1
d31 1
a31 1
\centerline{\Large Version 0.9: July, 1994}
a33 1
\vspace{0.5in}
d36 4
a39 1
\hfill\epsfysize=.5in \epsffile{grilogo.eps}\hspace{0.5in}
a40 2
\vspace{8pt}
\noindent The \SU\ project is partially funded by the Gas Research Institute and by the Consortium Project on Seismic Inverse Methods for Complex Structures at the Center for Wave Phenomena.
d42 1
d46 1
d48 3
a50 2
\chapter{Acknowledgments}
We are pleased to acknowledge that \SU\ is partially supported by the Gas Research Institute.  This welcome support comes at a time when \SU\ is achieving world wide distribution and is thus making greater demands on our resources.
d52 4
a55 1
The sponsors of the \CWP\ Consortium Project have long been partners in the \SU\ project and we are pleased to explicitly acknowledge that here.  In addition, we wish to acknowledge extra support supplied in the past by \Sf{IBM} and by the Center for Geoscientific Computing at the Colorado School of Mines during the period when \SU\ was ported to the modern workstation from its previous incarnation on graphics terminals.
d57 1
a57 1
So many faculty and students both at our Center and elsewhere have contributed to \SU, that it is impossible to list them all here.  However, certain people have made such signal contributions that they deserve mention whenever the package is discussed.
d59 1
a59 1
Shuki Ronen wrote the first draft of what is now called \SU\ in cooperation with Einar Kjartessen while both were graduate students at Stanford University.  In turn, some of the fundamental concepts they implemented were formulated by their mentor, Jon Claerbout, Director of the Stanford Exploration Project.  Ronen brought this work to our Center during a two year stay here and, during this time, aided Cohen in turning \SU\ into a supportable and exportable product.
d70 4
d75 2
d84 1
a84 1
The \SU\ package is \Em{free software}, distributed with the full
d99 1
a99 1
assume that the data is written in \Sf{SEG-Y} format
d113 2
a114 2
The \SU\ community is now world-wide, spanning six continents, more than
16 countries and over 200 known installations on a variety of hardware platforms ranging from mainframes to workstations and \Sf{PC}'s.
d148 1
a148 1
an annotated transaction listing is given in section~\ref{SU:sec:anonftp}.
d182 2
a183 2
\section{Requirements for Installing the Codes}
The only requirements for installing the codes are:
d190 1
a190 1
The codes have been successfully installed on:
d206 1
a206 1
The distribution contains a series of files that detail the installation process.  They should be read in the following order:
d218 1
a218 1
\section{A Quick Test}
d418 1
d429 1
a429 1
        long tracr;     /* trace sequence number within reel */
d431 1
a431 1
        long fldr;      /* field record number */
d433 1
a433 1
        long tracf;     /* trace number within field record */
d435 1
a435 1
        long ep;        /* energy source point number */
d437 1
a437 1
        long cdp;       /* CDP ensemble number */
d439 1
a439 1
        long cdpt;      /* trace number within CDP ensemble */
d509 1
a509 1
to use \unix\ shell-redirecting and pipes you are ready to start
d542 1
a542 1
This example shows a band limiting operation being ``piped'' into
d547 1
a547 1
The strings with the \verb:=: signs illustrate
d556 1
a556 1
one popular style.  On the other hand, spaces around the \SU\ \verb:=:
d567 2
a568 2
The built-in \unix\ commands do not have a self-documentation
facility like \SU's---instead, they have ``man'' pages (on most systems).
d606 1
a606 1
Let's first find out more about \Bf{sufind} by asking for its self-doc:
a607 10
% sufind

sufind - get info from self-docs about SU programs
Usage: sufind [-v -n] string
"sufind string" gives brief synopses
"sufind -v string" verbose hunt for relevant items
"sufind -n name_fragment" searches for command name
\end{verbatim}}\noindent
Next use \Bf{sufind} to see if there are any \Sf{DMO} programs:
{\small\begin{verbatim}
d634 1
d728 11
d753 1
a753 1
\noindent\Bf{Remark}: All the \unix\ shells, Bourne (sh), Cshell (csh),
d779 2
d825 1
a825 1
plotting programs (e.g., supsimage).  For example, with \Bf{supsimage}in
d860 1
a860 1
here we assume that the parameters \verb:cdpmin=601:, \verb:cdpmax=610: are
d899 2
d927 1
a927 1
the subsidiary nmo processes.  Since these picks must be consistant
d967 2
a968 2
Related shell programs are \Bf{su/examplesNmostack} and
\Bf{su/examplesMig}.
d974 1
a974 1
\Bf{Velan} in \Bf{/su/examples}.
d977 1
a977 1
a high level programming language---consequently effective shell
d979 1
a979 1
provide some useful templates to for some of the
d986 3
a988 1
not contained in any single \SU\ program.  \Bf{Remark}:  For most of us,
d1068 1
d1071 1
a1071 1
\item This is a cheap trick.  We needed to get some information
d1095 1
a1095 1
to quit after it passes along the first line.  The second pass through
d1105 1
a1105 1
\Bf{bc} calculator program along with the ``here document'' facility.
d1114 3
a1116 2
following the here document redirection symbol \verb:<<: are arbitrary,
the shell reads along until it comes to a line that contains the same
d1130 4
a1133 2
Again be warned about spaces: the spaces around the bracket
symbols are essential.  Caveat: The bracket notation is a nice
d1221 1
a1221 1
\section{A Brief Tour of the Source Directories}
d1238 1
a1238 1
	\item psplot---PostScript graphics:
d1249 1
a1249 1
	\item xplot---xlib based X Windows graphics
d1255 1
a1255 1
	\item Xtcwp---toolkit based X Windows graphics
d1306 1
a1306 1
To fix the problem of a missing \Tt{strtoul} routine replace:\\ \Tt{CWPROOT/src/par/lib/atopkge.c}\\
d1312 1
a1312 1
For a missing \Tt{strerror} routine replace:\\
d1331 2
a1332 1
If the \Sf{GNU} libraries have not been installed, then the
d1445 1
a1445 1
\Tt{xwigb}, \Tt{xgraph}, or~\Tt{xmovie},
d1483 1
a1483 1
\noindent\Bf{Caveats}: The above calculations assume that you have
d1539 1
a1539 1
\section{Tape handling}
d1601 1
a1601 1
This section addresses general questions about the package.
d1604 1
a1604 1
Why are \Sf{CWP/SU} Releases given by integers (22, 23, 24, etc...)
d1609 1
a1609 1
The \Sf{CWP/SU} Release numbers are chosen to correspond
d1617 1
a1617 1
\noindent Remark:  In the early days, we \Em{did} use \Sf{RCS} to
d1642 1
a1642 1
line also have the feature of begin able to read from a
d1664 1
a1664 1
\section {A Template SU Program\label{SU:sec:template}}
d1666 1
a1666 1
looks like the program listing below (we excerpted lines from the program \Tt{sumute} to build this template).  The numbers in square brackets at the end of the lines in the listing are not part of the listing---we added them to facilitate discussion of the template.  The secret to efficient \SU\ coding is finding an existing program similar to the one you want to write.  If you have trouble locating the right code or codes to ``clone'' ask us---this can be the toughest part of the job!
d1669 1
a1669 1
/* SUMUTE: $Revision: $ ; $Date: $	*/                               [1]
d1703 1
a1703 1
        int ns;                /* ...                        */      [5]
d1744 1
a1744 1
\item We maintain the internal versions of the codes with the \unix\ utility \Sf{RCS}.  This item shows the string templates for \Sf{RCS} and can be ignored.
d1747 1
a1747 1
\item This is an external declaration of an \SU\ (\Sf{SEG-Y}) trace template.  It is external to avoid wasting stack space.  Some codes require two such templates (typically called \Tt{intrace} and \Tt{outtrace} or \Tt{intrace1} and \Tt{intrace2}).
d1751 1
a1751 1
\item The \Tt{initargs} subroutine sets the command line ``getpar'' parameter passing facility (syntax is like Fortran namelist).
d1753 3
a1755 3
\item This is typical code for reading ``getpar'' specifications from the command line.  Parse it like this:  If the user did not specifiy a value, then use the default value.  The subroutine must be type-specific, here we are getting an \Em{integer} parameter.
\item Read the first trace, exit if empty.  The subroutine \Tt{fgettr} ``knows about'' the \SU\ trace format.  Usually the trace file is read from standard input and then we use \Tt{gettr} which is a macro based on \Tt{fgettr} defined in \Tt{su.h}.  Note that this code implies that the first trace is read into the trace buffer (here called \Tt{tr}) and we will have to process this trace before the next call to \Tt{fgettr}.
\item We've read that first trace because typically, we need to get some trace parameters from the first trace header.  Usually these are items like the number of samples (\Tt{tr.ns}) and/or the sampling interval (\Tt{tr.dt}) that, by the \Sf{SEGY-Y} standard, are the same for each trace.
d1759 1
a1759 1
\item \Tt{fputtr} and \Tt{puttr} are the output analogs of \Tt{fgettr} and \Tt{gettr}.  If trace header values have been changed, these changes must be registered before calling these output trace routines (the code developed below will contain an example).
d1766 1
a1766 1
A recent visitor from Russia asked about \SU\ processing for variable length traces.  At his institute, data is collected from time of excitation to a variable termination time.  The difficulty is that \SU\ processing is based on the \Sf{SEG-Y} standard which mandates that all traces in the data set be of the same length.  Rather than contemplating changing all of \SU, it seems to us that the solution is to provide a program that converts the variable length data to fixed length data by padding with zeroes where necessary at the end of the traces---let's name this new program \Tt{suvlength}.  We can make the length of the output traces a user parameter.  If there is a reasonable choice, it makes sense to provide a default value for parameters.  Here, using the length of the first trace seems the best choice since that value can be ascertained before the main processing loop starts.
d1771 1
a1771 1
/* SUVLENGTH: $Revision: $ ; $Date: $	*/
d1829 1
a1829 1
Now we run into a small difficulty.  Our only parameter has a default value that is only obtained after we read in the first trace.  The obvious solution is to reverse the parameter getting and the trace getting in the template.  Thus we resume: 
d1841 1
a1841 1
/* SUVLENGTH: $Revision: $ ; $Date:  $        */
d1925 2
a1926 1
For any new \SU\ code, one should provide an example shell program to show how the the new code is to be used.  Here is such a program for X Windows graphics:
@
